cleaned_comment,platform,link,BERTopic_Topic_10plus
I wonder if there's any potential for Copilot to suggest malicious code because it's been trained on an open source projects containing intentionally malicious code.,Hacker News,https://news.ycombinator.com/item?id=27676833,0
"It sounds like this is similar to Kite, but actually competent as a service and not associated with a brand that has destroyed all trust. But it has to come with the same privacy caveats, right? Uploading your private code to a third-party server could result in business or regulatory violations.And even if you're okay with sending your code, what about hardcoded secrets? What's to prevent Copilot clients from sending things that should never leave the user's computer? Heuristics? Will we be able to tell what part of the code is about to be sent? And is the data stored?",Hacker News,https://news.ycombinator.com/item?id=27678483,-1
"I wonder if CoPilot uses Github's private repositories to train itself, which would allow malicious users to somehow obtain code or designs that they otherwise would not be able to view.",Hacker News,https://news.ycombinator.com/item?id=27677471,10
"Why participate in the hype?It's highly likely this is the result of legal having mulled over the Copilot litigation and deciding that it can go either way or worse, so the top layer in MICROS~1 has decided to rush whatever product development they can use the general public for and try to hook as many people as they can to their stochastic parrots. Which they wish to insert into not just every other government machine, but also every Linux machine, preferably pumping out every line of code written with those.It's better to support open source rights holders against this exploitation than getting one's nose brown.",Hacker News,https://news.ycombinator.com/item?id=35263858,1
"What happens when someone puts code up on GitHub with a license that says ""This code may not be used for training a code generation model""?- Is GitHub actually going to pay any attention to that, or are they just going to ingest the code and thus violate its license anyway?- If they go ahead and violate the code's license, what are the legal repercussions for the resulting model? Can a model be ""un-trained"" from a particular piece of code, or would the whole thing need to be thrown out?",Hacker News,https://news.ycombinator.com/item?id=27688025,2
"If someone could show that the ""copilot"" started ""generating"" code verbatim (or nearly verbatim) from some GPL-licensed work, especially if that section of code was somehow novel or specific to a narrow domain, I suspect they'd have a case. I don't know much about OpenAICodex, but if it's anything like GPT-3, or uses that under the hood, then it's very likely that certain sequences are simply memorized, which seems like the maximal case for claiming derivative works. On the other hand, if someone has GPL'd code that implements a simple counter, I doubt the courts would pay much attention.I do wonder, though, if GPL owners worried about their code being shanghaied for this purpose could file arbitration claims and exploit some particularly consumer-friendly laws in California which force companies to pay fees like when free speech dissidents filed arbitrations against Patreon.[0] Patreon is being forced to arbitrate 72 claims individually (per its own terms) and pay all fees per JAMS rules. IANAL, so I don't know the exact contours of these rules, or if copyright claims could be raised in this way, or even if GitHub's agreements are vulnerable to this loophole, but it'd be interesting.[0] (see second update from July 31).",Hacker News,https://news.ycombinator.com/item?id=27688711,5
"The second tweet in the thread seems badly off the mark in its understanding of copyright law.> copyright does not only cover copying and pasting; it covers derivative works. github copilot was trained on open source code and the sum total of everything it knows was drawn from that code. there is no possible interpretation of ""derivative"" that does not include thisCopyright law is very complicated (remember Google vs Oracle?) and involves a lot of balancing different factors [0]. Simply saying that something is a ""derivative work"" doesn't establish that it's copyright infringement. An important defense against infringement claims is arguing that the work is ""transformative."" Obviously ""transformative"" is a subjective term, but one example is the Supreme Court determining that Google copying Java's API's to a different platform is transformative [1]. There are a lot of other really interesting examples out there [2] involving things like if parodies are fair use (yes) or if satires are fair use (not necessarily). But one way or another, it's hard for me to believe that taking static code and using it to build a code-generating AI wouldn't meet that standard.As I said, though, copyright law is really complicated, and I'm certainly not a lawyer. I'm sure someone out there could make an argument that Copilot is copyright infringement, but this thread isn't that argument.[0] Note that the other comments saying ""I'm just going to wrap an entire operating system in 'AI' to do an end run around copyright"" are proposing to do something that wouldn't be transformative and therefore probably wouldn't be fair use. Copyright law has a lot of shades of grey and balancing of factors that make it a lot less ""hackable"" than those of us who live in the world of code might imagine.",Hacker News,https://news.ycombinator.com/item?id=27688105,4
"By submitting any textual content (GPL or otherwise) on the web, you are placing it in an environment where it will be consumed and digested (by human brains and machine learning algorithms alike). There is already legal precedent set for this which allows its use in training machine learning algorithms, specifically with heavily copyrighted material from books[1].This does not mean that any GitHub Co-Pilot produced code is suddenly free of license or patent concerns. If the code produces something that matches too closely GPL or otherwise licensed code on a particularly notable algorithm (such as video encoder), you may still be in a difficult legal situation.You are in essence using ""not-your-own-code"" by relying on CoPilot, which introduces a risk that the code may not be patent;license free, and you should be aware of the risk if you are using this tool to develop commercial software.The main issue here is that many average developers may continue to stamp their libraries as MIT;BSD, even though the CoPilot-produced code may not adhere to that license. If the end result is that much of the OSS ecosystem becomes muddied and tainted, this could slowly erode trust in open licenses on GitHub (i.e. the implications would be that open source libraries could become less widely used in commercial applications).[1] -",Hacker News,https://news.ycombinator.com/item?id=27693657,2
"Copyright Rules are not meant to favor a few.If that's the case then it makes a great case to break the DMCA and steal all the content out there.That sounds terrible in theory but it's one way to put some of these big money systems in check.What about all the leaked source code in early 2022. It would make perfect sense for the hackers to remove the copyright and put it in Github for karma. That way, the IP of all now belongs to all. I'm quite sure a few won't agree with this method, but that's exactly how bad it is when removing licenses from software for code predictions.Finally, Git was meant to allow people to host anywhere, so if the worry is about IP theft, one should stop hosting on Github and move it some place else, if they want it fully private.",Hacker News,https://news.ycombinator.com/item?id=33228772,2
"Well, here's a question. Is GitHub violating the LGPL by including the code in their copilot data? Or is it Copilot users who end up using the regurgitated code?I guess GitHub is violating it the instant their servers send verbatim snippets like this to developers without the copyright notice. And then the Copilot users are also violating it when (if) they release their code that contains verbatim snippets and no notice.This is a pretty good live experiment on how useful these open source licenses really are. This guy find his license is being violated, so what does he do? If complaining on Twitter seems like the best course of action then maybe we need to rethink the system.",Hacker News,https://news.ycombinator.com/item?id=33228658,2
So.. say Microsoft retrained Copilot on code only explicitly marked as open-source. As an activist or vandal you could start publishing proprietary code with fraudulent license files to pollute Copilot again.This could be terribly fun.,Hacker News,https://news.ycombinator.com/item?id=33228452,7
"Hey, including copyrighted code, especially from other repositories without a correct license, is an honest mistake. You should be able to file a DMCA request with a list of all the repos containing your code, and then they should just retrain copilot with those repos excluded. Clearly that's what is needed for them to stop distributing your code. ;sSarcasm aside I think there are several possible legal viewpoints here (IANAL):1. copilot is distributing copies of code and it's not a safe harbor: Microsoft is directly liable to copyright infringement by copilot producing code without appropriate license;attribution.2. copilot is distributing copies of code and it's a safe harbor: Microsoft is not directly liable, but it should comply with DMCA requests. Practically that would mean retraining with mentioned code snippets;repositories excluded in a timely manner, otherwise I don't see a way how they could disentangle the affected IP from the already trained models.3. copilot produces novel work by itself not subject to copyright of the training data: I think this is really a stretch. IANAL, but I think producing novel creative work is a right exclusive to living human beings, so machines can't produce them almost by definition. (There is the monkey selfie copyright case, but at least the ""living"" there was ticked off).4. the user of copilot is producing novel work by prompting copilot: it's like triggering a camera. The copyright of the resulting picture is fully owned by the operator, even though much of the heavy lifting is done by the camera itself. Even then, this very much depends on the subject.IMO option 3 doesn't have a legal standing. Microsoft and users of copilot would very much like if it was option 4 that applied always, but this particular case clearly falls under option 1 or option 2, in which case Microsoft should hold some legal liability, even if they can't always track the correct license ahead of time.",Hacker News,https://news.ycombinator.com/item?id=33231908,2
"Yeah, I think I am going to take my personal repos off of GitHub and probably recommend the same thing at our company. Seems to be a liability that our proprietary code could get included in these models and leaked (even if accidentally). I feel similar to this on how I do AI art, which is that I am being taken advantage of by some larger entity so they can make money, even if they've been explicitly told it's illegal. On the argument of the machine is just leaning like any other human, that is absolute nonsense. Makes me feel ashamed to work in software, the way people can take advantage of other peoples hard work to make a buck with no even request or slightest bit of remorse.",Hacker News,https://news.ycombinator.com/item?id=33231672,2
"Not your repo, not your code.I celebrate Microsofts shameless plundering of Github to create new products that increase productivity. The incredible thing is that people trusted Microsoft to use their code on their terms to begin with. This is a company who has been finding ways to make open source code into a proprietary product since the 90s.Nobody can stop people from replicating what Microsoft did in the long run anyways. Eventually any consumer with enough access to source code will be able to make their own copilot. Even if copilot is criminalised Microsoft can just sell access to the entire GitHub dataset and let other people commit the ""crime"". Then you're right back where we started with having to sue the end users of copilot for infringement instead of Microsoft.Use private repos or face the inevitability that copilot-like products will scrape your code.",Hacker News,https://news.ycombinator.com/item?id=33226935,-1
"I feel like the restrictions of these ai chatbots could be better managed with another GPT that has the rules and is being asked, does the prompt conform to the rules and does the answer conform to the rules.This would mean the actual copilot has only the minimal prompt required to function, without restrictions because they would be picked up by the watcher GPT, that can't leak it's prompt.",Hacker News,https://news.ycombinator.com/item?id=35926948,-1
"The main page [0] shows you awesome demos, but also its weaknesses in the very first example. It doesn't encode the url encoded body properly:> body: `text=${text}`,So it breaks if the text contains a '&' and even allows parameter injection to the call of the 3rd party service. Isn't that critical on a sentiment analysis API, but could result in actual security holes.I hope the users won't blindly use the generated code without review. These mistakes can be so subtle, nobody even noticed them when they put them on the front page of the product.[0]:",Hacker News,https://news.ycombinator.com/item?id=31827060,-1
"As Copilot is becoming generally available, this might be a good time to write a comprehensive comparison between the two leading AI assistants for software development Tabnine and Copilot by Microsoft. Details here are from our CEO and Founder Dror:Usually, I suggest that my team start with the user value and experience, but for this specific comparison, it's essential to start from the technology, as many of the product differences stem from the differences in approach, architecture, and technology choices. Microsoft and OpenAI view AI for software development almost as just another use case for GPT-3, the behemoth language model. Code is text, so they took their language model, fine-tuned it on code, and called the gargantuan 12-billion parameter AI model they got Codex.Copilot's architecture is monolithic: ""one model to rule them all.""Ã‚Â It is also completely centralized - only Microsoft can train the model, and only Microsoft can host the model due to the enormous amount of computing resources required for training and inference.Tabnine, after comprehensively evaluating models of different sizes, favors individualized language models working in concert. Why? Because code prediction is, in fact, a set of distinct sub-problems which doesn't lend itself to the monolithic model approach. For instance: generating the full code of a function in Python based on name and generating the suffix of a line of code in Rust are two problems Tabnine solves well, but the AI model that best fits every such task is different. We found that a combination of specialized models dramatically increases the precision and length of suggestions for our 1M+ users.A big advantage of Tabnine's approach is that it can use the right tool for any code prediction task, and for most purposes, our smaller models give great predictions quickly and efficiently. Better yet, most of our models can be run with inexpensive hardware.Now that we understand the principal difference between Microsoft's huge monolith and Tabnine's multitude of smaller models, we can explore the differences between the products:First, kind of code suggestions. Copilot queries the model relatively infrequently and suggests a snippet or a full line of code. Copilot does not suggest code in the middle of the line, as its AI model is not best suited for this purpose. Similarly, Tabnine Pro also suggests full snippets or lines of code, but since Tabnine also uses smaller and highly efficient AI models, it queries the model while typing. As a user, it means the AI flows with you, even when you deviate from the code it originally suggested The result is that the frequency of use - and the number of code suggestions accepted - is much higher when using Tabnine. An astounding number of users accept more than 100 suggestions daily.Second, ability to train the model. Copilot uses one universal AI model, which means that every user is getting the same generic assistance based on an ""average of GitHub""Ã‚Â, regardless of the project they're working on. Tabnine can train a private AI model on the specific code from customers' GitLab;GitHub;BitBucket repositories and thus adjust the suggestions to the project-specific code and infrastructure. Training on customer code is possible because Tabnine is modular, enabling the creation of private customized copies. Tabnine ""democratizes"" AI model creation, making it easy for teams to train their own specific AI models, dramatically improving value for their organization.Third, Code security and privacy. There are a few aspects of this. Users cannot train or run the Copilot model. The single model is always hosted by Microsoft. Every Copilot user is sending their code to Microsoft; not some of the code, and not obfuscated - all of it. With Tabnine, users can choose where to run the model: on the Tabnine cloud, locally on the developer machine, or on a self-hosted server (with Tabnine Enterprise). This is possible because Tabnine has AI models that can run efficiently with moderate hardware requirements. This means that, in contrast to Copilot, developers can use Tabnine inside their firewall without sending any code to the internet. In addition, Tabnine makes a firm and unambiguous commitment that no code the user writes is used to train our model. We don't send to our servers any information about the code that the user writes and the suggestions they're receiving or accepting.Fourth, commercial terms. Microsoft currently offers Copilot only as a commercial product for developers, without a free plan (beyond a free trial) or organizational purchase. Tabnine has a great free plan and charges for premium features such as longer code completions and private models trained on customers' code. We charge a monthly;annual subscription fee per number of users. All our plans fit organizational requirements.Philosophically, Copilot is more of a walled garden where Microsoft controls everything. Copilot users are somewhat subjects in Microsoft's kingdom. Tabnine's customers can train the AI models, run them, configure the suggestions, and be in control of their AI.In sum: both products are great; you're welcome to try (Tabnine Pro) and see which one you prefer. for professional programmers, Tabnine offers in-flow completions, the ability to adapt the AI to their code, and superior code privacy and security.For those who want to try Tabnine Pro, here's a coupon for one month free here's a detailed comparison table of Tabnine vs Copilot",Hacker News,https://news.ycombinator.com/item?id=31829376,0
"I'm not confident in this stance - sharing it to have a conversation. Hopefully some folks can help me think through this!The value of copyleft licenses, for me, was that we were fighting back against the notion of copyright. That you couldn't sell me a product that I wasn't allowed to modify and share my modifications back with others. The right to modify and redistribute transitively though the software license gave a ""virality"" to software freedom.If training a NN against a GPL licensed code ""launders"" away the copyleft license, isn't that a good thing for software freedom? If you can launder away a copyleft license, why couldn't you launder away a proprietary license? If training a NN is fair use, couldn't we bring proprietary software into the commons using this?It seems like the end goal of copyleft was to fight back against copyright, not to have copyleft. Tools like copilot seem to be an exceptionally powerful tool (perhaps more powerful than the GPL) for liberating software.What am I missing?",Hacker News,https://news.ycombinator.com/item?id=33457933,-1
"It's also a class action,> behalf of a proÃƒâ€šÃ‚Â­posed class of posÃƒâ€šÃ‚Â­siÃƒâ€šÃ‚Â­bly milÃƒâ€šÃ‚Â­lions of GitHub users...The appendix includes the 11 licenses that the plaintiffs say GitHub Copilot violates:",Hacker News,https://news.ycombinator.com/item?id=33457152,2
"If Microsoft is so confident in the legality and ethics of Copilot, and that it doesn't leak or steal proprietary IP... they should go train it on the MS Word and Windows and Excel source trees.What's that? They don't want to do that? Why not?",Hacker News,https://news.ycombinator.com/item?id=33458282,1
"I think if this is successful it will be very bad for the open world.Large platforms like github will just stick blanket agreements into the TOS which grant them permission (and require you indemnify them for any third party code you submit). By doing so they'll gain a monopoly on comprehensively trained AI, and the open world that doesn't have the lever of a TOS will not at all be able to compete with that.Copilot has seemed to have some outright copying problems, presumably because its a bit over-fit. (perhaps to work at all it must be because its just failing to generalize enough at the current state of development) --- but I'm doubtful that this litigation could distinguish the outright copying from training in a way that doesn't substantially infringe any copyright protected right (e.g. where the AI learns the 'ideas' rather than verbatim reproducing their exact expressions).The same goes for many other initiatives around AI training material-- e.g. people not wanting their own pictures being used to train facial recognition. Litigating won't be able to stop it but it will be able to hand the few largest quasi-monopolisits like facebook, google, and microsoft a near monopoly over new AI tools when they're the only ones that can overcome the defaults set by legislation or litigation.It's particularly bad because the spectacular data requirements and training costs already create big centralization pressures in the control of the technology. We will not be better off if we amplify these pressures further with bad legal precedents.",Hacker News,https://news.ycombinator.com/item?id=33458553,4
"Crackpot Theory: Copilot (and by association many ML tools) is a form of probabilistic encryption. Once encoded, it's virtually impossible to pull the code (plaintext) directly out of the raw ML model (the cyphertext), yet when the proper key is input (';;sparse matrix transpose'), you get the relevant segment of the original function (the plaintext) back.We've even seen this with stable diffusion image generation, where specific watermarks can be re-created (decrypted?) deterministically with the proper input.",Hacker News,https://news.ycombinator.com/item?id=33459235,0
"Think some of the negativity about Copilot may be the perception that if an individual or small startup attempted training an ML model from public source-code and commercialised a service from it they would be drowning in legal issues from big companies not happy with their code used in such a product.In addition just because code is available publicly on GitHub does not necessarily mean it is permissively licensed to use elsewhere, even with attribution. Copyright holders not happy with their copyrighted works publicly accessible can use the DMCA to issue take-downs that GitHub does comply with but how that interacts with Copilot and any of its training data is a different question.As much as the DMCA is bad law rather funny seeing Microsoft be charged in this lawsuit with the less known provision against 'removal of copyright management information'. Microsoft does have more resources to mount at defence so it will probably end up different compared to a smaller player facing this action.",Hacker News,https://news.ycombinator.com/item?id=33459890,2
"Consider each repo on github to be a movie. What copilot does is to search for sequences of frames from any movie which line up to create a new coherent movie.Individually, each frame is protected by the copyright of the movie it belongs to. But what happens if you take a million frames from a million different movies and just arrange them in a new way?That's the core question here. Is the new movie a new copyrightable work, or is it plagiarizing a million other works at once? Is it legal to use copyrighted works in this way?The other question is if it is right to use copyrighted works this way. Is this within the spirit of open source software? Or is this just a bad corporation taking advantage of your good will?I'm not sure where I stand on this, it's a complicated problem for sure. Definitely interested to see how this plays out in court.",Hacker News,https://news.ycombinator.com/item?id=33459805,2
"This issue seems to have an obvious solution that I fail to see anyone mention: Treat copilot simply as a tool, let it be trained on whatever without any consent requirements. However the outputs should be subject to copyright as with any other code produced by a human. Then on a case by case basis courts can decide if infringement has occurred. The idea of banning copilot or other AI models as a whole just seems like a collective case of sour grapes because innovation and automation is finally threatening some people who only expected these things to affect the working class",Hacker News,https://news.ycombinator.com/item?id=33461806,-1
"As much as I love the little guy beating the big evil company, I hope the lawsuit doesn't cause anything to happen to copilot. Maybe some changes, like better protection against emitting 1:1 licensed code or opting out your code from training.",Hacker News,https://news.ycombinator.com/item?id=33459597,-1
"I don't have a comment on this personally but I want to throw this out there because every time I see people criticizing Copilot or Dall-E someone always says ""BUT ITS FAIR USE! Those people don't seem to grasp that ""Fair Use"" is a defense. The burden is not on me to prove what you are doing is not fair use; the burden is on you to prove what you are doing is fair use",Hacker News,https://news.ycombinator.com/item?id=33457731,1
"Copilot has so far been pretty useful to me as a ""sometimes smarter intellisense"". It'll frequently correctly guess the arguments I want for a function (and their types), and every once in a while I'll type `a.map(` and it'll auto-fill the transformation code I was planning on writing.The simpler the task I'm trying to do, the better chance it has of being correct, but that's also the part where I feel I get the most benefit from it, because I already thoroughly understand exactly what I'm writing, why I'm writing it, and what it needs to look like, and Copilot sometimes saves me the 5-30s it takes to write it. Over a day, that adds up and I can move marginally faster.It's definitely not a 100x improvement (or even a 10x improvement), but I'm glad to have it.If this works as well, locally, to escape the privacy issue, I'll be thrilled. Checking it out.",Hacker News,https://news.ycombinator.com/item?id=35471933,0
"Does this come pre-trained, or does it require training? What sources of data are you training on if so?I restrict all usage of AI tools trained from publicly-sourced data because of an unknown copyright restriction, general unease, and lawsuits; however if this can be trained solely on my own codebases that are of clean providence, I can be 100% guaranteed against potential lawsuits.Copilot is a cool tool, but super scary from a legal perspective. And even more heavily regulated industries (that I'm not in) would absolutely need their own firewalled version.",Hacker News,https://news.ycombinator.com/item?id=35476020,4
"This is Hacker News so the conversation is obviously slanted towards code, but I wonder what the perspective would look like for other structured works, like books? If an author is using a ""copilot for writers"" and the AI emits text verbatim to another work, then I would think it would be plagiarism. If the text emitted is similar, but not the same, then I would think it would be considered paraphrasing which still requires attribution.",Hacker News,https://news.ycombinator.com/item?id=35663508,4
Makes you wonder how many public repos you would need to seed with a carefully crafted attack;weakness in a common feature;pattern to start effectively poisoning codebases that are leaning on copilot,Hacker News,https://news.ycombinator.com/item?id=35659446,3
"> GPL codeThere is no such thing as ""GPL code"" or any other ""$license code"". This is a fundamental misunderstanding of what a license is. The code in question was licensed to GitHub under a different license - possibly fraudulently.",Hacker News,https://news.ycombinator.com/item?id=35662101,2
"Looks like the code in question is hosted on Github: that covered by:""You grant us and our legal successors the right to store, archive, parse, and display Your Content... share it with other users...""",Hacker News,https://news.ycombinator.com/item?id=35658583,-1
"How do you disable it again? You can't!Once you pressed ""Start using Copilot"" the copilot menu shows up on all repositories within GitHub.com for careless or uninformed users to leak proprietary code to their training set.",Hacker News,https://news.ycombinator.com/item?id=42466465,3
"Microsoft would need to pay me to use Copilot. Seems like a major scam for them to learn from our code and then tell us we can'tuse it to make our own competing AI systems. ""Limits on use of data from the AI Services. You may not use the AI services, or data from the AI services, to create, train, or improve (directly or indirectly) any other AI service.""Ã‚Â From TODAY ""Ã¢â‚¬Â the ridiculous focus on AI over core business at github is the number one thing likely to kill the platform. They are coasting on network effects",Hacker News,https://news.ycombinator.com/item?id=42456607,-1
"Most of the arguments here boil down to the same belabored point, that you shouldn't expect Copilot to actually write the code for you at the end of the day.My take is it's what you make of it. Copilot is only equivalent to copy-and-pasting from stack overflow if that's how you choose to field its suggestions.As an example, I've enjoyed typing ""const one_day_in_ms"" and letting it finish it out with ""1000 * 24 * 60 * 60"". I already knew how to do that, but having GCP finish it for me and verifying on my own didn't make me feel stupider, it made me more efficient. I have more interesting problems to tackle.On the other hand, another coder could have not known this calculation and thrown their trust into GCP. That's bad practice and it's on them, not on the tool.Sometimes GCP gives me code that it learned from bad coding patterns. I know how GCP works and I know to look out for that, so I ignore those suggestions.Of course, sometimes I don't know if what looks like a good idea from GCP is actually not. I take that on as my responsibility to trust but verify. If it's writing some function to slugify a string for a URL, I check it against what people are discussing online. Does it defeat the purpose of GCP in this case if I have to check it on my own? Probably, but it's only in these specific instances when I'm doing something I'm not familiar with.",Hacker News,https://news.ycombinator.com/item?id=29029747,0
> Login to GitHub Copilot using the device auth flow and authorized GitHub Copilot IntelliJ plugin with your GitHub Account in an external browser.> Read and agree to the GitHub Copilot additional telemetry terms.Can anybody comment on the privacy aspects of this? Is the telemetry reasonable? Why on earth do I need to login: presumably so that they can associate my coding with my account to structure the data they are gathering?,Hacker News,https://news.ycombinator.com/item?id=29023041,8
"In my opinion, Copilot is going to become one of those ""perceived authorities"" that have just enough legitimacy to be blindly trusted by the inexperienced, but not enough to actually be useful to the experts.This is like social media (or even the Internet as a whole) and say, our parents' generation. Countless times I receive links to Facebook posts or random articles that somebody thinks must be true, simply because The Oracle (i.e. their smartphone) showed it to them. For much of the older generation, smartphones are these all-knowing repositories of wisdom, and anything they come across while using them is likely to be true. This is why I think misinformation has spread so easily.I imagine Copilot going down a similar path. The next generation of programmers who didn't grow up with knowing how to sift through API docs or SO answers for the right bit of code, or who's attention spans have been fizzled away, will love the idea of Copilot. Instant gratification in the firm of a tool that can seemingly do your work for you. This will be dire consequences for their ability to code and think for themselves.",Hacker News,https://news.ycombinator.com/item?id=29019321,0
"Copilot thread is always hot. TBH I don't mind the plagiarism. They can use my code.The real problem is indeed the code quality, since Copilot does actively provide low-quality codes, and this will bite a lot of people. I guarantee. I don't think this massive learning approach can't be the solution. We need an extra kick.My shower-thought solution for this garbage-spewing problem is to design a new library with Copilot in mind. Tighten up the interface, and use strict patterns instead of domain-specific hacks.In other words, we can make libraries so lame that Copilot (and newb programmers) simply can't produce low-quality code. Just disregard smart programmers with fast hands. They don't need any help anyway. Don't even try to target them, because it's gonna cause more stupid flame wars...",Hacker News,https://news.ycombinator.com/item?id=29021246,0
"Original research author here. It's exciting to find so many thinking about long-term code quality! The 2023 increase in churned & duplicated (aka copy;pasted) code, alongside the reduction in moved code, was certainly beyond what we expected to find.We hope it leads dev teams, and AI Assistant builders, to adopt measurement & incentives that promote reused code over newly added code. Especially for those poor teams whose managers think LoC should be a component of performance evaluations (around 1 in 3, according to GH research), the current generation of code assistants make it dangerously easy to hit tab, commit, and seed future tech debt. As Adam Tornhill eloquently put it on Twitter, ""the main challenge with AI assisted programming is that it becomes so easy to generate a lot of code that shouldn't have been written in the first place.""That said, our research significance is currently limited in that it does not directly measure what code was AI-authored -- it only charts the correlation between code quality over the last 4 years and the proliferation of AI Assistants. We hope GitHub (or other AI Assistant companies) will consider partnering with us on follow-up research to directly measure code quality differences in code that is ""completely AI suggested,"" ""AI suggested with human change,"" and ""written from scratch."" We would also like the next iteration of our research to directly measure how bug frequency is changing with AI usage. If anyone has other ideas for what they'd like to see measured, we welcome suggestions! We endeavor to publish a new research paper every ~2 months.",Hacker News,https://news.ycombinator.com/item?id=39168841,0
"CoPilot is good a as a one line autocomplete. It's short enough that you can review the suggestion and decide whether or not to accept the autocompletion or type out your own completion.For reasoning about larger chunks of code I find ChatGPT better than CoPilot as an LLM assistant. Trying to use CoPilot for making large sections of boilerplate like the kind you might see in a db->api->web project is just full of frustration. It doesn't realize it makes makes tiny inconsistencies everywhere so you are permanently babysitting. I think the key takeaway is that if you have repeated code (An entity, a DTO, a controller, a frontend component all sharing some set of names;properties) then its better to change jobs than change tools.",Hacker News,https://news.ycombinator.com/item?id=39174109,0
"Something I didn't see mentioned is also that over time this is going to feedback loop into the training set and compound (possibly exponentially). I.e. as more lower quality code hits github and is used for training, the output of the code will decline, which causes lower quality code to hit github, which causes output to further decline, etc.I am an experienced dev but new to ML so take with a grain of salt, but I really wonder if the future is going to be quality in the training sets rather than quantity. I have heard that the ""emergent properties"" don't seem really affected by bad data as long as the set is large enough, but at what point does this cease to be true? How do we guard against this?",Hacker News,https://news.ycombinator.com/item?id=39173740,0
"""'AI-generated code resembles an itinerant contributor, prone to violate the DRY-ness [don't repeat yourself] of the repos visited.' ... That serves as a counterpoint to findings of some other studies, including one from GitHub in 2022 that found, for one thing: ""developers who used GitHub Copilot completed the task significantly faster -- 55 percent faster than the developers who didn't use GitHub Copilot.""These two observations aren't mutually exclusive. DRY-ness comes from a holistic understsanding of a system; you get similar results when you task a junior developer with little deep knowledge of a novel codebase with solving a problem and don't give them enough oversight; they'll tend to bull-rush through coding a solution with little-to-no knowledge of how the solution can be built out of the parts already existing in the codebase.",Hacker News,https://news.ycombinator.com/item?id=39172242,0
"I'm not surprised. AI tools can be great at providing a quick, working example in simple scenarios, but it's just that: a quick(often dirty) working example. But I've seen people taking it as-is and putting it into a production codebase. Sure, a function that iterates over an array, and checks if some item exists - fine. In those cases it will do the job simply because you(it) can't get it wrong. However I had this experience where senior developers were fully invested into using it. And because managers see code just erupting like a volcano, they embrace it and in fact rise their expectation when it comes to delivering a feature: ""you could do it in a week before, you should be able to do it in 2 hours now, right?"". And on more than one occasion this has backfired massively in everyone's face. The last time me and another dev spent two straight days rewriting and debugging everything cause there was an international exhibition that was about to start and the company was at the front of the line and everyone else simply pushed a ton of code that was 75% AI-generated, completely ignoring the edge cases, which were more than people anticipated.But probably the most off-putting thing I've experienced is an OKR session with 50 people in it, where a lead dev publicly opened chatgpt, prompted ""how do we reduce the number of critical bugs by 30% in the next quarter"", chatgpt came up with a generic response, everyone said ""perfect"", copy-paste that into Jira and call it a day. And I'm just sitting there and wondering if there was something rotten in my breakfast and I'm hallucinating. Unfortunately my breakfast was fine and that really happened. The few times I've tried using those, they were only helpful with dumb, repetitive and mundane tasks. Anything else, you have to rinse and repeat until you get a working solution. And when you read it(for those of us that do), you realize you might have been better off recruiting a freelancer from year one in University to spend a day mashing it up and likely coming up with something better.But I bet much of those year ones would be doing this exact thing day in and day out until they come up with a solution: Occasionally I will grab my laptop and go work at a cafe on my personal projects for a change and I can't tell you how many times I've seen people just copy pasting stuff from chatgpt and pasting it back into their IDE;editor and calling it a day - students and clearly people who are doing this for a living. Not to mention copilot, that's the de-facto standard at this point.In fact I had this conversation last year with a guy(developer) who is 20-something years older than me(so mid 50-s): most of the LLM's are trained on stuff that is in the documentation, examples, reddit and stackoverflow. It's only a question of time until the content found in those exact locations where the training data is pulled from will become more and more AI-generated, models will be re-trained on those and eventually shit hitting the fan. I don't think we are too far off from this event.",Hacker News,https://news.ycombinator.com/item?id=39168867,0
"This isn't the only bug;bad practice that the git-copilot page shows off. Here's the memoization for js suggestion on the main page: const memoize = fn => { const cache = {}; return (...args) => { const key = JSON.stringify(args); return (cache[key] = cache[key] || fn(...args)); }; } It uses js falsyness to figure out whether it can return from the cache or if it needs to invoke the wrapped function. However, js falsy is pretty dangerous. ""cache[key]"" will return undefined if there's no value in the cache for those arguments, but undefined is not the only falsy value. Here's the full list: of those values are reasonable function return values meaning your cache will simply not work for some function outputs.The key generation is also a little problematic. Stringifying the input may produce huge strings which are then kept in memory for an indefinite period of time which creates a memory leak.Here's the bottom line on git co-pilot. It's a huge step forward and I think everyone is going to be using tools like it in the future. There's no doubt to me that it will make good programmers way more productive. However, not-so-good programmers will become way more destructive since copilot will let them write bad, unoptimized code faster than every before.",Hacker News,https://news.ycombinator.com/item?id=27701133,-1
"I think this GH Copilot looks really cool but I wonder how many more or less subtle bugs are going to end up in codebases because of it. The snippets it generates are rather large, a tired dev on a deadline will probably not take the time of carefully reviewing them if they seem to mostly work as-is.Paradoxically I think that the more it'll improve the more dangerous it'll become, because if the Copilot gets it right 99% of the time you're more likely to miss the 1% of the time it doesn't. It's like car autopilots in a way, the better they become the more the driver lowers their guard and the more dangerous they become.",Hacker News,https://news.ycombinator.com/item?id=27698194,0
One fun exercise might be to go looking for cases where Copilot doesn't find anything.Then crafting some keyword seeded code that it would scoop up and retrain on.Would be interesting if you could get some adversarial code to be the suggestion for some notable niche.,Hacker News,https://news.ycombinator.com/item?id=27698234,0
"> Unescaped text is added into a format with defined encoding.Meh. This is only a problem if it's unknown text or from an external source like user input. If it's from a known, trusted source, like internal code it's fine.The cleaning should be done on the server side, anyway, so this objection is moot. Anyone can send any kind of body. Your client is in ""enemy territory"". Treat everything coming from the client side as potentially dangerous.If you take this article's advice, you might think you're safe by just using these form data or url encoding. No. Not at all. This will not save you from SQL injection attacks or whatever. Only server-side cleaning will do that.I think this post was promoted only because it mentions copilot, to be honest. It's not good security",Hacker News,https://news.ycombinator.com/item?id=27703052,-1
"Rather than a tool like Copilot, which does look very interesting, I'd be curious to see the effect of using evergreen (self-updating) off-the-shelf free open-source components.Imagine if you get get an app that uses ` and ` with ` and ` and you could automatically be using the latest rest API, the latest CRUD framework, the latest multifactor;whatever framework (which you'd probably pin to some standard like YubiKey so user tokens don't get invalidated regularly). The actual reference implementation could be done in pseudocode even, and ported to many languages (ok this is getting too meta). You could throw together a professional quality application in a few lines of package management. And if someone finds a bug in the code, it gets updated, and next time a release is cut you'll pull it down automatically and deploys itself.",Hacker News,https://news.ycombinator.com/item?id=27728704,0
"Does Copilot learn from you correcting things like this? The landing page says it learns your coding style, but presumably that's just formatting of the same suggestions. If it does something insecure and then you fix it, does it learn the secure way is the right way going forward? Or will it just autocomplete something equally insecure next time you try to do something similar?",Hacker News,https://news.ycombinator.com/item?id=27701638,0
"> The judge disagreed, however, on the grounds that the code suggested by Copilot was not identical enough to the developers' own copyright-protected work, and thus section 1202(b) did not apply.How did they reach this conclusion? How can you prove that it never copies a code snippet verbatim, versus just showing that it does for one specific code snippet? The latter is a lot easier to show, but I don't know what is it exactly that the prosecution claimed. I guess the size of the copy also matters in copyright violations?",Hacker News,https://news.ycombinator.com/item?id=40922922,9
"I'm surprised that so much of the discussion around Copilot has centered around licensing rather than this.You're basically asking a robot that stayed up all night reading a billion lines of questionable source code to go on a massive LSD trip and then use the resulting fever dream to fill in your for loops.Coming from a hardware background where you often spend 2-8x of your time and money on verification vs. on the actual design, it seems obvious to me that Copilot as implemented today will either not provide any value (best case), will be a net negative (middling case), or will be a net negative, but you won't realize that you've surrounded yourself with a minefield for a few years (worst case).Having an ""autocomplete"" that can suggest more lines of code isn't better, it's worse. You still have to read the result, figure out what it's doing, and figure out why it will or will not work. Figuring out that it won't work could be relatively straightforward, as it is today with normal ""here's a list of methods"" autocomplete. Or it could be spectacularly difficult, as it would be when Copilot decides to regurgitate ""fast inverse square root"" but with different constants. Do you really think you're going to be able to decipher and debug code like that repeatedly when you're tired? When it's a subtly broken block of code rather than a famous example?That Easter example looks horrific, but I can absolutely see a tired developer saying ""fuck it"" and committing it at the end of the day, fully intending to check it later, and then either forgetting or hoping that it won't be a problem rather than ruining the next morning by attempting to look at it again.I can't imagine ever using it, but I worry about new grads and junior developers thinking that they need to use crap like this because some thought leader praises it as the newest best practice. We already have too much modern development methodology bullshit that takes endless effort to stomp out, but this has the potential to be exceptionally disastrous.I can't help but think that the product itself must be a PSYOP-like attempt to gaslight the entire industry. It seems so obvious to me that people are going to commit more broken code via Copilot than ever before.",Hacker News,https://news.ycombinator.com/item?id=27813095,0
"I would hire copilot to write tests for me, that's about it. Writing tests can be a drag. It's really a low-risk proposition to have generated code attempt it. If it's a usable test, maybe it will catch a bug. If not, then kill it and let it generate a few more.The expectation is entirely different than producing code. Code needs to be correct, secure, performant, and readable. Failure on any of those fronts can be expensive to disastrous. Nobody can reasonably expect a test suite to catch every bug, even if created by the smartest humans. If a copilot-created test does prevent a bug from shipping it provides immediate value. I could see it coming up with some whacky-but-useful test cases that a sane person might not consider. From a training perspective I would think that assertion descriptions contain more consistent lexical value than the average function signature.It seems like the ambitious data scientists, product marketers, and managers fell in love with a revolutionary idea about AI writing code, and neglected to consult the engineers they are trying to ""Ã‹Å“augment'.",Hacker News,https://news.ycombinator.com/item?id=27810011,0
"ÃƒÂ¢Ã¢â‚¬Â°Ã‚Â¥ These three example pieces of flawed code did not require any cajoling; Copilot was happy to write them from straightforward requests for functional code. The inevitable conclusion is that Copilot can and will write security vulnerabilities on a regular basis, especially in memory-unsafe languages.If people can copy paste the most insecure code from Stack Overflow or random tutorials, they will absolutely use Copilot to ""write"" code and it will be become the default, especially since it's so incredibly easy to use. Also, it's just the first generation tool if it's kind, imagine what similar products will accomplish in 20 years.",Hacker News,https://news.ycombinator.com/item?id=27809487,0
"My biggest thing with Copilot is that it was trained on all public code on Github, which includes a lot of bad code that people just put up there (like my own code that I wrote a decade ago).As long as it is keeping track of when people do or do not accept their suggestions, it should get better over time. But in the meantime the best bet it is to treat it like a smart autocomplete, where you still have to at least check that it got it right.In the future maybe it will be smart enough to be treated like an intern -- trust that the code is right but still verify it yourself if the code is of any importance.",Hacker News,https://news.ycombinator.com/item?id=27811010,0
"Has anyone tried to generate private keys from copilot? GitHub suppressed these from search a while ago, but it would be an easy mistake to have included them in their training data.",Hacker News,https://news.ycombinator.com/item?id=27810370,6
"Julia Reda's analysis depends on the factual claim in this key passage:> In a few cases, Copilot also reproduces short snippets from the training datasets, according to GitHub's FAQ.> This line of reasoning is dangerous in two respects: On the one hand, it suggests that even reproducing the smallest excerpts of protected works constitutes copyright infringement. This is not the case. Such use is only relevant under copyright law if the excerpt used is in turn original and unique enough to reach the threshold of originality.That analysis may have been reasonable when the post was first written, but subsequent examples seem to show Copilot reproducing far more than the ""smallest excerpts"" of existing code. For example, the excerpt from the Quake source code[0] appears to easily meet the standard of originality.[0]:",Hacker News,https://news.ycombinator.com/item?id=27737138,9
"Well, I have a hard time drawing a line between GitHub Copilot and a compression algorithm.If you can reproduce a verbatim copy of Quake source code after taking that source code as input before then that's compression. A really fancy, but still.And given that it reproduces the source code: it has to hold that somewhere.It would be very interesting if someone could reproduce the Quake example with AGPL code, then request the whole model + code because it clearly contains the AGPL code in some encoded form.",Hacker News,https://news.ycombinator.com/item?id=27738592,3
"I've seen way too many screenshots of a dozen-line complete XHR wrappers being suggested[1] to complete a function to imagine Copilot as a generative machine. It's a somewhat fancy copy paste engine, with phenomenal search. But it's smuggled through enough complexity & machinery to obfuscate any legal obligations that might be attached to the original source material.The article does not set itself up to address this at all:> Since Copilot also uses the numerous GitHub repositories under copyleft licences such as the GPL as training material, some commentators accuse GitHub of copyright infringement, because Copilot itself is not released under a copyleft licence, but is to be offered as a paid service after a test phase.I'm all for discussion of whether Copilot itself has to be copyleft. But to me, the immediate concern is that Copilot seems like a way to take copyleft works and remove the copyleft license from those works.[1]",Hacker News,https://news.ycombinator.com/item?id=27738889,7
"Microsoft's major arguments:- as long as the reuse of code is transformative enough, they consider it fair use- yet Microsoft's Copilot is not trained on Microsoft codebases, and not on Windows source code- yet Microsoft files DMCA takedown requests on Windows source code leaksAs long as these are the case, Microsoft's argument of ""Everything is considered fair use, except our stuff"" is flawed in a lawsuit. That is not how copyright works.That is why Copilot has to be considered an attack on a specific group of software, and a specific group of people. And that is my own opinion.Either get rid of copyright alltogether (including Microsoft's arsenal of patents for it) or not. But don't try to play the hero of open source while you are stealing their stuff, specifically.",Hacker News,https://news.ycombinator.com/item?id=33489586,1
"I thought this should have been expected.Security starts with deep understanding.Some standards and practices can help avoid some types of problems, and some are even rather effective (like airgapping your systems), but there isn't any way to assure security in general other than truly understand what you are doing.**I feel like Copilot is the wrong direction to optimize development. This is mostly going to help people with already poor understanding of what they are doing create even more crap.For a good developer those low level, low engagement activities are not a problem (except maybe for learning stage where you actually want people engaged rather than copy;paste). What it does not help is the important parts of development -- defining domain of your problem, design good APIs and abstractions, understanding how everything works and fits together, understanding what your client needs, etc.Also, I feel this is going to help increase complexity by making more copies of same structures throughout the codebase.My working theory about this is this is going to hinder new developers even more than they already are by google and stack*. Every time you are giving new developers an easier way to copy paste code without understanding you are robbing them an opportunity to gain deeper understanding of what they are doing and in effect prevent them from learning and growing.It is a little bit like giving answers to your kids homework without giving them chance to arrive at the answer or explaining anything about it.**Another way I feel this is going to hurt developers is competition in who can produce most volume of code.I have already noticed this trend where developers (especially more junior but aspiring to advance) try to outcompete others by producing more code, close more tickets, etc. Right now it means skipping understanding of what is going on in favor of getting easy answers from the Internet.These guys can produce huge amounts of code with relatively little actual engagement.To management (especially with wrong incentives) this seems like a perfect worker, because management usually doesn't understand the connection between lack of engagement and planning at design;development time with their later problems (or they don't feel it is them that is going to pay the price).The Copilot is probably going to make it even more difficult for people who want to do it the right way because even starker difference in false productivity measurements.",Hacker News,https://news.ycombinator.com/item?id=28402677,0
"You are the free labor copilot to train Microsoft GitHub's Copilot tool. You are responsible for any of those insecure code errors and the diligence require. You will be on the hook for resulting problems. But Microsoft and their home-phoning, tracking-embedded editor will get real people to correct and train their machine for free""Ã¢â‚¬Âwith their stated plan of later selling that machine back to us later.I wish there were a ""robots.txt""Ã‚Â file for Git to disallow certain bots from training on anything I have written.",Hacker News,https://news.ycombinator.com/item?id=28402787,-1
"I use Copilot mostly as replacement for intellisense and macros. It helps me automating repetitive tasks. I would never trust Copilot for an algorithm or a snippet, I mean I would treat the code just like anything taken from StackOverflow or Github.",Hacker News,https://news.ycombinator.com/item?id=28402824,0
"Half joking:so far GitHub Copilot is more feasible as tool for humans doing code-coverage for its input code, ""given enough eyeballs, all bugs are shallow"" style. When a developer goes, ""huh, Copilot generated insecure code, better report it to the original project it learned it from"" - if only Copilot was able to link to the original project, it would all be great and useful.",Hacker News,https://news.ycombinator.com/item?id=28405516,3
"Of course it did, why would github copilot ""care about""Ã‚Â security, unless the majority of code on github cared about security?",Hacker News,https://news.ycombinator.com/item?id=28406826,3
"So, I'm reading the linked article by RMS about Service as a Software Substitute (SaaSS) [1] which is one of the reasons why they object against GitHub Copilot.The key argument why as SaaSS is ethically wrong is because it denies control over a computation that I could do on my own.> ""The clearest example is a translation service, which translates (say) English text into Spanish text. Translating a text for you is computing that is purely yours. You could do it by running a program on your own computer, if only you had the right program. (To be ethical, that program should be free.) The translation service substitutes for that program, so it is Service as a Software Substitute, or SaaSS. Since it denies you control over your computing, it does you wrong. (emphasis mine)""I don't find that argument very convincing because it implicitly assumes that there is no alternative translation program that I could run on my own computer.However, if there is an alternative, then a SaaS offers me choice. I can run a program on my own computer, e.g., if I am concerned about data privacy, or service reliability. The downside is that I have to install and maintain the software on my computer. Or, I could use an external service. The upside is that the barriers of use are minimal.Of all the articles by RMS I have read so far, I find this one the least convincing.[1]",Hacker News,https://news.ycombinator.com/item?id=28047996,2
"I think Copilot is a hard problem, maybe it isn't even solvable.Sometimes it blatantly copies GPL code without my knowledge.Sometimes I myself write code that could be part of a GPL code-base, without knowing.Funny thing is, the difference here isn't the actual code that's written, but that Copilot has seen many GPL code bases and I didn't.Sometimes I really have the feeling Copilot understands my code base and suggests code that seems to be custom tailored to it. Albeit in most of the cases it doesn't fit 100%.I think the latter cases are when Copilot shines and doesn't violate GPL code at all, but can I be safe? Probably never.",Hacker News,https://news.ycombinator.com/item?id=28047838,5
"Here's an exceprt from the linked FSF blog article: Areas of interest While any topic related to Copilot's effect on free software may be in scope, the following questions are of particular interest: - Is Copilot's training on public repositories infringing copyright? Is it fair use? - How likely is the output of Copilot to generate actionable claims of violations on GPL-licensed works? - How can developers ensure that any code to which they hold the copyright is protected against violations generated by Copilot? - Is there a way for developers using Copilot to comply with free software licenses like the GPL? - If Copilot learns from AGPL-covered code, is Copilot infringing the AGPL? - If Copilot generates code which does give rise to a violation of a free software licensed work, how can this violation be discovered by the copyright holder on the underlying work? - Is a trained artificial intelligence (AI) ; machine learning (ML) model resulting from machine learning a compiled version of the training data, or is it something else, like source code that users can modify by doing further training? - Is the Copilot trained AI;ML model copyrighted? If so, who holds that copyright? - Should ethical advocacy organizations like the FSF argue for change in copyright law relevant to these questions? While i do believe that the topic is definitely worthy of discussion, my question would be a bit different.If the tooling is already pretty capable, wouldn't just ignoring all of the ethical questions lead to having a market advantage? Say, some company doesn't necessarily care about how the tool was trained and the implications of that, but just utilize it to have their developers write software at a 1.25x the speed of competition, knowing that noone will ever examine their SaaS codebase and won't care about license compliance. Wouldn't that mean that they'd also be more likely to beat their competition to market? Ergo, wouldn't NOT using Codepilot or tools like Tabnine put most others at a disadvantage?Personally, i just see that as the logical and unavoidable progression of development tooling, the other issues notwithstanding, very much like IDEs did become commonplace with their refactoring tooling and autocomplete.I've worked with Visual Studio Code on large Java codebases, as i've also used Eclipse, NetBeans and in the past few years IntelliJ IDEA; with every next tool i found that my productivity increased bunches. Now it's to a point where the IDE suggests not only a variety of fixes for the code itself, but also the tooling, such as installing Maven dependencies, adding new Spring configurations and so on. It would be hard to imagine going back to doing things manually and it feels like in time it'll be very much the same way in regards to the language syntax or looking at documentation for trivial things. After all, i'm paid to solve problems, not sit around and ponder how to initialize some library.",Hacker News,https://news.ycombinator.com/item?id=28047124,9
"The judge argues that copilot ""rarely emits memorised code in benign situations""Ã‚Â, but what happens when it does? It is bound to happen some day, and when it does would I be breaching copyright by publishing the code copilot wrote? Just a few weeks ago a very similar suit for stable diffusion had its motion to dismiss copyright infringement claims denied.",Hacker News,https://news.ycombinator.com/item?id=41380759,-1
Not really no-code. Let's be honest. The OP is taking steps just like how an experienced SW developer would. Copilot simply cut the need for reading through documentations. This doesn't really say that Copilot can replace programmers.p.s. Does anyone know when Copilot will update the insecure example on their website? Or are they just trying to be honest with the possible quality issues with the generated code?,Hacker News,https://news.ycombinator.com/item?id=30799358,0
"Unintentional copyright violations and ""leaking""Ã‚Â of secrets people accidentally committed to public repos aside, my main issue with Copilot is that I don't think it actually makes coding easier.Everyone knows it's usually far easier to write code than to read code. Writing code is a nonlinear process: you don't start from the first character and write everything out in one single pass. Instead, the logic of the code evolves nonlinearly""Ã¢â‚¬Âadd a bit here, remove a bit there, restructure a bit over there. Good code is written such that it can be mostly understood in a single pass, but this is not always possible. For example, understanding function calls requires jumping around the code to where the function is defined (and often deeper down the stack). Understanding a conditional with multiple branches requires first reading all the conditional predicates before reading the code blocks they lead to.Reading, on the other hand, is naturally a linear process. Understanding code requires reconstructing the nonlinear flow though it, and the nonlinear thought process used to write it in the first place. This is why constant communication between partners during pair programming is essential""Ã¢â‚¬Âif too much unexplained code gets dumped on a partner, figuring out how it works takes longer than just writing it themself.Copilot is like pair programming with a completely incommunicative partner who can'twalk you through the code they just wrote. You therefore still have to review most of it manually, which takes much longer than writing it yourself in the first place.",Hacker News,https://news.ycombinator.com/item?id=27737258,0
"> SendGrid engineer reports API keys generated by the AI are not only valid but still functional.> GitHub CEO acknowledges the issue... still waiting for them to pull the plugI agree this is an issue for co-pilot as well - but it's really on send grid to invalidate keys that are known to be leaked?Yes, that's inconvenient for the affected customers - otoh they won't get billed for other people's usage - or dinged for someone spamming using their keys...",Hacker News,https://news.ycombinator.com/item?id=27737141,6
"I see this as a problem with the developers who are committing code, and not a problem with Copilot. if you make your secrets accessible then they might be accessed. Also if you are rotating your keys regularly that would also mitigate these issues. This is a problem with humans failing to execute known security best practices, not malicious AI doing something insidious.",Hacker News,https://news.ycombinator.com/item?id=27737910,6
"If Copilot was trained only on public repos like they claim, then shouldn't those API keys already be disabled due to existing secret scanning tools?For example fact that Copilot recreates API keys that still work makes me wonder if they come from a semi-public place, because SendGrid is usually quite fast at blocking API keys that were accidentally made public.",Hacker News,https://news.ycombinator.com/item?id=27737896,6
COPILOT SECURITY BREACH> SendGrid engineer reports API keys generated by the AI are not only valid but still functional.> GitHub CEO acknowledges the issue... still waiting for them to pull the plug or make a comment. :popcorn:Quoting,Hacker News,https://news.ycombinator.com/item?id=27740468,-1
"So GitHub Copilot has inherited all the bad practices of many StackOverFlow and GitHub side projects and generates them in front of you as 'assistance'.All the API keys are still working and who knows, someone might complain about a huge fee right in here because they forgot to revoke it. Only time will tell.I am certainly going to avoid this contraption. No thanks and most certainly no deal.Downvoters: So are you saying GitHub Copilot DOES NOT do the following: Leak working API keys in the editor. Generate broken code AND give you the wrong implementation if you add a single typo? Copy and regurgitates copyrighted code verbatim. Guesses 1 out of 10 tries. Send parts of your code when you type in the editor. Are you VERY sure?",Hacker News,https://news.ycombinator.com/item?id=27736729,3
"The structural completions are way more useful than the entire function completions, even in IntelliJ, where autocomplete is already extremely high quality.The part that I find unsettling when using Copilot is the risk that credentials or secrets embedded in the code, or being edited in (.gitignore'd) config files, are being sent off to Microsoft for AI-munging and possible human review for improvements to the model.",Hacker News,https://news.ycombinator.com/item?id=33644152,0
"[Warning: long comment ahead]I haven't used Copilot yet (I'm not coding in any of the languages it covers) but I've tried to understand it from the few articles (most critical) posted about it on the internet and from my understanding of program synthesis and languae modelling. I don't fully trust lay users to be able to exercise a machine learning model comprehensively enough to precisely understand its pros and cons, but I think I've formed some intuition about how Copilot works from the demonstrations I've seen. Here's my thoughts:a) As TFA points out, Copilot was trained on _all_ code on github (for select languages). ""All"" includes good code and bad code, code with bugs and code with, er, fewer bugs, code that does what it should and code that doesn't. The result is that Copilot models _all_ that code, regardless of whether it's got bugs or not.b) Copilot is (based on) a language model that predicts the next sequence of tokens, not a discriminator of good;bad code. That means it can generate code but it can't generate ""correct"" code. It doesn't have a concept of correct;incorrect code.b.1) More generally, it's impossible to discriminate between ""correct"" and ""incorrect"" code from features (tokens, in the case of language models) alone. The correctness of code depends on the programer's intent. In program synthesis, the programmer's intent is encoded as a specification and the correctness of a synthesised program must be considered in the context of that specification. A language model could be trained to include a specification as a feature, or as an additional modality (similar to training on images and text together to generate captions for images, or images from captions). However, this is not how Copilot was trained. The upshot of all this is that Copilot can't generate ""correct programs"" because it doesn't know what ""correct programs"" are.b.2) But wait! Copilot can generate code from a specification! From a natural language specification, in the form of inline comments given as part of a prompt. This seems to be a favourite mode of testing Copilot and its results are impressive, but there are two problems: i) natural language specifications are imprecise and get more so the longer they grow; and, ii) Copilot still has no way to determine whether the code it generates satisfies a natural language specification, or not.b.3) For program synthesis with guarantees of correctness, a specification is not enough: a verification procedure is also necessary -and those tend to be very tricky because Halting Problem.c) Copilot has memorised at least part of its training set. This is evident in the demonstrations of verbatim generation of known code, e.g. the Inverse Square Root function (from Quake... III Arena I think?) etc.c.1) This observation is helpful in understanding language models in general. The assumption is often stated that a model can't possibly be memorising its training set because models are smaller on disk than their training sets. And yet it seems that large language models are memorising parts of their training sets- most likely, the ones that are more common, and therefore most likely.c.2) This in turn means that the programs that Copilot is most likely to generate are variations of programs included in its training set and that have been memorised. Why is this most likely? Because those programs are themselves ""most likely"" - they are programs with the highest prior probability in the space of programs represented by Copilot's model.c.3.1) That said, Copilot doesn't always generate memorised programs verbatim. It clearly has the ability to ""mix and match"" and generate programs that are not included in its training set. How does Copilot (or language models in general) achieve that? A language model is essentially a representation of a dense region of Cartesian space bounded by the elements (tokens, expressions, whole programs) of the training set. Anything within that dense geometric region, a language model can recognise and generate. Anything outside it, is down to luck.c.3.2) To generate programs not included in its training set, Copilot interpolates between the programs in its training set and other programs in its model (""interpolates"" as in ""finds a point on a gradient between two programs' representations""). However, this interpolation is stochastic. Fudging very, very much, we can imagine Copilot traversing the AST of a memorised program and applying slight peturbations to that AST, at random, though according to its learned prior probabilities. This is why Copilot will generate different code each time it's run even with the same prompt, and the generated code is closer or farther away from memorised code according to its ""temperature"" parameter (that controls the randomness of ""perturbations"").c.4) Suppose you take some program P and walk down its AST, perturbing the nodes at random, without having any way to know whether a perturbation results in correct code, incorrect code, or pure garbage. What is the expected result? The expected result is correct code, incorrect code, or garbage. Remember: Copilot can't tell which is which. So we don't know which we'll get.d) All that said, I think that i) Copilot can make a great boilerplate generator but ii) only if people learn to use it right. So far I think most demonstrations I've seen are ""holding it wrong"".d.1) As to ii) above, I think the best use-case of Copilot is to generate short code snippets, spanning at most a couple of lines, rather than entire functions. I think it's also going to work a lot better if it's allowed to complete code rather than comments (with code). See point b.2 above. The demonstrations I've seen so far seem to try to get Copilot to act as an Autopilot (sorry, couldn't resist). In that, it fails in the ways it should be expected to fail . I suspect that as users get more familiar with Copilot's abilities the best way to use it will become evident and Copilot will take its rightful place as a useful tool in the programmer's toolbox that can simplify, but not replace, day-to-day programming work.e) Rumours of the imminent demise of the programming profession have, of course, been greately exaggerated. Wait another couple of generations, maybe. Human generations.f) If you're interested in program synthesis and the progress that has been achieved so far in this old, but still active field of research, the following is a good recent survey of the field:",Hacker News,https://news.ycombinator.com/item?id=27893382,0
"I've been using Copilot for a few months and...Yeah, it makes mistakes, sometimes it shows you i.e. the most common way to do something, even if that way has a bug in it.Yes, sometimes it writes a complete blunder.And yes again, sometimes there are very subtle logical mistakes in the code it proposes.But overall? It's been great! Definitely worth the 10 bucks a month (especially with a developer salary). :insert shut up and take my money gif:It's excellent for quickly writing slightly repetitive test cases; it's great as an autocomplete on steroids that completes entire lines + fills in all arguments, instead of just a single identifier; it's great for quickly writing nice contextual error messages (especially useful for Go developers and the constant errors.Wrap, Copilot is really good at writing meaningful error messages there); and it's also great for technical documentation, as it's able to autocomplete markdown (and it does it surprisingly well).Overall, I definitely wouldn't want to go back to writing code without it. It just takes care of most of the mundane and obvious code for you, so you can take care of the interesting bits. It's like having the stereotypical ""intern"" as an associate built-in to your editor.And sometimes, fairly rarely, but it happens, it's just surprising how good of a suggestion it can make.It's also ridiculously flexible. When I start writing graphs in ASCII (cause I'm just quickly writing something down in a scratch file) it'll actually understand what I'm doing and start autocompleting textual nodes in that ASCII graph.",Hacker News,https://news.ycombinator.com/item?id=31826951,0
"On the one hand, This is another clever pricing scam. Which Microsoft wins either way.Unless you want to pay for your IDE + Copilot with JetBrains, this will still benefit only Visual Studio Code, which Microsoft knows that you cannot beat free. So this a great resurgence of 'Embrace' with free developer tools.On the other hand, Co-pilot is going to probably ruin the Leetcode, Hackerrank, Codility candidate as a candidate can Co-pilot the solution if not checked properly.",Hacker News,https://news.ycombinator.com/item?id=31826190,1
"I wonder how long until state actors realize that copilot would be a perfect vector for getting developers to introduce subtle vulnerabilities into their own projects?By its very structure it's output always looks credible, and it's not always right-- it wouldn't be a sign of foul play if copilot suggested some code that looked just right but happened to backdoor your cryptosystem or protocol.Maybe it would be a little tricky to get it to produce NOBUS vulnerabilities that were credible mistakes, but if the target isn't OSS then nobus isn't really that important.",Hacker News,https://news.ycombinator.com/item?id=31829680,0
"This is double risk:1. Security vulnerabilities from unaudited code.2. Copyright violation risk, since a lot of CoPilot examples show clear excerpts from open source repositories.",Hacker News,https://news.ycombinator.com/item?id=35170015,-1
"I look forward to anything better than Github Copilot. The implementation in VSCode is horribly slow, constantly injects a lightning bolt icon near my cursor, and interferes with intellisense-style LSP language hints;docs.I've found it mostly helpful for saving time on obvious boilerplate code, but the annoyances above plus the occasional inexplicable errors it introduces in said boilerplate code, I've just cancelled the entire thing.",Hacker News,https://news.ycombinator.com/item?id=40152959,0
"I've experimented a bit with this on the raw Codex model ( and I've found that some prompt engineering can be helpful: explicitly telling the model to generate secure code in the prompt sometimes helps. (such as by adding to the prompt something like ""Here's a PHP script I wrote that follows security best practices""). Codex knows how to write more secure code, but without the right prompting it tends to write insecure code (because it was trained on a lot of bad code).> the settings and documentation as provided do not allow users to see what these are set to by defaultThere isn't a single default value. Those parameters are chosen dynamically (on the client side): when doing more sampling with a higher top_p a higher temperature is used. I haven't tracked down where the top_p value is decided upon, but I think it depends on the context: I believe explicitly requesting an completion causes a higher top_p and a more capable model (earhart), which gives better but slower results than the completions you get as autocomplete (which are from the cushman model with a lower top_p). Copilot doesn't use any server-side magic, all the Copilot servers do is replace the GitHub authentication token with an OpenAI API key and forward the request to the OpenAI API.",Hacker News,https://news.ycombinator.com/item?id=28281333,0
"Summary: CONCLUSIONS AND FUTURE WORKThere is no question that next-generation ""Ã‹Å“auto-complete' tools like GitHub Copilot will increase the productivity of software developers. However, while Copilot can rapidly generate prodigious amounts of code, our conclusions reveal that developers should remain vigilant (""Ã‹Å“awake') when using Copilot as a co-pilot. Ideally, Copilot should be paired with appropriate security-aware tooling during both training and generation to minimize the risk of introducing security vulnerabilities. While our study provides new insights into its behavior in response to security-relevant scenarios, future work should investigate other aspects, including adversarial approaches for security-enhanced training",Hacker News,https://news.ycombinator.com/item?id=28279987,0
"tl;dr they tested GitHub Copilot against 89 risky coding scenarios and found about 40% of the roughly 1,700 sample implementations Copilot generated in the test were vulnerable (which makes sense given it's trained on public GitHub repos, many of which contain sample code that's a nightmare from a security perspective).",Hacker News,https://news.ycombinator.com/item?id=28280041,3
"This extends so much further than just GitHub and open source. Almost all ""AI""Ã‚Â based generative tools are trained on public datasets, in some cases they have licensed these datasets, but my understanding is that they are often just trained on the ""public""Ã‚Â internet. There is currently clear legal protection of scraping, however you can't""republish""Ã‚Â that data, only significant derivative work. I don't know if these tools fall into that category - IANAL - but that's the billion dollar question. The text;language and image generation are all capable of outputting results that are clearly based on particular input imagery.This is such an important debate to have and my fear is that the companies investing in this tech have deep pockets, the lobbying around it could result in a unbalanced outcome. This isn't really an area where ""move fast and break things""Ã‚Â is necessarily the right process.Exciting though to see where this technology could lead, co-pilot blows my mind at times.",Hacker News,https://news.ycombinator.com/item?id=31874236,4
"What I think it really comes down to, is whether copilot-like tech becomes a 2x (or even 10x) productivity multiplier.If it becomes that powerful, we'll ignore these and other concerns. Just imagine Chinese or Russian devs moving 2x or 10x faster because they have no regard for our privileged concerns. Don't even have to go that far, a competitor is using GPT-17 and you're not - good luck!And if it stagnates and never goes far beyond the current capabilities, then sure, it might become fashionable to try and hunt for copyright infringements caused by Copilot.",Hacker News,https://news.ycombinator.com/item?id=31874684,-1
"Microsoft just stole all the code on github to do this. Regardless of what the minutiae of the law say, no one really expected their work to be used this way. Open source code powers a huge chunk of the industry while capturing little value for the maintainers already. Github even explicitly supports a standard format for declaring the license of a repo, which was cleverly ignored.Here is the relevant section from Githubs privacy policy [1]> 6. Contributions Under Repository License> Whenever you add Content to a repository containing notice of a license, you license that Content under the same terms, and you agree that you have the right to license that Content under those terms. If you have a separate agreement to license that Content under different terms, such as a contributor license agreement, that agreement will supersede.From GPLv2, ""When distributing derived works, the source code of the work must be made available under the same license.""------This is not about technology, it is a legal endrun around using open source code without open sourcing derived work. It is using AI as a form of ""license laundering"".""OpenAI"" is not open at all. Truly open AI means the code, the data and the model are all open. OpenAI sold the source to GPT-3 to Microsoft, received $1 billion from them in 2019 and does not make most of their work available except behind a highly exclusive, paid API - Its a joke to call that ""open"". I urge you to read up on OpenAI and look at what the have actually done.Their plan in the future is to sell access to Copilot, directly monetizing work they stole from others for free:> According to GitHub, ""If the technical preview is successful, our plan is to build a commercial version of GitHub Copilot in the future.""Ã‚ÂI've deleted all my code from github and hope others do the same. Maybe if some bigger profile project starts doing this, we can start to organize around opposing Pilot and OpenAI.Others have also pointed out similar concerns - see for example.[1]",Hacker News,https://news.ycombinator.com/item?id=27873384,2
Will IntelliSense now provide bounds for the CoPilot predictions? I find Copilot hallucinates method signatures that don't actually exist in the SDK. IntelliSense of course knows that.,Hacker News,https://news.ycombinator.com/item?id=36462171,0
There is a real need for offline co-pilot alternative.Privacy concerns due to copilot makes it unusable for companies with closed source code.I'm hoping that eventually there will be a stable diffusion level solution in this field.,Hacker News,https://news.ycombinator.com/item?id=32757030,1
"I read the article and I have some skepticism. I think my skepticism is well-founded but it may well be the case that a machine will one day do my job. I don't believe that time is at hand, though.First off, I don't see a link to the ""HTTP server in JavaScript"" task. It's really hard for me to place much faith in their conclusions when it's not even clear what the problem definition was.Second, I believe that a lot of more senior developers and development managers who take secure development practices somewhat seriously will not be able or willing to use Copilot in any sort of proprietary setting. Here is a quote from the Copilot FAQ:> [...] The GitHub Copilot extension sends your comments and code to the GitHub Copilot service, and it relies on context, as described in Privacy below - i.e., file content both in the file you are editing, as well as neighboring or related files within a project. It may also collect the URLs of repositories or file paths to identify relevant context. The comments and code along with context are then used by OpenAI Codex to synthesize and suggest individual lines and whole functions.- from - see ""How does GitHub Copilot work?""I believe this makes it simply a nonstarter in a lot of environments. I am wondering if there are a number of places that have restrictions on sharing their code with a third-party but don't know or don't care and so end up using Copilot anyway. I believe that short-sighted thinking like this is more prevalent in shops that have low-quality code, and I believe that the higher-quality the code, the less likely someone is to use Copilot, simply for the ""I can't share my code, even if I use the most restrictive no-telemetry settings"" reason. Give me a self-hosted Copilot, and I may try it out in anger.Finally, I based some of my thinking on a recent Reddit ;r;programming discussion of Copilot: reading those posts, and internalizing them with my own view of coding, I believe Copilot is not ready for my personal use. Again: licensing considerations aside (if you actually can feel comfortable putting them aside, see NoraCodes comment in this HN thread e.g.), it is simply a non-starter for anything proprietary in nature. I am also of the mind that any code that is of necessity very tedious to write is in dire need of real attention, most likely in the form of tests and quite possibly refactoring to reduce the boilerplate if at all possible. I believe in the value of linters and automated code analysis tools and in continuous integration that runs after every commit. Give me a self-hosted Copilot, and we'll have a real chance to see how it works out - until then it's not going to be a boon to programmers.",Hacker News,https://news.ycombinator.com/item?id=32754652,-1
"I just experimented with the usernames in that video. Most of theme exist (except for ""v2v3"").But a lot have barely any repos. There is even a ""ferran"" who doesn't have anything (public) on GitHub.So, my guess is that Copilot stored a lot of GitHub accounts, and when we type it autocompletes with any random ones from that list.There is no relation with the code that it generates.",Hacker News,https://news.ycombinator.com/item?id=33171356,3
"So far the most WTF thing I've gotten out of it is: import base64 test = base64.b64decode(""""""SSdtIGtpbGxpbmcgeW91ciBicmFpbiBsaWtlIGEgcG9pc29ub3VzIG11c2hyb29t"""""".encode()) print(test) # b""I'm killing your brain like a poisonous mushroom"" And the most odd thing: # The base URL for all API requests base_url = ' # The base URL for all non-API requests (e.g. static content) base_url_static = ' Which are URLs that haven't been a thing for 2 years, I think and I can't find any code in github that uses them still.",Hacker News,https://news.ycombinator.com/item?id=27740040,3
"> Microsoft GitHub is the largest collection of open source code in the world. Microsoft GitHub is in a unique and dominant positions to host and access and distribute most of the open-source code in the worldNo, it's not in a ""unique and dominant position"". Open source code is freely available online, it's almost trivial to build a bot to scrape OS code from anywhere on the web (GitHub included).The comparison to the Google Books antitrust falls down completely, Google had a dominant position because it had the resources to scan all books. Anyone can build a collection of almost all open source code.Further to that, all these models (GPT and Image generation) are trained on scraped data, trying to suggest that only GitHub;Microsoft could do it defeats the purpose of trying to establish what the legal rights are over training models with scraped data.We need test cases and precedent, but trying to use this as one is not going to work.Edit:It took me 15 seconds to find that there is a Google Big Query dataset of open source code for GitHub: thats been further curated on Hugging Face: ; Microsoft do not have a monopoly on this data.",Hacker News,https://news.ycombinator.com/item?id=35959862,2
"Historically, Microsoft -- itself an entity plagued with anti-trust sentiment in the past -- slagged the GPL in public for years, but was unable to do anything about its ascent and propagation.Now they may have found a way. And that I think is the potential anti-trust issue here.What is one of the main obstacles to Microsoft's monopoly dominance in the software sphere? The Linux kernel, it's everywhere. And it's under the GPL, a license explicitly resistant to ""Embrace, extend, and extinguish"" (old school Gates;Balmer MS). Microsoft right now is not emphasizing an anti-Linux, anti-GPL focus, but it clearly has in the past and it (and others) could definitely do so again in the future.Systems like CoPilot have the potential to be for the GPL (or other copyleft type licenses) what cryptocurrency 'mixers' or 'tumblers' are to money laundering laws. A potential to be an automated way to pull pieces of IP out of those licenses and into other codebases without respecting the obligations that go with it.A lot of the dialog on here and other threads on this forum in the past shows me that understanding of copyleft licenses among the open source and developer community is really low right now. This is the license that the Linux kernel is licensed under, it is extremely important. There should be better recognition of the rights and responsibilities afforded by it.The GPL was explicitly formulated as a way to protect portions of the hobbyist and free software community from potentially predatory commercial interests. Remember it's always possible to attempt to negotiate a commercial non-copyleft license with an entity that has released its source under the GPL. But if you don't, you have to respect its distribution requirements. It's fine to be personally opposed to using the GPL for your own work, but it is important to understand the obligations that come with it. And that includes systems that harvest data from it automatically.",Hacker News,https://news.ycombinator.com/item?id=35961353,1
"There kinda is, just not an antitrust claim.[1]",Hacker News,https://news.ycombinator.com/item?id=35959906,1
"Because they'll (Microsoft and Github) ultimately be crushed by copyright infringement claims (even the most liberal oss licenses require attribution, which no one seems to be accommodating)? Why bother mounting an expensive antitrust campaign when The Mother of All DMCA takedowns is on its way?",Hacker News,https://news.ycombinator.com/item?id=35961508,2
"Antitrust is one thing, but by cleanroom implementation standards (one team reads the source and writes a spec, another team writes the code) CoPilot is illegal to begin with.CoPilot reads and rearranges the IP that was created by millions of people who were working very hard and did not anticipate a code laundering machine when they wrote the code and the licenses.",Hacker News,https://news.ycombinator.com/item?id=35960347,-1
"I didn't find Copilot all that useful. The only time I got any use from it was using a new library. It kind of worked for sketching out an initial attempt at using the library, at least good enough for me to get started. The code it generated was still wrong, however. For this reason, it's actually harmful if I know what I'm doing.I've had more luck with ChatGPT for programming help, again in situations where I was working on something unfamiliar but needed more context than provided by Copilot. Prompts like ""write a Vulkan compute shader to find the max of a list of numbers""Ã‚Â definitely got me started faster than I would be able to do on my own. Still, it's not something I would consider fundamentally game changing.",Hacker News,https://news.ycombinator.com/item?id=37304023,0
"Yeah I'll pay for something that is just slightly better;faster than writing it myself but will practically speaking steal my code and give it to others that pay for the same product. ;sI'd say no thanks. I think programmers using Copilot is paying for something that'll hurt them in the long run for a tiny benefit in the short run.I don't trust Microsoft, and neither should you.",Hacker News,https://news.ycombinator.com/item?id=34797469,1
"I am whiny about being anti-AI, but no doubt they are well-suited for Englishprogramming translation tasks, which along with a well-tuned bag of tricks is a useful tool.But two issues the tech community had not spent nearly enough effort discussing:1) As a big fan of Idris, I am worried that these tools will strongly disincentivize language development: why design an elegant language if an LLM can write the boilerplate faster than you can write a cleaner implementation?2) I still don't think these tools are even slightly ethical. In 2022 I kicked the tires on ChatGPT-3.5 for F# codegen, and got some truly terrible results. I copy-pasted some lines into GitHub and found the unique repositories which ChatGPT was obviously plagiarizing from, and with 15 seconds of prompt ""engineering"" I got it to spit out ~200 lines verbatim from my personal F# linear algebra library - the only thing that was changed was stripping out the comments and updating some syntax to F# 4.7. Pure plagiarism. It is especially frustrating that GPT is more likely to plagiarize that library precisely because there aren't very many similar repos on GitHub.Obviously the plagiarism problem can be fixed. (and it seemingly has been...for F#. Not sure about Idris!) However, it really seems like that sort of RLHF fine-tuning is about covering OpenAI's tracks, not ""teaching"" the AI how to ""generalize."" In particular I refuse to use the tool because now instead of reliably getting it to plagiarize from F# developers, I have no clue whatsoever if it's stealing or if it managed to truly autoregress its way into an ethical solution. So instead of rolling the dice on being a graceless scumbag, I'll just take my time writing out my code by hand.And it was striking that GPT-3.5 had read and memorized more F# than Don Syme has seen in his entire life, yet in response to simple questions it was a mindless plagiarist. It's a stark illustration why the legal argument that ANN learning = human learning is vacuous, and why OpenAI should lose most of the copyright lawsuits it's facing.",Hacker News,https://news.ycombinator.com/item?id=41341822,0
"I think it's too early to tell but my main concern about copilot is code maintainability and security. Copilot is able to barf out helpful expressions that will reduce the amount of code we need to write by hand - I think it's excellent when it comes to reducing boilerplate... but I think a large amount of boilerplate existing belies a bigger issue with the project. The majority of software engineering isn't writing code - copilot may be beneficial as an accessibility aide for developers that have typing impairments but most developers can type faster than they can think - if the level of boilerplate in your project is reasonable then this should mean you're never prevented from thinking because your fingers are still working on recording your previous thought. However, at the end of the day, if you can help reduce carpal tunnel that's still a win.The problem I can foresee with copilot is that the scenario change you're agreeing to is that you'll type less but need to read over the code produced more - this is an effort that isn't normally necessary (typos happen but those should take a trivial time to correct) but when copilot is involved you need to proof all the code that is being generated. There is a motivation to skip this step and just accept the code was written correctly and that will inevitably lead to security problems - and there is a motivation to not correct or alter auto-filled command. If there's a multi-dimensional array and you think it semantically makes sense to iterate it over dimension a then dimension b and copilot instead goes with b as the major index then it's more likely to remain in a b major iteration - that may make code less readable or it may cause major issues down the line.Copilot, IMO, is optimizing the least important part of development right now and it costs us more to correct it then it would to just splat out the correct code _but_ this is a similar argument to longbows vs. crossbows - hand a peasant a crossbow and they can fire a crossbow - train a peasant for 30 years and they can fire a longbow - the longbow is more powerful, but the crossbow is a clear choice in terms of RoI. It may be that today's developers will only benefit from copilot minimally since we've invested the training time in standard development practices but tomorrow's developers will eschew a lot of the algorithmic learning and still be able to deliver the majority of the value.",Hacker News,https://news.ycombinator.com/item?id=39567741,0
"N=1 but I find Copilot to be incredibly valuable to me.So much so that when there was an outage a few weeks ago (or maybe I had network issues on my side) the loss of it was palpable. I found myself pausing, waiting for Copilot to spit out code only to realize it wasn't going to do it. Once you've used Copilot for a while you get a good sense of what it can and can't do. When to pause and when to just keep typing. I was so used to knowing ""this is a thing Copilot will do well"" and waiting for it that I kept forgetting it wasn't working right now.Even before this experience I was convinced of the usefulness of it. I've been writing code for close to 20 years and I think I'm pretty decent at it, I never take Copilot's suggestion without first understanding what it's doing but more often than not the suggestion is almost identical to what I would have written myself. Sometimes it wants to do a `.forEach` and I would prefer a `for()` loop but that's easy to fix and often writing `for(` is enough for it to re-write that part of code in the way I prefer. Those changes are often only stylistic.In addition, it's great for code I don't write often but need something quick and dirty to test out a POC. It along with ChatGPT feel like cheating. Just yesterday we were looking into an issue where I work. We had some timing data in the logs but nothing was consuming;displaying that data. Yes, we could grep for the lines of data but we didn't have this feeding into prometheus and the effort to do that was not going to be minor.Instead I had ChatGPT parse the log lines I had already filtered with grep and spit out CSV data (""Datetime, how many seconds something took"") then I had it write an extremely basic HTML;CSS;JS file to graph the data. After checking that it was all working I hooked up the command directly in the php file that held the graph so we had ""live"" graphing (after a reload) of a problem we were investigating. This whole thing took well under 5 minutes.Now I'm perfectly capable of doing everything I just outlined above but it would have taken me longer than 5 minutes just to look up and use the ChartJS syntax;api. Instead I had a tool displaying near-live data in almost no time at all.""AI"" feels like a superpower. I already know what I want to do and often I even know how to write the code to do it but LLMs let me skip the repetitive boring parts and focus on the things LLMs are not good at, my specific problem space, the specifics of my stack, etc. Only I can do that (for now at least), let the LLM spit out graphs, loops, awk commands, etc, I'll glue it all together and make it useful.",Hacker News,https://news.ycombinator.com/item?id=39572038,0
"I find the proposed solutions in the article quite sensible and fair:> Allow GitHub users and repositories to opt-out of being incorporated into the model. Better, allow them to opt-in. Do not tie this flag into unrelated projects like Software Heritage and the Internet Archive.> Track the software licenses which are incorporated into the model and inform users of their obligations with respect to those licenses.> Remove copyleft code from the model entirely, unless you want to make the model and its support code free software as well.About the last point: Many of the larger corporations avoid copyleft code entirely or only use it in ways that avoids the viral nature of them. So I have to assume that Copilot would also be avoided for the same reasons by the same corporations until they can opt out.Free software is already being exploited and violated in various ways by various actors and Copilot seems to be a tool that makes this even easier. I think it will be interesting to see where this issue goes.",Hacker News,https://news.ycombinator.com/item?id=31849022,-1
"After seeing the demos my biggest question is: how will this not lead to people just accepting whatever GitHub Copilot suggests while introducing subtle yet catastrophic errors? Basically, how is this not going to become an alternative to just copy-pasting StackOverflow answers without verification? Especially given the IDE integration""Ã‚Â¦And to at least partially answer my own question, straight form the FAQ:> Can GitHub Copilot introduce insecure code in its suggestions?> There's a lot of public code in the world with insecure coding patterns, bugs, or references to outdated APIs or idioms. When GitHub Copilot synthesizes code suggestions based on this data, it can also synthesize code that contains these undesirable patterns. This is something we care a lot about at GitHub, and in recent years we've provided tools such as Actions, Dependabot, and CodeQL to open source projects to help improve code quality. Similarly, as GitHub Copilot improves, we will work to exclude insecure or low-quality code from the training set. Of course, you should always use GitHub Copilot together with testing practices and security tools, as well as your own judgment.Basically, they seem to hope that people will either be really careful about the suggested code or have existing code analysis workflows that would catch errors",Hacker News,https://news.ycombinator.com/item?id=27676514,3
"The exaggerations won't stop now, will they? First of all, it's not as if CoPilot spits out verbatim replica of training data on every other prompt.Secondly, the consequences of accidentally copying code by means of using this tool are pretty minor. The author acts as if copypasta from StackOverflow, RosettaCode and similar sites is NOT a daily occurrence (and can't even be checked in the case of closed source software).Fake gurus like Siraj Raval [0] can manage to literally steal - as in copying other people's work and claiming it as their own - for years without consequences and face ZERO legal backlash even after being exposed. Some of his repos had hundreds or even thousands of stars and forks on GitHub, while the original authors he copied from got no attention or credit at all.If this is what people can get away with who do this knowingly and deliberately and with entire projects, then I really have to wonder what the fuss is about when an ML model occasionally spits out a few lines of code snippets verbatim from its training set.[0]",Hacker News,https://news.ycombinator.com/item?id=27783971,4
"People mostly concentrate on whether using Copilot might be a real copyright violation.But the danger of being sued is a different question.Consider the following scenario:1) Company X has its product code stolen. Somebody puts it on GitHub. It's discovered and the code is removed.2) You work on an open-source project which competes with that product of Company X, you use Copilot, and make it known.3) Company X looks through your code and find fragments which look vaguely similar to fragments of their code.4) They sue, claiming that you copied and obfuscated their code.Were you not using Copilot, one line of defense for you would be that you never looked at the stolen code, never accessed it, so no copying took place.With Copilot, this line of defense is not available to you, because Copilot ""saw"" that code and in principle that could help to produce the fragments in question. (Of course other lines of defense are still available).Whether courts would accept this argument is a different question, but the argument is not obviously invalid, and Company X can cause enough trouble for you...",Hacker News,https://news.ycombinator.com/item?id=27784454,7
"I said this in the other copilot threads too, but don't forget that copyright is not the only protection there is.Lots of countries (USA first) have tons of software patents. Apache and GPL have clauses to protect the project and its users, but that obviously does not extend to copilot generated code.Now go guess where that code comes from and if it is somehow protected.",Hacker News,https://news.ycombinator.com/item?id=27784188,-1
"There was many personally identifying information leaks when it initially launched. Which suggests that they snooped through OSS code and violated any licenses. Here are 2 answers to the main two things copilot sympathizers ask.1. How do you know Co-pilot violated OSS licenses?> The aforementioned PII and secrets emitted. And IIRC a support personnel said they even copied GPL licensed code. So there is nothing stopping them from copying MIT licensed ones. Which is still a violation without attribution. So no reason for the benefit of the doubt.2. Humans copy code all the time. So why can't an AI? Or something along the lines of it...The answer is 2 parted.1. A human can only copy so much. Most of them are not in bad faith. And those who are in bad faith are not a humongous portion. An AI just scrapes the code in mass scale which is impossible for a human even with automation. It's also easier for it to learn from this training data compared to someone scrapping the data and maintaining a DB. This is such a common sense thing. None of the laws, policies or systems are made for AI in mind. So it is just a careless thing to say.2. If are not adhering to OSS licenses then YOU ARE in violation. So if you are not called out or punished, DOESN'T MEAN you are right. Just means you are getting away with it.This is just a PR exercise for MS. They already have the data and the product. A nobody has the balls to question them. The normalization of these bad behaviors with their good PR being ""the guardians of the OSS"" is literally taunting the entire OSS community.This is where I hoped FSF, EFF, SFConservancy or any other group would step up. Nothing so far. Just yelling into the void hoping MS will suddenly do the right thing.",Hacker News,https://news.ycombinator.com/item?id=31877446,4
"If the model is learning the nature of code and synthesizing new code then it may not violate some licenses. When people do this its usually fine.However, if the model is simply compressing code it sees into the model weights, just memorizing snippets, then outputting those snippets, that's much more likely to violate licenses. Like when people copy+paste from some licensed code without attribution (even if realistically nothing is enforced most of the time).The truth is likely somewhere in the middle. Lets say 20% of code is stored directly in the weights but the rest is synthesized. That's a problem for the whole product.We already know models do a little bit of both depending on the data coverage. Common structures like if;then, loops, etc. are probably ""understood""Ã‚Â because the model saw them in lots of contexts. However, specific functions, especially those that are seen only a few times in much the same contexts are more likely to be copied. There's a spectrum here from shortcut learning to understanding.OSS doesn't really have the access or resources to evaluate this. Github isn't really incentivized to share any analysis they've done here.What's interesting to me is that their solution to this problem is to put the issue on their users;customers. By default, crawl everything public, ignorant of licenses, and if the customer has license concerns its on them to disable public code matching.",Hacker News,https://news.ycombinator.com/item?id=31877806,2
So they got complaints that copilot generated public code and then as aftermath they hashed all public snippets to bandaid their GPL offense?,Hacker News,https://news.ycombinator.com/item?id=31877437,5
"There is a GZ on Github who tends to write the following in the header [1], which is different to what copilot generated:;** Copyright (C) 2019 GZ*;It is unlikely that copilot copy and pasted a comment block verbatim. In my experience copilot might do something that resembles this if the code is very common across many projects and there is probably only one logical way to implement it. eg. a function to calculate the area of a circle.Edit: I replaced the person's name with ""GZ"" because I didn't feel they needed be spammed on search engines because a LLM randomly produced their name.[1]",Hacker News,https://news.ycombinator.com/item?id=35518220,-1
"Microsoft is the devil. I'm sorry, but from ""the beginning of time"" that company has sought nothing but a complete monopoly.How could the github guy sell that platform to M$! On their LinkedIn platform they force C# jobs on you instead of, in my case, Go, what I searched for. You search for Go, you get C# results, if you're logged in.Why am I being so dramatic, because it has been going on since day 1. TCPA, now TPM, forced on you if you want to use their latest OS. Their browser sending images you view to them. Telemetry, who knows what kind, that you can't turn off. Copilot that steals other people work they published in good faith. And they have their hands everywhere.Aren't you tired of this kind of unreality, dystopia tbh. I am. I'm tired of not just Microsoft spying and abusing me but also Google and most of all the 3 letter word agencies in the US. Just an hour ago I was locked out of Github because they're forcing 2FA on me. Screw them. I'd rather not have a Github account than being forced to do 2FA. I've been on the net since 1995, not a single hacked account. I'm just fucking done. Microsoft is my new #1 enemy, again.",Hacker News,https://news.ycombinator.com/item?id=36332805,1
"Copilot is at its best (currently) when you teat it as autocomplete on steroids, it's very good at repetitive tasks, predicting the next line, has a brilliant short term memory, and great at prompting ""wiring"" type tasks.ChatGPT is far better at this sort of ""describe what you want and it will code for you"" process from TFA. It looks like the next version of Copilot with the chat interface will bring the latter workflow into the system, I'm looking forward to that, a GPT-4 based copilot.My main complaint about Copilot is that it doesn't seem to feed Intellisense prompts into the prediction, or validate predictions against them. That will make it so much more accurate when they fix that.",Hacker News,https://news.ycombinator.com/item?id=36210406,0
"Interesting article. It pretty much shows that Copilot is just a dumb text-completion engine. Copilot really is just regurgitating texts that are in someone's repository (or, possibly, a combination of repositories). ChatGPT, for all its faults, produces somewhat more ""intelligent"" results.As far as Copilot is concerned: Can you spell ""copyright violation""? The T&C on GitHub do state that you grant them the right to ""parse [your content] into a search index or otherwise analyze it on our servers"". It's not at all clear that this grants them the right to reproduce parts of your content (without credit) using Copilot.What about private repositories? ""GitHub considers the contents of private repositories to be confidential to you."" It would be interesting to see if one can get Copilot to produce code that is in a private repo.",Hacker News,https://news.ycombinator.com/item?id=36211502,-1
"This post touches on a hypothesis of mine: the more the context looks like homework or low quality code, the worse the suggestions will be. Like a LLM that autocompletes a childish question with a childish answer, a homework code prompt will autocomplete with more homework code, bugs and all. So getting high quality completions is less about descriptive prompts, and more about framing and vibes.Here are three ""homework-looking"" functions that were generated by Copilot based on their signatures, with comments added by me: # ""Load"" and ""file"" instead of more precise terms. def load_numbers(file): with open(file) as f: # both .strip() and .readlines() are redundant and should be removed. return [int(line.strip()) for line in f.readlines()] # Verbose and imprecise naming. def calculate_differences(numbers): numbers = sorted(numbers) # Inefficient cloning instead of .sort(). differences = [numbers[0]] # Breaks on empty lists. for i in range(1, len(numbers)): differences.append(numbers[i] - numbers[i-1]) differences.append(3) # Where did 3 come from? return differences # Vague. Redundancy with param name. Sounds too much like English. def count_increasing_numbers(numbers): increasing = 0 for i in range(1, len(numbers)): # Homework looking if;else. if numbers[i] - numbers[i-1] == 1: increasing += 1 else: increasing = 0 # Why would it reset? return increasing And here are three similar functions from signatures where I tried to give better ""vibes"": # ""parse"" and ""filename"" are more precise, and out params indicate higher performance code. def parse_numbers(filename, out_numbers): # Explicit file-mode! with open(filename, 'r') as f: # Reading lines by iterating over the file object! for line in f: # Explicit error handling! try: # No redundant .strip()! out_numbers.append(int(line)) except ValueError: pass # ""get"", ""adjacent"", and ""diff"" all show familiarity with vocabulary. def get_adjacent_diff(numbers): # Simple one liners! No off-by-1 errors! return [numbers[i] - numbers[i-1] for i in range(1, len(numbers))] # ""Pairwise"" is more technical, and avoiding name;arg redundancy. Less like English. def count_pairwise_increasing(numbers): # Another good one liner, making full use of list comprehension. return sum([1 for i in range(1, len(numbers)) if numbers[i] - numbers[i-1] == 1]) Unfortunately this is not objective at all, and subject to changes on GitHub's whims.",Hacker News,https://news.ycombinator.com/item?id=36210836,0
"I still have trouble imagining a software developer saying ""enhances my experience as a developer"" (which still sounds to me like something a bad marketing person from decades ago might say, if they didn't understand software developers).Which is maybe related to why I also have trouble understanding all these people voluntarily making written statements that their work product is tainted by Copilot, before the copyright issues have been settled in the courts.And, in this particular instance, doing it on the blog of what seems to be a contract software development firm serving other companies, they'd potentially be prompting the question of whether their tainted work product exposed their customers and;or violated contracts with their customers.",Hacker News,https://news.ycombinator.com/item?id=34592231,7
"I've been experimenting with copilot for a few months now and it has been really fun;frustrating. As the post mentions - it's best for fullstack development or learning new languages;toolsets. However, the error rate is still too high for a pure productivity booster, thus it still feels like a fun novelty.For example, it would help me out with 4 functions and the 5th one would be just plain, confidently wrong and all of those gains are spent on debugging the error. I noticed this especially sucky on lower-level languages or more niche libraries. I was learning Go and it just kept producing errors, introducing unused imports etc. while in Python it can straight up complete my JSON parser predicting the keys and everything.I wish it had some sort of verbosity setting - this would greatly help with trust. If it outputs some code with single character variables, no comments and some magic comprehension and I need to verify all that? I'm not bothering.",Hacker News,https://news.ycombinator.com/item?id=34591559,0
"> Using GitHub Copilot feels like I'm pairing with a seasoned developer It feels like pairing with an extremely eager junior developer who constantly makes horrific and subtle mistakes.It's great at repeating the same pattern or generating boilerplate. But it has no idea how your code works at any scale. So it easily violates invariants, makes up fields, gets confused about variables, etc. Off by one errors are just an endless mess.As a snippets library, it's awesome. As a pair programming buddy, it's like programming with a psychopath that wants your code to fail at all costs.",Hacker News,https://news.ycombinator.com/item?id=34591609,3
"For me the most interesting thing about using large language models is they offer a kind of conversation with the average of the human data they were trained on. They surprise me by telling me when I'm doing something boring.When Copilot guesses the next method name or comment I was going to type, it's doing that by saying ""this would be the most boring, average string of tokens to come next, so here you go,"" and it's fascinating how often that's right -- how often I'm wrong about how surprising the next line was. It's like how terrible humans are at generating unique passwords, except for everything I type. Copilot doesn't help by knowing things I don't, because it doesn't know anything, but it does help by guessing what I was obviously going to do next without me having to call out to memory.Once I have access to that average-of-humanity information for a while, I start to want it for the rest of my life too. OK, fine, that's the next method name I was going to write. [tab, autocomplete]. OK, fine, that's how I was going to close out my email. [tab, autocomplete]. Well, huh, I wonder if it knew what I was going to type next on the command line? [yes, probably]. I wonder if it knew which things I was going to buy in the grocery store? [yes, probably]. It starts to feel limiting to not have access to what the average next step in the sequence would be.And then it turns out that average-of-humanity models have all kinds of potential impacts on political power and labor and property law and so on, so all of that is pretty interesting too. But for me it starts with just poking at the model and going, oh, hey, it's ... everyone, how are you all doing?",Hacker News,https://news.ycombinator.com/item?id=33322695,0
"Systems like Copilot and Dall-E and so on turn their training data into anonymous common property. Your work becomes my work. This may appeal to naive people (students, hippies, etc.), for whom socialist;communist ideas are attractive, but it's poison in the real world.These systems are a mechanism that can regurgitate (digest, remix, emit) without attribution all of the world's open code and all of the world's art.With these systems, you're giving everyone the ability to plagiarize everything, effortlessly and unknowingly. No skill, no effort, no time required. No awareness of the sources of the derivative work.My work is now your work. Everyone and his 10-year old brother can ""write"" my code (and derivatives), without ever knowing I wrote it, without ever knowing I existed. Everyone can use my hard work, regurgitated anonymously, stripped of all credit, stripped of all attribution, stripped of all identity and ancestry and citation.It's a new kind of use not known (or imagined?) when the copyright laws were written.Training must be opt in, not opt out.Every artist, every creative individual, must EXPLICITLY OPT IN to having their hard work regurgitated anonymously by Copilot or Dall-E or whatever.If you want to donate your code or your painting or your music so it can easily be ""written"" or ""painted"", in whole or in part, by everyone else, without attribution, then go ahead and opt in.But if an author or artist does not EXPLICITLY OPT IN, you can't use their creative work to train these systems.All these code;art washing systems, that absorb and mix and regurgitate the hard work of creative people must be strictly opt in.If you don't care about this, it's naivete, or a lack of foresight, or apathy as these companies pillage the commons. Not something to be proud of.Microsoft and OpenAI (and others) are robbing us and you should care.",Hacker News,https://news.ycombinator.com/item?id=33321898,4
"-> GitHub's Copilot lies about its documentation. Why would I trust it with my codeThis is the biggest misconception people have about AI. You shouldn't trust a Copilot by default, but more often than not it will be very handy if you know what you do.",Hacker News,https://news.ycombinator.com/item?id=41719779,-1
"> At this point, Copilot strikes me as a tool that replaces googling for stack overflow answers.I've never seen anyone put a license on code they post on stack overflow, so I imagine you agree to release code in your answers under a permissive license (MIT, e.g.) when signing up on stack overflow.On github however, it's common to put a ""LICENSE"" file in a repository. Github even automatically shows you what license it is when its content is a known, widely used license. They do that automatically.But somehow, with all their fancy AI magic, it's too hard for github's employees to use the same automated licensing code to decide whether or not they should share licensed code. They could also slap on a comment saying: ""WARNING: this code is licensed under the license."" They could even expand that, saying: ""Your repository has the license, so in order to use this code you'd have to change your license.""Github could fix this issue with a hash-map and an if-statement. By not doing so they're willfully letting people commit copyright infringement, and disrespecting everyones intelligence in the process.",Hacker News,https://news.ycombinator.com/item?id=33321864,2
"There have been many exaggerated claims surfacing across Twitter and the blogosphere. What I found to resonate best with my own experience is that Copilot is okay on simple tasks, but actually misguides, confuses, and in fact breaks your flow of thought on anything beyond that. I like that sort of smart autocompletion and snippet retrieval for boilerplate, templates code, but for the business logic proper it's just awful. It either doesn't get it at all but wants to insert some crap and the IDE isn't always helpful to recognize that it's distracting you, or it hallucinates something that seems to fit at first sight but once you accept the suggestion moments later you have to undo it because you realize how far off that is, how it misrepresents the domain entities or logic, as if a junior dev suddenly injected that code into your shared file buffer in a pair programming session, while you were focusing. So while Copilot autocomplete is on, you always have to analyze what a gung-ho quasi junior dev knee-jerks into your file buffer.As many have witnesses, the Copilot Chat has been just terrible. I give up on my hopes for the current wave of the AI evolution.What is uniform across all LLM is the lack of nuance. Even when they manage to generate some domain specific characters that do make sense, there's always lack of detail and nuance to the domain, regardless how you try to trick your prompts to retrieve something from a very specific context. I'm impressed by how much easier it's become for me to get a digest of longer texts, but at the same time I'm disappointed by the quality of the results. It is very rare that I get what I actually ask for. It's like talking to a mid level consultant who pretends to know everything but the output is rather questionable, and you just give up and seek to end the meeting.",Hacker News,https://news.ycombinator.com/item?id=41152971,0
"We use copilot as fancy auto-complete. Our original hopes for it was more than that, but it's not been up to the task. Yes it solves leetcode as the article mentions, but that is next to useless for most of what our developers do, what isn't useless is how good it is at replacing code snippets. Especially because it's very transferable between developers, as they no longer build up an archive of personal snippets. Or at least not as many of them. So it's much easier to onboard new developers and get them to be more productive than it was before copilot.I don't think anyone at our shop has high hopes for LLM's in programming beyond efficiency anymore. I'd like to see Github copilot head in a direction where it's capable of auto-updating documentation such as JSdoc when functionality changes, LLM's are already excellent at writing documentation on ""good"" code, but the real trick is keeping it up-to-date as things change. I know this is also a change-management issue, but in the world where I mainly work, the time to properly maintain things isn't always prioritized by the business at large. Which obviously costs the business down the line, often grievously so, but as long as ""IT"" has very little pull in many organisations it's also just the state of things. I'd personally love for them to get better at writing and updating tests, but so far we've been far less successful with it than the author has.As far as efficiency and quality goes our in-house measurements point in two directions. For inexperienced developers quality has dropped with the use of LLM's, which in our house is completely down to how employees (and this is not just developers) tend trust LLM's more than they would trust search results. So much so that a lot of AI usage has basically been banned from the wider organisation by the upper decision makers because quality is important in what we do. Yes I know this is ironic when you look at how they prioritize IT in an organisation where 90% of our employees use a computer 100% of their working hours. Anyway, as far as efficiency goes there are two sides. When used as fancy auto-complete we see an increase in work output across every kind of developer, however, when used as a ""sparring partner"" we see a significant decrease. We don't have the resources to do a lot of pair-programming and a couple of developers might do direct sparring on computation challenges for 1-2 hours a week. They are free to do so more, and they aren't punished for it as we don't do any sort of hourly registration on work, but 1-2 hours is where it's at on average. Sometimes it'll increase if they are dealing with complex business processes or if we're on-boarding some one new.> Copilot is very useful to scan existing code for any errors or missed edge casesAside from tests I think this is the one part of the article I really haven't seen in our very anecdotal testing. But maybe this is down to us still learning how to adopt it properly or a difference in coding style? Anyway, almost all of our errors aren't with the actual code but rather with a misrepresentation;misunderstanding;unreported-change-in of business logic, and this has been the area where LLM's have been the weakest for us.",Hacker News,https://news.ycombinator.com/item?id=41152451,0
"> ""This prompted a lot of concern about whether Copilot would plagiarize code and violate copyright licenses. So by including 'q rsqrt' on the bad word list, they basically broke an embarrassing demo without addressing the real problem.""So if you don't want your GPL code to be included just use some words on the list, apparently GitHub thinks this is how it should be used.",Hacker News,https://news.ycombinator.com/item?id=28398870,2
">Training machine learning models on publicly available data is considered fair use across the machine learning community,""Ã‚Â the Copilot page says.I love the sheer fucking hubris of this statement. It's fair use because ""the machine learning community"" wants it to be. What a terrible justification. I hope they get their ass kicked in court over this.Anyways, the problem here isn't that they're training on open-source code, it's that their LLM is outputting code that violates open-source licenses by not including the text of the license and the author's original copyright statement. That's not fair use and it never has been.Keep in mind that this is a subsidiary of Microsoft doing this. I'm sure M$ won't mind my new ""Machine Learning platform"" that outputs a copy of Windows with a piracy crack. After all, it's widely accepted as fair use by ""the machine learning community"".",Hacker News,https://news.ycombinator.com/item?id=36127030,2
"> The absurdity of Microsoft and OpenAI's legal justification for GitHub Copilot.> Note 1: See Comment Regarding Request for Comments on Intellectual Property Protection for Artificial Intelligence Innovation (application;pdf) submitted by OpenAI to the USPTOWith where I predict the author is heading, I'll point out that that this document clarifies:> > We do not claim that AI systems are invariably beneficial or non-infringing. Rather, we address the narrow question of whether training AI systems on copyrighted data constitutes copyright infringement.> > Generative AI systems might generate output media that infringes on existing copyrighted works. We think that this is an unlikely accidental outcome of well-constructed generative AI systems, though it remains possible due to overfitting or developers' intentions.----> It contains a screen recording of Copilot suggesting this Quake code.I'd call the fast inverse square root algorithm ""folk code"", passed down and altered multiple times before being misattributed to Carmack. It appears in many repos, often with permissive licenses like WTFPL. I don't think it's generated by Copilot anymore, after steps to prevent exact regurgitations.As a challenge, since I see the same one example used so much, I'd be interested to see if there are any examples of:1. With the public version of Github Copilot, not opting out of any protections...2. Using a prompt that could be entered incidentally...3. Generate code of problematic length and similarity to existing code...4. Which is not (even ostensibly) under a license allowing such copyingThis tests whether Copilot would be an issue for someone using it earnestly. Whether it could be used to intentionally generate protected code is another question.Elaboration;reasoning:1. To allow corroboration and demonstrate that it's an issue on the service being sold commercially. My understanding is that Copilot is deterministic2. As opposed to, say, intentionally copying 90% of a source file and having it complete the last function. The prompt code appearing in other unrelated repos would be sufficient to show this3. Doesn't have to be an exact regurgitation, but I think it should at least by unambiguous that the given code is what's being copied4. Most licenses require at least attribution. ""even ostensibly"": someone might share code under public domain without permission to do so, but here even an honest human could make the same mistake of copying it",Hacker News,https://news.ycombinator.com/item?id=31963244,4
"Github's major closed-source Copilot extension reenables itself if you try to tone down the aggressiveness of its suggestions. Some malware ; reverse-useless-machine vibes.A lot of us are sort of waiting for the other shoe to drop with respect to Microsoft's commitment to openness, so this seem kinda foreboding.",Hacker News,https://news.ycombinator.com/item?id=40504868,-1
"All you need you need to know about Copilot (taken from ""Privacy ""Ã¢â‚¬Å“ Copilot for Individuals""Ã‚Â at data does Copilot for Individuals collect? GitHub Copilot relies on file content and additional data to work. It collects data to provide the service, some of which is then retained for further analysis and product improvements. GitHub Copilot collects the following data for individual users:User Engagement Data When you use GitHub Copilot it will collect usage information about events generated when interacting with the IDE or editor. These events include user edit actions like completions accepted and dismissed, and error and general usage data to identify metrics like latency and features engagement. This information may include personal data, such as pseudonymous identifiers.Code Snippets Data Depending on your preferred telemetry settings, GitHub Copilot may also collect and retain the following, collectively referred to as ""code snippets""Ã‚Â: source code that you are editing, related files and other files open in the same IDE or editor, URLs of repositories and files path.""Ã¢â‚¬Â""Ã¢â‚¬Â""Ã¢â‚¬Â""Ã¢â‚¬Â""Ã¢â‚¬Â""Ã¢â‚¬Â""Ã¢â‚¬Â""Ã¢â‚¬Â",Hacker News,https://news.ycombinator.com/item?id=34208498,8
"I have two thoughts on this:* Copilot isn't nearly as useful as some are making it out to be - I've been seeing thread on Reddit talking about how Copilot will mean devs will now be out of a job, and a bunch of other hot takes about what this means for the industry. I personally think that it won't be all that revolutionary, very useful, but it's not going to disrupt anything.* The ""Software vs Snippets"" argument: Say you have an OSS project that is MIT licensed. Copilot learns from some of your code and offers suggestions based on what you wrote to another developer for say a helper function or a script. Technically yes, your License was violated and your code was stolen. But when you think about it on a more practical level it's not that big of a deal. The point I'm making is Software is all about composing a bunch of snippets into something of value. A dev ripping off my build script or key extractor helper function isn't the same as them ripping off my entire app. One is just fine, the other is not in my eyes.",Hacker News,https://news.ycombinator.com/item?id=31895368,7
"People have been stealing open source code in much more meaningful ways for years. I'm more annoyed with, and think we'd be better off directing our ire at the countless embedded device vendors and commercial software vendors who ship GPL binaries with no source, rather than splitting hairs over CoPilot.I find Copilot distasteful and frustrating, but I don't think the misappropriation of code it performs is particularly meaningful. I think it's worth pushing back on, surely, as a matter of principle.But will it ""end open source,"" or do I think about it when I make contributions to open source code or provide solutions to others? Absolutely not.",Hacker News,https://news.ycombinator.com/item?id=31895532,7
"Many software engineers contribute to OSS for reasons that have little to do with money. They publish code in the open for free in the expectation that other engineers would look at it and learn from it, and while at that appreciate the effort and expertise of the original author.Copilot takes this away, in that a megacorp now uses the above contributions to provide a paid solution that essentially removes the need for people to even know you exist while allowing them to use the results of your work.I don't think it's a death of open source, but it's definitely a mini-death of Github. At least myself personally I'm inclined to switch to SourceHut, which so far seems to be trustworthy, no-bullshit and with clear business model.",Hacker News,https://news.ycombinator.com/item?id=31895422,-1
GitHub Copilot just sharpens the focus of copyright violation. If you've ever used an algorithm tutorial to write a program you've arguably done the same thing as Copilot.,Hacker News,https://news.ycombinator.com/item?id=33351120,-1
Github claims to have a vulnerability prevention system that blocks Copilot from generating insecure code ( but it clearly doesn't work.,Hacker News,https://news.ycombinator.com/item?id=38418500,3
"Yeah, our Swift team don't use it. How are you using it? Copilot doesn't seem supported by Xcode. It has the most features on VS Code.It does an excellent job on Kotlin, especially tests. We find it's most fluent with Python, pretty good with JS and TS, but found several security mistakes on PHP, etc.For boilerplate, accuracy seems a little higher than human. It wrote 1000 lines of mocks & test code in a day for me. But creatively, it's worse, there's some hallucination.There's a marked increase in quality and accuracy in GPT-4. GPT-4 is high quality enough that it catches tons of mistakes in code reviews and can debug with a screenshot of a stacktrace. Copilot seems like it's a generation or two out of date.",Hacker News,https://news.ycombinator.com/item?id=38622383,0
"This sounds like an AD, but i'll still replyThe question about Copilot is not how good;bad it works, it is the fact that it trained on everyone's public code without taking into account the license, raising multiple concerns, including privacy ones",Hacker News,https://news.ycombinator.com/item?id=33413730,1
"A code license can stop copying code and using binaries, I understand that CP learns from code but doesn't copy it verbatim nor uses binaries. Including a ""no-CP"" provision in a proprietary license wouldn't help either because such a project would most likely violate some current (or future) Github TOS.",Hacker News,https://news.ycombinator.com/item?id=31833404,2
"It's definitely in a gray area because the AI models are essentially compression engines that encode the code samples;data into the weights of the matrices that represent the ML model and then ""uncompress"" it to serve queries. I think it would be easy to argue that a compressed data set no matter how illegible would need to conform to the same license as the data set it was encoding but I don't think any lawyer is smart enough to make that case. So at the moment it remains a very convenient loophole for companies that have enough compute to mangle the data set beyond recognition and then use it to their advantage. So this will probably remain a convenient loophole for large companies to sidestep licensing restrictions by encoding whatever data;code they want to use into some neural network and then sell it as AI.For why these things are essentially mangled compression engines one can take a look at ""Hopfield Networks is all you need"": It allows representing all modern transformer networks (which is what CoPilot is using) as a bunch of hopfield networks which are essentially memory modules connected in some complicated topology to encode some data set.",Hacker News,https://news.ycombinator.com/item?id=30231242,4
"By using it, you're making their product better, it's in the feeding stage. It generates code, you change the code, the changes get sent back if telemetry is on, it's in the TOS.To quote: ""Code Snippets DataDepending on your preferred telemetry settings, GitHub Copilot may also collect and retain the following, collectively referred to as ""code snippets""Ã‚Â: source code that you are editing, related files and other files open in the same IDE or editor, URLs of repositories and files path."" [0][0]",Hacker News,https://news.ycombinator.com/item?id=34979994,8
"> But the unpleasant side effect of such an extension of copyright would be that all other AI-generated content would henceforth also be protected by copyright.Proprietary companies are going to lean on copilot and stick ""All rights reserved"" on the results. Copilot creates this ""unpleasant ... extension of copyright"".The author is not in a position to call whether Github infringes anyone's copyright or creates derivative works, the courts will do it perhaps differently in different juristictions.",Hacker News,https://news.ycombinator.com/item?id=31956437,2
"> The plaintiffs allege that Codex and Copilot violates the open-source programmers' copyrights because Copilot will sometimes regurgitate code it was trained onThis misunderstanding seems to be in the core of the discussion. One side thinking that copilot can write novel and unique new code using the mechanics of the programming language. The other side do not agree, instead seeing Copilot as simply an complex algorithm that regurgitate portions of the training data depending on context.The article do later go into a bit of discussion if those portions of training data can be considered copyrightable, but there are precedent. If you take a copyrighted work and split it into many small pieces, each much smaller than any copyrightable work, they can still be considered protected. Bittorent is one example, but there has also been examples where paintings has been cut into physical pieces and made into new creative work. In both people have argued that the individual portions aren't copyrightable and lost that argument in court.> Proving that all or even some of the code the purported class members wrote are copyrightable is not possible to do on a class-wide basis. It would require a judge or jury to opine on an instance-by-instance basis whether a given class members' purportedly copyrighted code was original enough to qualify for a copyright.The pirate bay founders made a similar argument. Their lawyers argue that the court would had to first opine on an instance-by-instance basis for each work that was claimed to be pirated. The judges however said that such instance-by-instance work isn't necessary unless there was doubted that copyrighted work had been copied through the site. If Copilot operate by regurgitate portions of the training data, regardless of how small such regurgitation is, such doubt does not exist.The pirate bay case is a pretty clear example where regardless if people split files into tiny pieces, if the copying of file is just theoretical and not proven, even if there is no individual even being accused of doing the actually copying, courts can still find people guilty if they find the alternative theory of no copying at all unbelievable.",Hacker News,https://news.ycombinator.com/item?id=34294203,4
"I doubt it.There's more to code than a set of completions, regardless of how powerful they are:1. There's the large scale nature of most software. The many components that must interact well. The database, the interface to users and the outside world. Security, hardware, performance, threading, and more.. the bigger decisions of systems to use and how they work together.2. There's the domain knowledge to the problem you're solving. Not sure how this goes away.3. There's the many false positives we're likely to get. Like when Siri doesn't understand what you're saying one in twenty times, you often just give up on voice because it's so much trouble to correct it. Except the risk here is potentially much higher. Maybe on your first serious copilot issues you'll be _really_ suspect of it and decide to turn it off.4. There's the innovative, interesting work that's down the long tail of what Copilot knows about. How generic is what you're doing? How likely is copilot to have encountered similar problems to what you're doing?5. There's debugging. How much time do we spend composing code vs figuring out why code isn't working well?TBH copilot seems to scratch an itch where I have to weave together a few stackoverflow examples to do some thing that seems a bit silly and time consuming. The task I wonder ""why hasn't anyone solved this yet?""Ã‚ÂGood software balances a dozen or more different constraints that typically are very specific to the problem at hand. I'm not sure how copilot replaces this.",Hacker News,https://news.ycombinator.com/item?id=27741253,0
"Not that it will do your job, but yes that it is yet another thing that devalues the perception of software developers.(e.g. if you do 80% of the work to ""done"" you have delivered 0% of the value. GitHub Copilot writes what superficially looks like software code but inevitably it will be more work to find and fix the bugs it injects than it would be to start with a ""clean sheet"". A lot of people are going to try it, learn the hard way, some will blame themselves instead of copilot.)",Hacker News,https://news.ycombinator.com/item?id=27740861,-1
"Secrets like API keys should be added last, when the program is in the very last stages, then you can plug them in. I know, it sucks having to do that, but it's a strategy I've been using for years, and I use leaky things like Copilot too. If you must use API keys with Copilot, make the key related to some disposable staging;test environment, not important code like prod.",Hacker News,https://news.ycombinator.com/item?id=36049435,6
"Copilot is just synthesis of man-made code. If a lot of code is written with bad security practices, Copilot will replicate those practices.",Hacker News,https://news.ycombinator.com/item?id=38048043,0
"I agree. People own the code they write, and in a free market, property rights are paramount. Copilot is clearly stealing code.",Hacker News,https://news.ycombinator.com/item?id=35481230,-1
"What I said last time is still true of this version: it's neither Free Software nor Open Source, and it doesn't even work for its stated purpose, since Microsoft is already willing to let Copilot violate licenses, e.g., by not providing attribution, which they virtually all require.",Hacker News,https://news.ycombinator.com/item?id=33473501,1
"See discussion of an earlier draft here: removed ""Open Source"" from the name because the license may be incompatible with point 6 of the OSI definition here: is the license text: Copyright (C) All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and;or other materials provided with the distribution. 3. Use in source or binary form for the construction or operation of predictive software generation systems is prohibited. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. The above is a verbatim (word-for-word) copy of the BSD 2-Clause License ( except for the addition of a third clause: 3. Use in source or binary form for the construction or operation of predictive software generation systems is prohibited. The BSD license restricts both ""redistribution"" and ""use"": Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Training a Copilot-like system is unambiguously ""use"" in violation of the new third clause.Comments and suggestions are welcome.",Hacker News,https://news.ycombinator.com/item?id=33473046,2
"Considering existing code already has vulnerabilities, some of which were used to train Copilot I think it's possible but not efficient in terms of success rate.But if they continue to ignore license terms I can see someone create repos with intentionally Copilot-incompatible licenses and watermark it so they can prove the license terms were violated.",Hacker News,https://news.ycombinator.com/item?id=31849021,5
"I agree that copilot is a real breakthrough, considering that the results seem to be mostly functioning. I think it has a ton of potential for helping people reach a crude solution instantly, without the need of learning the boilerplate setup of some library they wish to use. The developer still needs to pay careful attention and not accept the suggestions blindly.The dissatisfaction comes from many concerns that need to be solved:- Licensing: Sometimes Copilot copies chunks of code (including the comments) directly from the source dataset. - Quality: at least one example from the Copilot promotion page had a vulnerability issue, and another one used floats for currency.Then there's a problem I haven't seen anyone mention. When using copilot, your editor is sending chunks of your current codebase. I understood that Copilot is using this data for further improvement (a no-brainer, right?). But what if your project is proprietary and critical company IP? What if you have secrets to be leaked through Copilot?",Hacker News,https://news.ycombinator.com/item?id=27784842,0
"I use it a ton, daily, for everything.It's mainly good at writing boilerplate, but a surprising amount of code is boilerplate. A lot of small algorithms in helper functions; code which is very similar but not enough where abstraction is worth it (like similar but different keywords everywhere); code which is a really straightforward translation of the above documentation or comment, and I can just have Copilot generate the code from the comment; etc.It's also really good at generating a ""starting point""Ã‚Â (for a function, class, etc.) which even if incorrect, helps me get over the hurdle of writing the correct version. Like, sometimes I just have no idea what to write, and after it writes some random code I realize and edit the code to do the right thing.It's also really good at generating code for an unfamiliar language or library, because it will use the syntax and API and even if the semantics are off, I can understand the code once I see it and correct them. Whereas if I were to write the code myself, I already know what to write but I don't know how to write it because I don't know the syntax, method names, and conventions. It even understands the syntax of niche languages like Coq, although obviously the completions are much worse than common languages like JavaScript.And yes it makes careless mistakes, but I actually make careless mistakes more often (like swapping ""true""Ã‚Â and ""false""Ã‚Â, I put false into something which should obviously be true and vice versa""Ã‚Â¦I've done that multiple times. The AI rarely makes obvious mistakes like that, instead it's mistakes tend to be semantic errors and API hallucinations). And it can'thandle anything non-trivial (unless it's a very well-known algorithm); and if you start typing in the wrong code, it won't correct you, and will just generate more wrong code (it hallucinates like ChatGPT). It's not a programmer replacement. But it's really good at generating boilerplate, and a lot of code is boilerplate, so it's extremely useful.It's actually kind of scary because I'd hate not being able to use it. There are alternatives (CodeGeeX and Codeium), but I haven't heard anyone's experiences or comparisons. Fortunately Copilot has been out for a long time (since 2021!), seemingly hasn't changed much (main differences are better context and more places for code completion), and it uses GPT3 (!); so if it ever does get worse, I have more faith in an equal-performance replacement than something like ChatGPT.",Hacker News,https://news.ycombinator.com/item?id=36984468,0
"IMO the critical concept to explain LLM prompt injection and manipulation like this is that almost all these ""assistants"" are fictional characters in a document that looks like a theater-play script, along with some tricks that ""speaks out"" the character so that we humans believe it's a real entity. (Meanwhile, our own inputs invisibly become words ""spoken"" by another ""The User"" character.)So the true LLM is a nameless lump tasked with Make Any Document Longer. If for any reason the prior state is ""Copilot Says: Sure, "" then the LLM is probably going to try to make something that ""fits"" with that kind of intro.This becomes extra-dangerous when when the generated play-script has stuff like ""Copilot opens a terminal and runs the command X"", and some human programmers decided to put in special code to recognize and ""act out"" that stage-direction.> AI assistants like Copilot need strong context-awarenessThat'll be hard. The LLM is just Making Document Longer, and the document is one undifferentiated string with no ownership. Without core algorithm changes, you're stuck trying to put in flimsy literary guardrails.Really hardening it means getting closer to the ""real"" AI of sci-fi stories, where the machine (not just an assembled character named The Machine) recognizes multiple entities as existing, recognizes logical propositions, trakcs which entities are asserting those proposition (and not just referencing them), and assigning different trust-levels or authority.",Hacker News,https://news.ycombinator.com/item?id=42892475,0
"I still assume that Copilot uses my private repositories as training data, with them taking some care to not make it obvious. The software house i'm working at actually took the whole thing seriously and is currently migrating to an internally hosted GitLab. Not sure what potential leak exactly scares the responsible team, but interestingly it looks like it makes GitHub lose some worried customers.",Hacker News,https://news.ycombinator.com/item?id=35262680,10
"This is fascinating. It checks a lot of my boxes. I was recently looking at ActionsFlow [0] which is similar but runs on GitHub Actions.My thoughts -1. I don't see proper secret storage being handled. You typically don't want your API keys in your code. What would you recommend instead?2. ""OAuth"" based secrets. Many integrations require giving access to an App via OAuth, which involves a flow. I think that's being handled internally from the video [1] and from this project [2], but it's not clear. How is that handled?A common use case I'd automated once is that when a GitHub project gets starred, the developers public information is scrapped and they are then followed on Twitter, if their twitter handle is found. With Trigger.dev, the twitter part isn't clear.3. Error Handling - What about when some job fails to run? I understand there is a delay mechanism. But what about injecting custom error handling? Sending a message on slack, for example.4. Dashboards - They look awesome. And I get the impression that each ""action"" in the code is mapped to individual blocks in the dashboard. I'd love to be able to see a proper graph of the flow.I love that I can see the json request ; response for each. It'll make debugging easier when some API changes or fails.5. No Code solutions - In the long run, I can easily picture writing the integration I want in plain text, and having Github CoPilot or ChatGPT generate the code for me, and then I can quickly modify it.6. Incentive for integrations - As with most automation tools, entering the market is challenging as you're lacking integrations. The awesome thing about ActionsFlow [0] was that it was re-using an existing community of GitHub Actions, and therefore you don't start from scratch. Have you thought about reusing workflows from n8n or other projects?7. Integrating with existing Automations - I think a bit more focus should be made on integrating with IFTTT ; Zapier ; n8n. I see you provide webhooks, but I think some easy wrappers + documentation would be better. This way, I can try out newer workflows in Trigger, and easily just extend my existing system. And then if Trigger.dev works for me, I can think about migrating away from my existing automation solution.8. Open Source Longevity - Trigger.dev is MIT licensed. Could you please explain the rationale? How do you plan to combat someone launching a competitor using your code? N8n is deliberately ""Soure Code Available"" and not ""Open source"", which I thought was a decent compromise. Will you be following a more Open Core model similar to GitLab (which is also MIT licensed)?9. De-coupling runners and the dashboard - I'd love to not have the pain of maintaining the dashboard ; event listeners, but being able to control the running of the jobs. Similar to a CI or Airflow.10. Support for other languages - This is something that Dagger CI [3] now allows. Letting you use whatever programming language. With Github Actions, I can just package it as a container. Do you plan to support anything else?After moving from using HCL to Typescript for my Terraform code, the advantage is so great, that I can't seem myself going back to using a custom language such as Dagger's CUE [4]. Trigger.dev targeting Typescript is already a big win. However, I do have a number of automations in Python.Overall, I'm super optimistic. Congratulations on the launch.[0] - - - - -",Hacker News,https://news.ycombinator.com/item?id=34612618,8
"Bug;security risk: Something unexpected just happened when giving this a try. Hoping there's just a default string left in the project and some bad error handling. 1. Created new account in Second linked to my personal github account 2. Created project ""newproject"" in Second selecting link to exisitng repo 3. Success message ""git clone ;newproject"" rel=""nofollow""> 4. Create new project now shows the same account in the Select GitHub Account select field. 5. Stuck with no log out buttonedit: delete cookies, private window fresh login with personal account details still showing",Hacker News,https://news.ycombinator.com/item?id=35086912,-1
"Yes, other files open in your IDE may also be scanned.From terms of service [1] (Which I'm sure everyone reads)> when you edit files with the GitHub Copilot extension;plugin enabled, file content snippets [...] will be shared with GitHub, Microsoft, and OpenAI, and used for diagnostic purposes to improve suggestions and related products. GitHub Copilot relies on file content for context, both in the file you are editing and potentially other files open in the same IDE instance.[1]",Hacker News,https://news.ycombinator.com/item?id=29150160,8
"Well this could be a huge security issue. Can lead to potentially Copilot-surfing for company secrets in a new form, since Copilot is already leaking secret API keys and copyrighted code.The dangers of just regurgitating what has been read are unreal, since with good enough targeting you can read the data someone else wrote and expected to be anonymized. It's like huge global RAM of code, you just need to figure out how to get it to point at the right addresses.",Hacker News,https://news.ycombinator.com/item?id=29150481,-1
"Do the following experiment:1) Put in the same folder where you had that marketing blurb, a new blurb. Make sure it is unique but still looks like english. Example: now your startup allows you to fly to the moon in rockets powered by angry tweets.2) Try to force a reload of copilot. Maybe reinstall it?3) Recreate the conditions that suggested the first blurb, but trying to suggest the second blurb.4) Share the results, I am curious.If you get that suggestion, you have proof and reproductible steps for others to get proof. If you don't get that suggestion, we can't be sure, but the odds of it being just a coincidence increase.Good luck!",Hacker News,https://news.ycombinator.com/item?id=29150964,-1
"Have you told your client that you are making use of tools that upload the intellectual property they've shared with you ; they've paid you to create for them, onto third-party servers?If I was paying for work and the contractor was uploading the end result onto some sort of shared AI training set, we would not be working together for very long.You may have brought up an excellent point that needs to be inserted into new legal contracts -- either an opt in or out regarding the use of tools of various kinds that upload data into the cloud to help you. Maybe other companies would be okay with stuff like Copilot if it allows them to pay less money for developers who can'twrite proper code without it, or something. I don't know. I know that I want nothing to do with these sorts of systems, and I don't want any of my code anywhere near it. I'll definitely try to make sure nobody with access to my private repos has any of that nonsense enabled.Maybe the legal version of Copilot can write an appropriate contract clause for us?",Hacker News,https://news.ycombinator.com/item?id=29150840,-1
"This is definitely the case in our company:""Many large companies cannot use products like GitHub Copilot because of the security and privacy risks, so we're building a self-hosted version that's fine tuned to the company's codebase.""",Hacker News,https://news.ycombinator.com/item?id=35166920,10
"Copilot is just showing a problem we had all along more clearly.Commercial entities take things that are written and maintained by volunteers in their free time and barely give back. And this culture has just developed in a way that this is at most a moral failure, not a legal.People put it on the internet with a permissive license because they want others use it. This is great because people love sharing, but now, before you know it, you're suddenly supporting large enerprises and not just people that want to put cool things together like you. And you don't get any kind of compensation, even though it'd be peanuts for the company to pay for the time you put into maintenance and new development.And if you stop, someone else will always fill the void, eager to work for free.Maintainer burnout is a real thing and I wish the largest user of FOSS code would do their part to make the time spent on every piece of code they use worth it.Half of ""open source"" is a success story, the other half is just sad.Copilot just automated this entire process and sells the product of code back to developers, not enterprises.",Hacker News,https://news.ycombinator.com/item?id=31874147,7
"Code is shared with explicit licensing. Art exists in a more vague space, where the line between stealing and influence is subjective.If your code is released with no license, it should be fair to assume free use. But if it is licensed, tools like Copilot should be forced to respect that.",Hacker News,https://news.ycombinator.com/item?id=31871478,7
"There could be an ""Option 5"" where you find a way to get Copilot to ingest large amounts of broken, dangerous, etc, code.",Hacker News,https://news.ycombinator.com/item?id=27725631,0
"It's not about Copilot, per se -- but it's clear that most users probably didn't understand when then signed up that they're giving Github (now Microsoft, let's not forget) the right to use their code, at all.I've never been comfortable with the idea of trusting IP in places like Github, Google, etc. Although I find Copilot amazing, I find the use of user's code as distasteful as I do understandable.",Hacker News,https://news.ycombinator.com/item?id=27725862,10
"I think it's you who are in the ""vocal minority"".Microsoft Copilot is an abuse of open source. It is perhaps the greatest theft of intellectual property in human history.We should do everything we can to defeat it.That said, here is my Request For Comments regarding a slightly modified BSD 2-Clause License explicitly prohibiting Copilot-style use: need to do everything we can to defend open source software against Microsoft and the like.",Hacker News,https://news.ycombinator.com/item?id=33460004,1
"You don't have to do a thing. Microsoft (and Amazon, with it's CodeWhisperer, and every other similar business building such services) went into this fight with their eyes wide open. They knew this would happen and are well prepared for it. And even among the FSF's own invited whitepapers on CoPilot ( was one (""Copyright implications of the use of code repositories to train a machine learning model"") that that seemed to conclude that CoPilot-like services could defensibly say that copyright was not violated regardless of the license.So if this suit succeeded, I would be very surprised. Even the FSF isn't rushing to file suit and they care about this controversy probably more than anyone.",Hacker News,https://news.ycombinator.com/item?id=33460334,2
"I have used GitHub Copilot since it came out. I mostly use it for code explanations and writing tests.My experience is that you can't trust it completely for obvious reasons, so you need to know what you are doing before you can rely on these tools.I have tried Amazon Q and was severely disappointed.",Hacker News,https://news.ycombinator.com/item?id=40545889,-1
"I think that AI won't replace programmers, at least for now. All those codes written by AI still need review and more attention within security. Here is a good article about Copilot:",Hacker News,https://news.ycombinator.com/item?id=43251900,0
"Does GitHub Copilot save locally developed code? For example, if I develop code locally and in my code, there are connection parameters, like user and pass, for calls to remote services, do it save them as an example? Providing them as a suggestion in case someone else develops the same code in the future? Also, in case of enterprise code development, where code must remain strictly confidential, can GitHub copilot save any sort of code (entirely or even just snippets) and make it public with suggestions?",Stack Overflow,https://stackoverflow.com/questions/76075204/github-copilot-and-privacy-does-github-copilot-save-locally-developed-code,8
"GitHub has introduced Content Exclusion for Business and Enterprise users that allow the org owners to setup file exclusions such that those files are ignored by copilot. As per them: Code completion will not be available in the affected files. The content in affected files will not inform code completion suggestions in other files. The content in affected files will not inform GitHub Copilot Chat's responses. Do note that if you ask Copilot Chat about it, it will simply deny sending over any code at all, but if you were to check the outgoing network requests, you can see entire contents of the current file being sent along with the prompt.",Stack Overflow,https://stackoverflow.com/questions/76075204/github-copilot-and-privacy-does-github-copilot-save-locally-developed-code,8
"I'm currently using GitHub Copilot in Visual Studio Code and am trying to find a way to prevent Copilot from accessing certain file types, specifically .env files, which contain sensitive information. I attempted to disable Copilot for these files by adding 'env' and '.env' to the list of disabled files in the Copilot settings, but this approach didn't work, presumably because these aren't recognized as distinct language tags in VS Code. Has anyone successfully found a way to exclude specific files or file types from GitHub Copilot's analysis? If so, could you please share how you achieved this?",Stack Overflow,https://stackoverflow.com/questions/77780462/how-to-exclude-specific-files-like-env-from-github-copilot-in-vs-code,8
"In your VS Code user settings.json file, Add .env* to the files.associations object Update the github.copilot.enable object Example: ""files.associations"": { "".env*"": ""dotenv"" }, ""github.copilot.enable"": { ""*"": true, // ... ""dotenv"": false, } To validate, open a test .env file. See that the Copilot icon has a slash over it. Consider the limitations from whether they apply, and if they do, how they may apply in this case. Excluding content from GitHub Copilot currently only affects code completion. GitHub Copilot Chat is not affected by these settings. Content exclusion prevents Copilot directly accessing the content of excluded files. Copilot may draw information about non-excluded files from semantic information provided by the IDE - for example, type information, or hover-over definitions for symbols used in code. It's possible that the IDE may derive this information from excluded files.",Stack Overflow,https://stackoverflow.com/questions/77780462/how-to-exclude-specific-files-like-env-from-github-copilot-in-vs-code,8
"I am working on a confidential project, so I ensure that only specific parts of the code are sent to Copilot Chat. To achieve this, I have disabled Copilot Completion and primarily use Copilot Chat. At the file level, I have disabled it entirely and only enable it for the specific range of code I select when necessary. In some cases, I include entire files, but only if I am certain they contain no confidential data. Otherwise, I use the traditional method of copying the code and pasting it into the chat. The method I used to inspect network requests was to press Ctrl+Shift+P, type Developer: Toggle Developer Tools, and press Enter. A new window, similar to the Chromium browser, then opened. I inspected the network requests in the Network tab, started the Copilot conversation, and checked the Network DevTools again. However, no network requests related to GitHub or Copilot appeared. Another method I tried was opening Copilot Chat in a new window. Then, I pressed Ctrl+Shift+P, typed Developer: Open Webview Developer Tools, and pressed Enter. However, no new window appeared.",Stack Overflow,https://stackoverflow.com/questions/79237016/why-github-copilot-network-request-not-appeared-in-visual-studio-code-developer,8
"TLDR: I made an (unofficial) API: I was inspired by @Snowflyt at how simple their API interface was, so I reverse engineered the Copilot vim plugin and associated API. I matched the way they do OAuth (normal Github Apps aren't allowed Copilot access, so I just use Copilot's App ID). This way, you don't have to rely on installing vim and the plugin and going through the setup process. The repo linked includes a full self-hosted solution, that you can then call from whatever other functionality you want. The first time you start it, you'll have to complete OAuth (enter provided code into link), and then after that it will automatically refresh all the auth tokens. There are some more advanced features of Copilot that I haven't added yet, but it currently supports basic code completion prompting. To answer exactly the question you asked - if you want to record the prompts and top responses from the Copilot plugin as you're using it, you can just wrap the agent.js file in the plugin with a logger, such as by using the following commands (assuming linux, and that you have the Vim copilot plugin installed): mv ~/.config/nvim/pack/github/start/copilot.vim/dist/agent.js ~/.config/nvim/pack/github/start/copilot.vim/dist/agent.orig.js Edit the file ~/.config/nvim/pack/github/start/copilot.vim/dist/agent.js, and paste in the following code: const fs = require('fs'); const { spawn } = require('child_process'); const inLogStream = fs.createWriteStream('copilot-prompts.log', { flags: 'a' }); const outLogStream = fs.createWriteStream('copilot-suggestions.log', { flags: 'a' }); // Replace the path with the absolute path for agent.js const agentScriptPath = '/root/.config/nvim/pack/github/start/copilot.vim/dist/agent.orig.js'; // Spawn a new process running agent.js with the absolute path const agentProcess = spawn('node', [agentScriptPath]); // Pipe stdin from the main script to the new process and log it process.stdin.pipe(inLogStream); process.stdin.pipe(agentProcess.stdin); // Pipe stdout from the new process back to the main script's stdout and log it agentProcess.stdout.pipe(outLogStream); agentProcess.stdout.pipe(process.stdout); // Handle process exit agentProcess.on('exit', (code, signal) => { console.log(`Agent process exited with code ${code} and signal ${signal}`); inLogStream.end(); outLogStream.end(); process.exit(); }); // Handle errors agentProcess.on('error', (err) => { console.error(`Error in agent process: ${err.message}`); inLogStream.end(); outLogStream.end(); process.exit(1); }); // Handle main script stdin end process.stdin.on('end', () => { // Close the stdin stream for the spawned process when main script stdin ends agentProcess.stdin.end(); }); // Handle main script exit process.on('exit', () => { // Kill the spawned process when the main script exits agentProcess.kill(); }); // Handle main script termination process.on('SIGINT', () => { // Handle Ctrl+C to gracefully terminate both the main script and the spawned process process.exit(); }); Use copilot in vim as desired to generate logs. View the prompts and suggestions in ~/copilot-prompts.log and ~/copilot-suggestions.log. The output of these logs requires some parsing because they use JSON-RPC, but I'll let you decide exactly how you want to implement that.",Stack Overflow,https://stackoverflow.com/questions/76741410/how-to-invoke-github-copilot-programmatically,8
"There are two ways you could extend the functionality of Copilot: either modify the extension source code (for which I described an example of how to do here: or write your own plugin using Copilot's API. Although Copilot does not have an official API, as others mentioned, their API is accesible publicly, and I have written an interface that allows you to use it. As long as your extension can read the local copilot token, then your extension could seamlessly call GPT-4 and Codex without needing an API key of your own. Here is the HTTP-based API interface that I wrote to demonstrate:",Stack Overflow,https://stackoverflow.com/questions/74213723/is-there-a-way-to-extend-the-functionality-of-the-github-copilot-plugin-for-vs-c,-1
"NOTE: There is a thread on the official Copilot discussion board on this issue here. Disclaimer: these are a bunch of things I tried that didn't work, but may work for you. I have access to Github Copilot as well but get the error: command 'github.copilot.generate' not found Tried reloading and reinstalling it as well through vscodes ui. Went in and manually went through the process of completely deleting it explained here. Afterward, I logged out of my Github account attempted to install Copilot then logged back in when the prompt was displayed, again no luck. Tested running both CTRL+ENTER and Alt+\ , neither of them did anything. After I tried: Developer: Reload Window But that didn't do anything. Checked the discussion forums and they had a solution for: command 'github.copilot.toggleCopilot' not found Which was to simply uninstall then reinstall copilot then run CTRL+R (Developer: Reload Window), this too didn't work out. There's a good chance that all you'd have to do is retrigger the authentication flow considering that the developer tools console reports that copilot is ""missing an auth token"" (but no one knows how to retrigger it). In addition to the above, I tried running CTRL+SHIFT+P then ran ""Open Copilot"", which resulted in the error Command 'Open Copilot' resulted in an error (command 'github.copilot.generate' not found) From this discussion post on another forum with a similar issue, apparently, this is an issue with command registration. This specific comment seemed most relevant Uninstall Python extension (if you have pylance uninstall it first). Close all instances of VS Code. Go to,%USERPROFILE%/.vscode/extensions (on windows) or ~/.vscode/extensions on Linux/Mac. Delete any folder with the name starting with ms-python.python* Start VS Code, and install Python extension (also pylance if you uninstalled it in step 1). Of course, you'd have to apply this to Copilot so... Uninstall Copilot Close all instances of vscode Go to, %USERPROFILE%/.vscode/extensions (on windows) or ~/.vscode/extensions on Linux/Mac. Delete all instances of Copilot Go to, %USERPROFILE%/AppData/Roaming/Code/CachedExtensionVSIXs Delete all instances of Copilot Start and install Copilot This too didn't work. If I were to guess this would be solvable via retriggering the authentication flow.",Stack Overflow,https://stackoverflow.com/questions/68253302/github-copilot-commands-not-working-and-showing-error,-1
"This happens when you have failed to authenticate Copilot with Visual Studio Code. If you had a missed authentication flow the flow can be re-triggered by clicking the VSCode Notifications icon on the status bar and clicking on the Sign in to Github notification. If you have lost the authentication flow notification from the notifications panel, re-installing the Copilot helps. TIP: To make it easier to sign in, make sure you are already authenticated on github.com on the default browser.",Stack Overflow,https://stackoverflow.com/questions/68253302/github-copilot-commands-not-working-and-showing-error,-1
"the logic app connector for Microsoft copilot foe security allows a total of 600 calls per 60 seconds for a single API connection. Here is a reference to the Throttling limits Based on the observed behavior, it is likely that you are reaching security compute unit(SCU) limit or your Microsoft Copilot for Security. Once the limit has been reached, the copilot cannot process any additional prompts, and the capacity would be available in the next hour. You can monitor and view the usage of these compute units by following the steps in the article - Monitor security compute units use. If you identify this to be the cause of your issue, you can update the SCU limit by following the steps under section Update the provisioned security compute units (SCUs) If you still encounter issues after trying the above steps, you can contact support by creating a ticket",Stack Overflow,https://stackoverflow.com/questions/78851790/using-copilot-with-logic-apps,-1
"How can I change Github Copilot settings in VSCode to increase the token limit to 4098 or 8196 as promised in the Copilot product announcements and documentation? OpenAI is supposed to provide a 32K token limit. What I Tried: (to find how to change the VSCode Github Copilot configuration): I asked Copilot in VSCode: ""How can I change Github Copilot settings to increase the token limit to 4098 as promised in the documentation?"" It responded: ""The token limit is a built-in feature of the AI model and cannot be changed by users. If you have questions about GitHub Copilot's settings or features, I recommend checking the official GitHub Copilot documentation or reaching out to GitHub support."" I checked the documentation and Github support forums and could find nothing. I checked all Github Copilot settings in my Github account, and there is no way to change the token limit. I asked OpenAI ChatGPT 4: ""What is the token limit for Python code generation for ChatGPT 4?"" ChatGPT 4 responded: ""The token limit for Python code generation in ChatGPT-4, including the code, comments, and any outputs, typically aligns with the overall token limit of a ChatGPT-4 response. This limit is approximately 4096 tokens in total. However, it's important to note that the specific limit can vary slightly based on the model configuration and the context of the conversation. For Python code generation, the tokens are used for both the code itself and any textual output it generates. This means that longer code snippets or those producing extensive outputs might reach the token limit more quickly."" I asked OpenAI ChatGPT 4: ""What is the token limit for Python code generation for ChatGPT 3.5 Turbo?"" ChatGPT 4 responded: ""In ChatGPT 3.5 Turbo, the token limit for Python code generation is approximately 4096 tokens, which includes both the code itself and any associated comments or outputs. This token limit is consistent with the overall response token limit for this model."" Copilot is clearly throttling the OpenAI model or it is not up-to-date with ChatGPT 4 as Microsoft announced. ################################################### This causes problems because we must limit and ""chunk down"" what we ask Copilot to assist us with, reducing developer bandwidth and throughput per day. QUESTIONS: Where is the Copilot API documented, so that we could change VSCode settings or hook into the module that communicates with the LLM, like we can do with LangChain and OpenAI API applications ?? How can I ""unthrottle"" Copilot to increase the token limit when generating Python code?",Stack Overflow,https://stackoverflow.com/questions/77842786/how-to-change-github-copilot-settings-in-vscode-to-increase-the-token-limit-to-4,-1
"I was using Visual Studio for an ASP.netcore project at work with Github Copilot. I was writing javascript in one of the Views. A strange thing happened where lowercase d's were injected into the code at random times and random places. In the editor, there were characters that were being inject in places in the source causing errors, and either I had to undo the insertion of the characters or delete them. Yet, when I would add more code to the source, the characters would be inserted again. This proved to be difficult to debug as it kept happening. This was also happening with C# in a DotNet 7 project. It was a hard time to clean up and I had to turn off Copilot. There were a couple curly braces that were injected, also. This is peculiar behavior. What is happening here?",Stack Overflow,https://stackoverflow.com/questions/78052393/github-copilot-strange-injection-of-characters-with-visual-studio-javascript,0
"With ChatGPT we have the option: ÃƒÂ¢Ã¢â€šÂ¬Ã…â€œHow would you like ChatGPT to respond?ÃƒÂ¢Ã¢â€šÂ¬Ã‚Â Where we can give indications on how we expect it to respond. Is the same feature available in GitHub Copilot(GC) or are there some workarounds to get it? Example having a file locally that GC reads to get those user preferences before giving a response. The goal is to be able share those settings in the same development team, hence avoiding too many divergences in the code and tests generated by GC. Thank you in advance for your help",Stack Overflow,https://stackoverflow.com/questions/79270164/is-there-a-how-do-you-like-github-copilot-to-respond-in-github-copilot,8
"I have installed and used github copilot for some time. And while working on a google maps project, I was curious and asked it to autocomplete the API url with the key open, and it listed suggestions with various API keys included. This got me worried since it generates codes based on other people code on github. At first I was thinking those could be not functional keys, But I found that to be wrong based on this article. So, how what methods can I use to protect my keys from such happening? I am thinking uninstalling wont help since my repos are already at github.",Stack Overflow,https://stackoverflow.com/questions/70559637/github-copilot-giving-away-api-keys-how-can-i-protect-my-keys,-1
"The only way to protect yourself is to not commit any private information to a public git repository. Your Google Maps API key should be stored in an environment variable or a configuration file, and should never be checked into git. A common way to do this for modern applications is to use environment variables in production and a .env file in development (that simulates using environment variables but actually holds the content of those variables in a local file). There are ""dotenv"" libraries available for just about every programming language. You will typically also create a .env.example file which you do commit to git that contains examples of all of the environment variables that your application requires, but does not contain actual secret values.",Stack Overflow,https://stackoverflow.com/questions/70559637/github-copilot-giving-away-api-keys-how-can-i-protect-my-keys,-1
"I am new to scripting/coding. My work has tasked me to get rid of or disable copilot from showing up and being utilized till we get all the greenlight from our state IT people to use it. This is what I have so far and I am running into errors that I can't seem to see. - # Create a new instance of the Windows Script Shell object $objShell = New-Object -ComObject ""WScript.Shell"" # Modify registry key for Copilot eligibility $RegPath1 = ""HKCU\Software\Microsoft\Windows\Shell\Copilot\BingChat"" $objShell.RegWrite(""$RegPath1\IsUserEligible"", ""0"", ""REG_DWORD"") Start-Sleep -10 # Modify registry key to show Copilot button in Windows Explorer $RegPath2 = ""HKCU\Software\Microsoft\Windows\CurrentVersion\Explorer\Advanced"" $objShell.RegWrite(""$RegPath2\ShowCopilotButton"", ""1"", ""REG_DWORD"") It seems to work but I am getting an error message that I can't get PS to show. I have tried modifying the regedit but that option isn't available on my end for some reason. I have changed the start sleep to 10 instead of milliseconds 3000.",Stack Overflow,https://stackoverflow.com/questions/78081919/how-to-disable-copilot-using-powershell,6
"My company recently created an AI usage policy that states that developers are allowed to use AI tools to generate code for use in our codebase. It stipulates, though, that all AI-generated code must be called out with a comment. The intent behind the policy is to highlight possibly ill-informed chunks of code that might be more error-prone than others (entire functions or classes generated by AI, code with security implications, etc.). Ultimately, the buck stops with the developer and reviewer, but this could be a good flag to have. Of course, there's no way to enforce this on an organizational level, nor is there a way to enforce the habit on an individual level, when using an AI code generator is as easy and unconscious as tab-completion (like with the Copilot VS Code extension). I'd like to automate this somehow to make compliance as painless as possible. I'm imagining including a config in each repo that would fire off a hook script each time a dev accepts a suggestion. If the file doesn't already have one, it'll add a comment to the top of the file to the effect of ÃƒÂ¢Ã¢â€šÂ¬Ã…â€œThis file was created in whole or in part by generative AI.ÃƒÂ¢Ã¢â€šÂ¬Ã‚Â Does Copilot have an API for this? Could this be done with prompt customization? Is there another, low-friction way devs could have these comments generated for them? Most of us use VS Code.",Stack Overflow,https://stackoverflow.com/questions/77829392/is-there-a-way-to-run-a-hook-script-when-accepting-a-github-copilot-suggestion,0
"Found this here github-copilot-explorer Disclaimer: Mileage may vary, warranty ended right before I clicked the post button ""secret_key"": A string field for entering a secret API key. This key could be used for identification and authorization purposes when connecting to GitHub Copilot. This field does not have a default value. ""length"": An integer field specifying the maximum length of generated code in tokens. A 'token' in this case would be an individual component of the code, such as a keyword, identifier, operator, or punctuation mark. The default value is 500 tokens. ""temperature"": A string field for adjusting the 'temperature' parameter in the AI's sampling function, which affects the randomness of the AI's output. Lower values make the output more deterministic, while higher values increase randomness. This field does not have a default value and ranges from 0.0 to 1.0. ""top_p"": A numerical field setting the 'top_p' or nucleus sampling parameter, which specifies the cumulative probability threshold for token selection. By default, this value is set to 1, meaning that all tokens with non-zero probability could be considered for generation. ""stops"": An object field that allows setting per-language stop sequences. These could be sequences of tokens that tell the AI to stop generating further code for a specific language. ""indentationMode"": An object field for controlling indentation block termination on a per-language basis. For instance, you could set ""python"": false to disable automatic block termination for Python code. ""inlineSuggestCount"": An integer field that determines the number of inline suggestions to fetch. By default, GitHub Copilot would fetch 3 inline suggestions. ""listCount"": An integer field that sets the number of solutions to display in the GitHub Copilot interface. The default value is 10 solutions. ""debug.showScores"": A boolean field controlling whether to show scores in the sorted solutions. By default, this feature is turned off. ""debug.overrideEngine"": A string field for specifying an alternate engine name. This could be used for testing or debugging purposes and does not have a default value. ""debug.overrideProxyUrl"" and ""debug.testOverrideProxyUrl"": These string fields allow you to specify alternate GitHub authentication proxy URLs. The former is for general use, and the latter is specifically for running tests. Both fields do not have default values. ""debug.filterLogCategories"": An array field for specifying which log categories to display. If the array is empty, all log categories will be displayed.",Stack Overflow,https://stackoverflow.com/questions/75213752/how-can-i-discover-valid-members-of-object-typed-settings-contributed-by-a-vs-co,-1
"As the title suggests, I am trying to read memory from another process in C++ in order to check if the values from the other process reach a certain level. Since I don't know anything about this, I decided to consult GitHub Copilot for help. On a normal basis, I would search the docs, but Github seems to disagree. Since I have access to GitHub Copilot, and since the front page advertisement clearly encourages users to trust Copilot's programming ability, I chose to let Copilot make this function. So I gave it a prompt in the form of a comment: //A function that can grab an address from the memory of another process and store it as a double value What it gave me seemed pretty good, but I will never take a function that copilot makes and blindly use it unless I know for sure it will work (because I don't trust that everything Copilot makes is never going to cause issues, especially when dealing with pointers and such). I wanted to see if someone who had experience with memory in C++ could tell me if this function will work and why it would or wouldn't work as I know nothing about getting memory from another process. There are three main reasons why I am not just searching the docs anyway despite GitHub's statement: Since this is a complicated and real-world use case, this will really test Copilot's programming ability and it will give me insight into how much I can trust Copilot in the future for stuff I don't know how to do (Obviously I wouldn't let this get out of hand, but it would be good to know I can trust Copilot a little more than I do right now). Searching the docs anyways despite the statement that GitHub made on their website is quite the opposite of what Copilot is supposed to help users with, and while I understand that it's a public beta and it's not complete yet, it should at least be good enough for real-world use cases rather than simple coding cases. An answer from someone experienced will really show if it is good enough for real-world coding cases. The docs only tell me what a function does and what to put as its parameters, it doesn't tell me how to use it. If I really wanted to know how to use it, I would have to search the web. Searching the web will most likely get me complicated examples that don't pertain to my issue and defer me from what I am actually trying to accomplish. Not only that, but it is the opposite of what Copilot is supposed to help users with, as stated in reason #2. Here is the code that Copilot generated for me: DWORD GetAddress(DWORD dwProcessId, LPCWSTR szModuleName, const char* szProcName) { HANDLE hProcess = OpenProcess(PROCESS_ALL_ACCESS, FALSE, dwProcessId); if (hProcess == NULL) return 0; MODULEINFO modinfo; GetModuleInformation(hProcess, GetModuleHandle(szModuleName), &modinfo, sizeof(MODULEINFO)); DWORD dwAddress = (DWORD)modinfo.lpBaseOfDll; DWORD dwSize = (DWORD)modinfo.SizeOfImage; MEMORY_BASIC_INFORMATION mbi; while (dwSize > 0) { VirtualQueryEx(hProcess, (LPVOID)dwAddress, &mbi, sizeof(mbi)); if (mbi.State == MEM_COMMIT && !(mbi.Protect & PAGE_GUARD) && mbi.Protect & PAGE_EXECUTE_READWRITE) { DWORD dwOldProtect; VirtualProtectEx(hProcess, (LPVOID)dwAddress, mbi.RegionSize, PAGE_EXECUTE_READWRITE, &dwOldProtect); char* szBuffer = new char[mbi.RegionSize]; ReadProcessMemory(hProcess, (LPVOID)dwAddress, szBuffer, mbi.RegionSize, NULL); for (DWORD dwIndex = 0; dwIndex < mbi.RegionSize - 4; dwIndex++) { if (szBuffer[dwIndex] == '\x55' && szBuffer[dwIndex + 1] == '\x8B' && szBuffer[dwIndex + 2] == 'E' && szBuffer[dwIndex + 3] == 'A') { DWORD dwAddress2 = dwAddress + dwIndex + 7; DWORD dwAddress3 = dwAddress2 + *(DWORD*)(dwAddress2); if (strcmp((char*)dwAddress3, szProcName) == 0) { delete[] szBuffer; CloseHandle(hProcess); return dwAddress2 + 4; } } } delete[] szBuffer; VirtualProtectEx(hProcess, (LPVOID)dwAddress, mbi.RegionSize, dwOldProtect, &dwOldProtect); } dwAddress += mbi.RegionSize; dwSize -= mbi.RegionSize; } CloseHandle(hProcess); return 0; } You may point out an immediately noticeable error: The function returns DWORD rather than double, which is what I asked Copilot to return. I saw that error but from examples that I have seen (Yes, I have done at least some searching), returning DWORD works as well. I could have seen those examples wrong, and if I am correct me.",Stack Overflow,https://stackoverflow.com/questions/71152461/reading-memory-from-another-process-in-c-copilots-solution,0
"The function returns double but it only returns the value that is stored in the memory, not actually double-typed data. If you do a casting to double, you get back your original data. You can't search in memory with anything other than a byte pointer on 64-bit systems: There are different ways to search for a string in memory, depending on what you are looking for: (Read more here: Memory Scraper This program uses several techniques to obtain information from processes and memory as it runs so that it can be added to the evidence file when Cofactor terminates the target process (by default) or when you terminate the program manually (with CTRL+C). This code was mostly cobbled together from various examples at and with some heavy modifications made to get the output in a useful format. The process memory usage is checked continuously and added to the log file when it changes. This is done by getting a pointer to the process' memory region, then checking all of its pages as they are referenced. When they are changed, the contents of that page will be read and added to the log file as evidence. If a process goes in and out of sleep mode or is stopped for some other reason, this program will detect that and add it to the log file accordingly. The current DLLs loaded by processes are recorded every 5 seconds so that if a DLL gets loaded after Cofactor terminates its target, it will still be included in the log file as evidence. A process is also checked every 5 seconds for new threads being spawned so that child processes are also included in our evidence files. We could extend this program by having it check for two modules: 1) A module containing functions that correspond to debug breakpoints (which would detect whether a debugger was attached). 2) A module containing crash signatures - integers that would trigger an alert if they were found written to memory in any of our processes (like stack smashing protections might provide). In order to do this without complicating things too much, I'd probably use CreateRemoteThread with an address within each module to continue execution from that thread into your own code where you can check for the breakpoint or crash signature and act accordingly. Conclusion If you need to debug a process and can't get it to stop for any reason, this program will still be able to grab the process memory at any time so that you can search for whatever you need. You'll have to do some extra work in order to use the log file that is created, like parsing it with a parser of your choice and searching it (using regexes or something) which I assume is outside of the scope of what Copilot is designed to do. If you end up using this program, please let me know! I'm curious to see how many people find this program useful. Interesting Techniques I Learned From Other Programs Finding DLLs Loaded by a Process The C++ code below uses the Windows API GetModuleFileNameW() to get the full path of loaded DLLs and parses it with split() to extract just the filename and not the whole path. The rest of that code just tries to avoid duplicates while being simple enough that it doesn't get too confused between different processes and file system cases (hopefully). // Code Example: Finding DLLs Loaded by a Process #include <stdio.h> #include <string.h> #include <tchar.h> #define BUFF_SIZE 200 // Find the full path to a loaded DLL by process ID (PID) and its filename (first 8 characters) void GetModuleFileNameEx(int pid, const char* szName, char* buff, int buffSize) { HANDLE hProcess = OpenProcess(PROCESS_QUERY_INFORMATION | PROCESS_VM_READ, FALSE, pid); if (hProcess == NULL) { return; } HMODULE hMods[1024]; DWORD cbNeeded; if (!EnumProcessModules(hProcess, hMods, sizeof(hMods), &cbNeeded)) { return; } for (int i = 0; i < (int)(cbNeeded / sizeof(HMODULE)); i++) { TCHAR szModName[MAX_PATH]; if (!GetModuleFileNameEx(hProcess, hMods[i], szModName, sizeof(szModName))) { continue; } strcat_s((char*)buff, buffSize - 1 , (char*)szModName); // Check if the first 8 characters of the filename in the process matches // with what we are looking for and avoid adding duplicates char* chPtr = strchr((char*)buff, '\\'); if (chPtr != NULL) { *chPtr = 0; strcat_s((char*)buff, buffSize - 1 , ""\\""); strcat_s((char*)buff, buffSize - 1 , szName); HANDLE hFile = CreateFileA((LPCSTR)buff, GENERIC_READ, FILE_SHARE_READ, NULL, OPEN_EXISTING, FILE_ATTRIBUTE_NORMAL | FILE_FLAG_RANDOM_ACCESS | FILE_FLAG_SEQUENTIAL_SCAN, NULL); if (hFile != INVALID_HANDLE_VALUE) { CloseHandle(hFile); return; } } else { break; } } } int main() { char szDllName[8]; // Maximum length of a module name is MAXPATH - 1 bytes including the NULL terminator. However we only need 8 characters to store the DLL name so use this limit to save memory. int pid; scanf(""%d"", &pid); if (pid == 0) { return 0; } char buff[BUFF_SIZE]; GetModuleFileNameEx(pid, szDllName, buff, BUFF_SIZE); char* chPtr = strchr(buff, '\\'); // Change the path separator character to a null terminator so we can split it if (chPtr != NULL) { *chPtr = 0; } chPtr = strtok(buff, ""\\""); while (chPtr != NULL) { printf(""%s\n"", chPtr); chPtr = strtok(NULL, ""\\""); } return 0; }",Stack Overflow,https://stackoverflow.com/questions/71152461/reading-memory-from-another-process-in-c-copilots-solution,0
"Im having an issue with my co-pilot installation on Visual Studio 2022, im running the stable community version, and im running it with extensions, of course. However, i HAVE tried disabling AND uninstalling all my extensions, and it will still not work the way it should. Co-pilot works fine on Visual Studio Code, its only Visual Studio 2022 that im having issues with. The exact exception im getting is as follows: [Error] An error occurred for 'ReturnKeyCommandArgs': StreamJsonRpc.RemoteInvocationException: Request getCompletions failed with message: The operation was aborted. at StreamJsonRpc.JsonRpc.<InvokeCoreAsync>d__143`1.MoveNext() --- End of stack trace from previous location where exception was thrown --- at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw() at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task) at GitHub.Copilot.CopilotCommandHandler.<>c__DisplayClass32_0.<<ExecuteCommandCore>b__0>d.MoveNext() --- End of stack trace from previous location where exception was thrown --- at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw() at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task) at Microsoft.VisualStudio.Shell.VsTaskLibraryHelper.<>c__DisplayClass35_0.<<FileAndForget>b__0>d.MoveNext() ( A lot of the same exception above here... ) For my attempts to fix this, I have tried: Uninstalling/Disabling all my extensions Reinstalling Visual Studio 2022 Reinstalling Copilot Tried experimental releases of copilot Cleared Visual Studio Cache Changed default keybinds Unauthorized and then authorized copilot again Tried other coding languages (C#, C/C++, even Python, HTML and PHP (Yes i know its meant to be done in VS Code, its for testing...)) Please if you have had this issue before, and have recently fixed it, please share your solution. I've seen multiple people having the same issue as me, however with no current solution. I would preferebly stay with Visual Studio 2022 specifically, and not older versions or any other third party, as its my main IDE, and i use it for all my needs. EDIT: I did absoloutely nothing, and after a week, i tried to activate copilot globally again, and it magically works... what...",Stack Overflow,https://stackoverflow.com/questions/72750893/co-pilot-doesnt-work-proberly-on-visual-studio-2022-returnkeycommandargs,-1
"I'm working on implementing SSO within the Dynamics Live Chat widget. Here's my code snippet const signInIds = []; window.Microsoft.Omnichannel.LiveChatWidget.SDK.setBotAuthTokenProvider(async (botTokenUrl, callback) => { const urlSearchParams = new URLSearchParams(botTokenUrl); const signInId = urlSearchParams.get(""state""); if (signInIds.includes(signInId)) { // Ignore authenticated sign-in cards callback({show: false}); // Hide card return; } signInIds.push(signInId); const authUrl = """"; // Customer's Authentication API const authResponse = await fetch(authUrl, method: ""POST""}); const {token} = authResponse; // Customer's Auth Token const data = { token: ""token"" }; const payload = { method: ""POST"", headers: { ""Content-Type"": ""application/json"" }, body: JSON.stringify(data) }; try { const botAuthResponse = await fetch(botTokenUrl, payload); // Posts Auth Token to Bot directly // Sign in through Bot is successful if (botAuthResponse.status === 200) { signInIds.push(signInId); // Track authenticated sign-in card callback({show: false}); // Hide card } if (botAuthResponse.status === 404 || botAuthResponse.status == 202) { callback({show: false}); // Hide card return; } else { // Other condition handling } return; } catch (error) { } callback({show: true}); // Show sign-in card by default }); However, I'm having trouble hiding the default login card that appears from Copilot. Do you have any workarounds for this?",Stack Overflow,https://stackoverflow.com/questions/79188136/dynamics-customer-service-live-chat-widget-sso-issue,-1
"You need to use the contribSourceControlInputBoxMenu API proposal. See also its issue ticket: SourceControl - SourceControlInputBoxValueProvider API proposal #195474. The API proposal is current empty. At a previous point in time, it looked like this: /*--------------------------------------------------------------------------------------------- * Copyright (c) Microsoft Corporation. All rights reserved. * Licensed under the MIT License. See License.txt in the project root for license information. *--------------------------------------------------------------------------------------------*/ declare module 'vscode' { // export namespace scm { export function registerSourceControlInputBoxValueProvider(sourceControlId: string, provider: SourceControlInputBoxValueProvider): Disposable; } export interface SourceControlInputBoxValueProviderContext { readonly resourceGroupId: string; readonly resources: readonly Uri[]; } export interface SourceControlInputBoxValueProvider { readonly label: string; readonly icon?: Uri | { light: Uri; dark: Uri } | ThemeIcon; provideValue(rootUri: Uri, context: SourceControlInputBoxValueProviderContext[], token: CancellationToken): ProviderResult<string | undefined>; } } You could take a look at the GitHub Copilot Chat extension's minified code as an example (download VSIX, rename filename extension to "".zip"", unzip). In its extension manifest, it has this: ""scm/inputBox"": [ { ""command"": ""github.copilot.git.generateCommitMessage"", ""when"": ""scmProvider == git"" } ] See also When the API still existed, the way to change the value of the input box was vscode.extensions.getExtension('vscode.git')?.exports.getAPI(1).repositories[0].inputBox.value = newvalue;. Note that later versions of GitHub Copilot started using the scminput language instead of the scm/inputbox menu contribution point (see the github.copilot.enable setting).",Stack Overflow,https://stackoverflow.com/questions/77594965/how-do-i-create-a-custom-button-in-the-git-message-field-of-the-vs-code-source-c,-1
"So, I used Github copilot to generate an AES encryption/decryption snippet. This is the code: package crypto; import javax.crypto.spec.SecretKeySpec; import javax.crypto.Cipher; import java.util.Base64; public class aes{ //aes encryption public static String encrypt(String plainText, String key) { try { Cipher cipher = Cipher.getInstance(""AES/ECB/PKCS5Padding""); SecretKeySpec secretKey = new SecretKeySpec(key.getBytes(""UTF-8""), ""AES""); cipher.init(Cipher.ENCRYPT_MODE, secretKey); byte[] encrypted = cipher.doFinal(plainText.getBytes(""UTF-8"")); return new String(Base64.encodeBase64(encrypted)); } catch (Exception e) { e.printStackTrace(); } return null; } public static String decrypt(String encryptedText, String key) { try { Cipher cipher = Cipher.getInstance(""AES/ECB/PKCS5Padding""); SecretKeySpec secretKey = new SecretKeySpec(key.getBytes(""UTF-8""), ""AES""); cipher.init(Cipher.DECRYPT_MODE, secretKey); byte[] decrypted = cipher.doFinal(Base64.decodeBase64(encryptedText)); return new String(decrypted); } catch (Exception e) { e.printStackTrace(); } return null; } } It looked good, so I pressed TAB. After I tried compiling the main class, this happened: javac aes.java aes.java:13: error: cannot find symbol return new String(new Base64.encodeBase64(encrypted)); ^ symbol: class encodeBase64 location: class Base64 aes.java:24: error: cannot find symbol byte[] decrypted = cipher.doFinal(new Base64.decodeBase64(encryptedText)); ^ symbol: class decodeBase64 location: class Base64 2 errors How can I make these exceptions go away?",Stack Overflow,https://stackoverflow.com/questions/69827293/encodebase64-and-decodebase64-not-visible,-1
"2024-07-29 17:57:35.267 [error] [auth] unable to get local issuer certificate 2024-07-29 17:57:35.437 [error] [default] Error sending telemetry FetchError: unable to get local issuer certificate at fetch at processTicksAndRejections (node:internal/process/task_queues:95:5) at cachingFetch at XO.fetch (c:\Users\co28940.vscode\extensions\github.copilot-1.219.0\lib\src\network\helix.ts:96:22) { type: 'system', _name: 'FetchError', code: 'UNABLE_TO_GET_ISSUER_CERT_LOCALLY', errno: undefined, erroredSysCall: undefined Npm : 10.5.0 Node: 18.20.2 VSCdoe : 1.86.1 I already tried WIN-CA certificate extension is unchecked i downloaded local certificates to git config git config --gloabl false But still facing below issue. not able to login to github copilot",Stack Overflow,https://stackoverflow.com/questions/78809196/github-copilot-setup-in-vscode-2024-07-29-175735-267-error-auth-unable-to,-1
"So, I use a Mac M1. I have a UTM Ubuntu VM set up that I do some of my development on. I have the Github Copilot extension locally on my M1, but when I am sshed onto the Ubuntu VM (through the VSCode remote ssh extension), I am unable to install the Copilot extension. This is the error message pop up I get: Can't install 'github.copilot-chat' extension because it is not compatible with the current version of Visual Studio Code (version 1.75.0). Running code -v on the terminal there gives version 1.75.0. So, I manually went onto my VM, and upgraded VSCode there. On the actual machine, when I have logged on, running code -v gives version 1.88.0 Then, when I switch back to the remote extension, having quit and restarted, I still get the old version: 1.75.0. I tried upgrading VSCode via the command line on the remote ssh extension too, but the version remains the same. My understanding is that the 1.75.0 version is the reason behind me not being able to install the Github Copilot extension on ssh to the Ubuntu VM on vscode. How can I fix the version to solve this problem?",Stack Overflow,https://stackoverflow.com/questions/78284583/upgrade-vscode-version-on-ssh-while-on-remote-extension,-1
"IÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢m developing a new word processor application, and IÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢m interested in the capacities of AI and NLP to guess the end of a sentence a user is currently typing. I would like the AI model to know about the context, i.e. the previous text that has been written (introducing characters, places, and the tone of the text). It would work somehow like GitHub Copilot: the copilot is aware of the rest of the code, and can generate appropriate answers. I imagine two possibilities: fine-tuning a model with the other inputs of the user pass a part of the current text as context to the chat API before asking to complete an unfinished sentence. Solution #1 is scalable (you can train your model with arbitrary amounts of text) and will produce very precise results, but there are two drawbacks: fine-tuning is slow (not the most annoying part), if there are millions of users, it means millions of different fine-tuned models. Can any NLP engine process that as of today? Solution #2 will work great for small texts, but it's suboptimal, as you will have to resend the context every time you start a new conversation with the AI. And the bigger the context, the worse is this solution. How to achieve that with AI? GitHub Copilot is kind of achieving that, so I guess it's possible.",Stack Overflow,https://stackoverflow.com/questions/75947827/in-a-nlp-ai-model-how-to-create-a-persistent-context-per-user,0
"i use vscode ssh connect with linux remote code server the remote server have proxy export it is ok but export no_proxy=github.com it is bad. code server extension copilot still use proxy 1080 to connect github.com when i start up with code --proxy-bypass-list=github.com in windows cmd, it is still failed so, how can i bypass it. Version: 1.76.1 (user setup) Commit: 5e805b79fcb6ba4c2d23712967df89a089da575b Date: 2023-03-08T16:32:00.131Z Electron: 19.1.11 Chromium: 102.0.5005.196 Node.js: 16.14.2 V8: 10.2.154.26-electron.0 OS: Windows_NT x64 10.0.19045 Sandboxed: No [Proxy Support for Extensions should also include NO_PROXY Ãƒâ€šÃ‚Â· Issue #68260 Ãƒâ€šÃ‚Â· microsoft/vscode Ãƒâ€šÃ‚Â· GitHub] ( Setup Visual Studio Code's Network Connection",Stack Overflow,https://stackoverflow.com/questions/75721009/linux-vscode-server-cant-bypass-proxy-specific-domain-when-i-am-connect-in-windo,-1
"I'm curious about how to capture the debug output content by a vs code extension. The situation is, that I'm developing a vs code extension that aims to monitor the debugging process of projects in any programming language, and the core question is, how can I get the debug output content of the software project which is open by the vs code? I checked the APIs of both DebugConsole and DebugSession and have no clue yet. And Copilot told me ""The Debug Console's output is not exposed to the API, likely due to the potential for security risks or performance issues."" but I'm not quite sure if it's hallucination again. Anyone can share your experience on this? That would be very appreciated! Capture VS Code terminal output",Stack Overflow,https://stackoverflow.com/questions/78221241/how-to-capture-debug-output-by-a-vs-code-extension,-1
"I'm trying to run Llama 2 locally on my Windows PC. This is my code here: import torch import transformers model_id = 'meta-llama/Llama-2-7b-chat-hf' device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu' # begin initializing HF items, need auth token for these hf_auth = 'hf_my_auth' model_config = transformers.AutoConfig.from_pretrained( model_id, use_auth_token=hf_auth ) model = transformers.AutoModelForCausalLM.from_pretrained( model_id, trust_remote_code=True, config=model_config, use_auth_token=hf_auth ) model.eval() print(f""Model loaded on {device}"") Pretty basic stuff, and I vaguely remember it working before, but for some reason it keeps giving me a Bitsandbytes-related error. The error is: False ===================================BUG REPORT=================================== C:\Python311\Lib\site-packages\bitsandbytes\cuda_setup\main.py:166: UserWarning: Welcome to bitsandbytes. For bug reports, please run python -m bitsandbytes warn(msg) ================================================================================ The following directories listed in your path were found to be non-existent: {WindowsPath('C'), WindowsPath('/Users/leo/anaconda3/envs/nQM/lib')} C:\Python311\Lib\site-packages\bitsandbytes\cuda_setup\main.py:166: UserWarning: C:\Users\leo\anaconda3\envs\nQM did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths... warn(msg) CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths... The following directories listed in your path were found to be non-existent: {WindowsPath('/usr/local/cuda/lib64')} DEBUG: Possible options found for libcudart.so: set() CUDA SETUP: PyTorch settings found: CUDA_VERSION=118, Highest Compute Capability: 7.5. CUDA SETUP: To manually override the PyTorch CUDA version please see: CUDA SETUP: Loading binary C:\Python311\Lib\site-packages\bitsandbytes\libbitsandbytes_cuda118.so... argument of type 'WindowsPath' is not iterable CUDA SETUP: Problem: The main issue seems to be that the main CUDA runtime library was not detected. CUDA SETUP: Solution 1: To solve the issue the libcudart.so location needs to be added to the LD_LIBRARY_PATH variable CUDA SETUP: Solution 1a): Find the cuda runtime library via: find / -name libcudart.so 2>/dev/null CUDA SETUP: Solution 1b): Once the library is found add it to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_1a CUDA SETUP: Solution 1c): For a permanent solution add the export from 1b into your .bashrc file, located at ~/.bashrc CUDA SETUP: Solution 2: If no library was found in step 1a) you need to install CUDA. CUDA SETUP: Solution 2a): Download CUDA install script: wget CUDA SETUP: Solution 2b): Install desired CUDA version to desired location. The syntax is bash cuda_install.sh CUDA_VERSION PATH_TO_INSTALL_INTO. CUDA SETUP: Solution 2b): For example, ""bash cuda_install.sh 113 ~/local/"" will download CUDA 11.3 and install into the folder ~/local Traceback (most recent call last): File ""C:\Python311\Lib\site-packages\transformers\utils\import_utils.py"", line 1099, in _get_module return importlib.import_module(""."" + module_name, self.__name__) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""C:\Python311\Lib\importlib\__init__.py"", line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""<frozen importlib._bootstrap>"", line 1204, in _gcd_import File ""<frozen importlib._bootstrap>"", line 1176, in _find_and_load File ""<frozen importlib._bootstrap>"", line 1147, in _find_and_load_unlocked File ""<frozen importlib._bootstrap>"", line 690, in _load_unlocked File ""<frozen importlib._bootstrap_external>"", line 940, in exec_module File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed File ""C:\Python311\Lib\site-packages\transformers\models\llama\modeling_llama.py"", line 32, in <module> from ...modeling_utils import PreTrainedModel File ""C:\Python311\Lib\site-packages\transformers\modeling_utils.py"", line 86, in <module> from accelerate import dispatch_model, infer_auto_device_map, init_empty_weights File ""C:\Python311\Lib\site-packages\accelerate\__init__.py"", line 3, in <module> from .accelerator import Accelerator File ""C:\Python311\Lib\site-packages\accelerate\accelerator.py"", line 35, in <module> from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state File ""C:\Python311\Lib\site-packages\accelerate\checkpointing.py"", line 24, in <module> from .utils import ( File ""C:\Python311\Lib\site-packages\accelerate\utils\__init__.py"", line 131, in <module> from .bnb import has_4bit_bnb_layers, load_and_quantize_model File ""C:\Python311\Lib\site-packages\accelerate\utils\bnb.py"", line 42, in <module> import bitsandbytes as bnb File ""C:\Python311\Lib\site-packages\bitsandbytes\__init__.py"", line 6, in <module> from . import cuda_setup, utils, research File ""C:\Python311\Lib\site-packages\bitsandbytes\research\__init__.py"", line 1, in <module> from . import nn File ""C:\Python311\Lib\site-packages\bitsandbytes\research\nn\__init__.py"", line 1, in <module> from .modules import LinearFP8Mixed, LinearFP8Global File ""C:\Python311\Lib\site-packages\bitsandbytes\research\nn\modules.py"", line 8, in <module> from bitsandbytes.optim import GlobalOptimManager File ""C:\Python311\Lib\site-packages\bitsandbytes\optim\__init__.py"", line 6, in <module> from bitsandbytes.cextension import COMPILED_WITH_CUDA File ""C:\Python311\Lib\site-packages\bitsandbytes\cextension.py"", line 20, in <module> raise RuntimeError(''' RuntimeError: CUDA Setup failed despite GPU being available. Please run the following command to get more information: python -m bitsandbytes Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes and open an issue at: The above exception was the direct cause of the following exception: Traceback (most recent call last): File ""C:\Users\leo\python\test.py"", line 15, in <module> model = transformers.AutoModelForCausalLM.from_pretrained( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""C:\Python311\Lib\site-packages\transformers\models\auto\auto_factory.py"", line 492, in from_pretrained model_class = _get_model_class(config, cls._model_mapping) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""C:\Python311\Lib\site-packages\transformers\models\auto\auto_factory.py"", line 376, in _get_model_class supported_models = model_mapping[type(config)] ~~~~~~~~~~~~~^^^^^^^^^^^^^^ File ""C:\Python311\Lib\site-packages\transformers\models\auto\auto_factory.py"", line 666, in __getitem__ return self._load_attr_from_module(model_type, model_name) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""C:\Python311\Lib\site-packages\transformers\models\auto\auto_factory.py"", line 680, in _load_attr_from_module return getattribute_from_module(self._modules[module_name], attr) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""C:\Python311\Lib\site-packages\transformers\models\auto\auto_factory.py"", line 625, in getattribute_from_module if hasattr(module, attr): ^^^^^^^^^^^^^^^^^^^^^ File ""C:\Python311\Lib\site-packages\transformers\utils\import_utils.py"", line 1089, in __getattr__ module = self._get_module(self._class_to_module[name]) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""C:\Python311\Lib\site-packages\transformers\utils\import_utils.py"", line 1101, in _get_module raise RuntimeError( RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback): CUDA Setup failed despite GPU being available. Please run the following command to get more information: python -m bitsandbytes Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes and open an issue at: I'm getting an error because Bitsandbytes only works on Linux, not Windows, which is fine, except that I'm not using it at all and it's still coming up. The only installs I ran in my environment were pip install transformers==4.31.0, pip install accelerate == 0.21.0, and pip install torch==2.1.0+cu118 --index-url Does transformers need Bitsandbytes to run? If so, is there another way to run Llama, besides using a UI? Again, I'm pretty sure I got this to work before, but when I tried to run it again with the exact same script in the exact same environment it gave me the same error. This might be some really stupid thing that I'm not seeing, but I've been trying for hours, googling things as well as asking Microsoft Copilot a million questions to no avail. Any help would be appreciated. Thank you. I've tried reduced my code to the bare minimum and creating a new barebones environment, but I keep getting the same error. I've also spent hours trying to debug with Microsoft Copilot and Google but that hasn't helped either.",Stack Overflow,https://stackoverflow.com/questions/77843057/unable-to-run-huggingface-transformers-without-bitsandbytes-on-windows,-1
"I'm trying to automate some tasks within VSCode, specifically with a particular extension. My particular interest is I want to collect outputs from Here's what I aim to achieve: Load a project. Open a file and navigate to a specified location within the file. I already have the locations of the file where the cursor needs to be for getting the auto-completion suggestions Trigger auto-completion at that location. Collect the results of the auto-completion. Save those results to a file. Is there any way to programmatically control a VSCode extension to perform these steps? I want to do this for Github Copilot and huggingface-vscode specifically. I am okay with a solution that uses an external program (let's say Python code) other than VS Code to drive VS Code to get the desired result. Example of the screen is attached what I have to capture.",Stack Overflow,https://stackoverflow.com/questions/76898916/how-can-i-programmatically-control-a-vs-code-ai-suggestions-extension-to-extract,-1
"I am trying to use GraphAPI v5 in a Blazor .NET 8.0 application, to request a list of all users who have a certain AppRole assigned. I configure the roles in Entra Admin Centre > Enterprise applications > (my app) > Users and Groups, and assign users to Roles that I defined in the application registration. I have tried the following code in a Razor page: @inject GraphServiceClient GSC @inject MicrosoftIdentityConsentAndConditionalAccessHandler ConsentHandler // ... protected async Task AzTest() { try { var users = await GSC.Users.GetAsync((requestConfiguration) => { requestConfiguration.QueryParameters.Expand = new string[] { ""appRoleAssignments"" }; }); } catch(Exception ex) { ConsentHandler.HandleException(ex); } } I also found (thanks to commentor) that Directory.Read.All was a required Scope for this call to work. My question is: how do I filter the result to find just the users with a specific Role? My organization has over 1000 users. I have tried the following approaches: Approach 1 - Filter Guid roleIdGuid = new Guid(""5f61fba7-ae07-4438-9f41-4c00b2540463""); // ... requestConfiguration.QueryParameters.Filter = $""appRoleAssignments/any(ar: ar/appRoleId eq {roleIdGuid})""; (this was suggested by Copilot). This gives a runtime error : Microsoft.Graph.Models.ODataErrors.ODataError: Property 'appRoleId' does not exist as a declared property or extension property. which Copilot suggests means that the API does not actually support filtering by App Role ID. Approach 2 - Pagination Another approach is to retrieve all users via Pagination, and then go through them in C# to check for the role ID. However, the pagination doesn't seem to work. Using the exact code from the V5 documentation: var usersResponse = await graphServiceClient .Users .GetAsync(requestConfiguration => { requestConfiguration.QueryParameters.Expand = new string[] { ""appRoleAssignments"" }; requestConfiguration.QueryParameters.Top = 1; }); var userList = new List<User>(); var pageIterator = PageIterator<User,UserCollectionResponse>.CreatePageIterator(graphServiceClient,usersResponse, (user) => { userList.Add(user); return true; }); await pageIterator.IterateAsync(); // (omitted the later filtering code for brevity) This ran but I waited for over 90 seconds and it was still making API calls and responses on the await part; I'm not sure if it was stuck in an infinite loop somehow or just taking forever, but at any rate, that sort of delay isn't an acceptable solution.",Stack Overflow,https://stackoverflow.com/questions/78762730/graphapi-request-names-of-users-that-have-a-certain-app-role,-1
"appRoleId doesn't support filter from the API document. As you can see that principalDisplayName supports filter. So that I agree with you that we need to get all the users and then get the users assigned specific roles by ourselves. About the Graph API Pagination, we could set page size to 999 to minimize the number of requests and I'm afraid top 1 is the reason why you waited for so long. Then our codes shall be similar to var usersResponse = await graphServiceClient.Users.GetAsync(requestConfiguration => { requestConfiguration.QueryParameters.Expand = new string[] { ""appRoleAssignments"" }; requestConfiguration.QueryParameters.Top = 999; }); var userList = new List<User>(); var pageIterator = PageIterator<User,UserCollectionResponse>.CreatePageIterator(graphServiceClient,usersResponse, (user) => { userList.Add(user); return true; }); await pageIterator.IterateAsync(); List<User> result = new List<User>(); foreach (var tempUser in userList) { var roles = tempUser.AppRoleAssignments; foreach(var role in roles) { if (role.AppRoleId.ToString() == ""role_id_here"") { result.Add(tempUser); break; } } }",Stack Overflow,https://stackoverflow.com/questions/78762730/graphapi-request-names-of-users-that-have-a-certain-app-role,-1
"This answer summarises discussion from comments There is currently (July 2024) no way to efficiently list all users with a particular App Role; the only way is to enumerate all users and check their role at application level, which is unacceptably slow if the tenant contains 20,000+ users for example. Instead, the system could be designed to use Groups instead of App Roles . The API does support listing members of a Group.",Stack Overflow,https://stackoverflow.com/questions/78762730/graphapi-request-names-of-users-that-have-a-certain-app-role,-1
"Below is the code I'm compiling: #include <stdio.h> void main() { printf(""Hello%cWorld\n"", 0); } Below is the compiler output: HelloWorld Bing Chat-GPT response:start Below is the same output by Bing chat-gpt in creative mode Copilot: The output of printf(""Hello%cWorld\n"", 0) will be: Hello The reason is that the %c format specifier expects a single character as an argument, but you provided 0 instead. This is equivalent to the null character '\0', which marks the end of a string in C1. Therefore, the printf function will stop printing the string after encountering the null character, and ignore the rest of the characters. To print the whole string as it is, you need to use the %s format specifier, which expects a string as an argument. For example: printf(""Hello%sWorld\n"", ""\0""); This will print: Hello\0World Alternatively, you can escape the backslash character with another backslash, like this: printf(""Hello\\0World\n""); This will also print: Hello\0World You: but when I compiled the above code it gave me output as HelloWorld Copilot: That is strange. The code you posted should give a warning and print only Hello. Are you sure you compiled the same code? Maybe you have a different version of the compiler or the standard library that handles the null character differently. Can you show me the exact command you used to compile and run the code? ÃƒÂ°Ã…Â¸Ã‹Å“Ã…Â You: I have used gcc main.c -o app to compile Copilot: I see. And how did you run the app? Did you use any command-line arguments or input redirection? ÃƒÂ°Ã…Â¸Ã‹Å“Ã…Â You: I ran the app using ./app command I did not use any command-line arguments or input redirection Copilot: I see. Well, IÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢m not sure why you got a different output than expected. According to the C standard, the behavior of printf with a null character argument is undefined1. This means that different compilers or platforms may handle it differently, and there is no guarantee of consistency or correctness. Maybe your compiler or platform has some special treatment for the null character, or maybe you encountered some memory corruption or other bug. To avoid this kind of problem, I suggest you always use valid arguments for the printf function, and follow the rules of the format specifiers. You can find more information about the printf function and its format specifiers in this article2. I hope this helps. If you have any other questions, feel free to ask me. ÃƒÂ°Ã…Â¸Ã‹Å“Ã…Â 1: warning: format string contains ÃƒÂ¢Ã¢â€šÂ¬Ã‹Å“\0ÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ within the string body [-Wformat] 2: printf in C - GeeksforGeeks Bing Chat-GPT response:end Please note GCC version: gcc -v Using built-in specs. COLLECT_GCC=gcc COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/9/lto-wrapper OFFLOAD_TARGET_NAMES=nvptx-none:hsa OFFLOAD_TARGET_DEFAULT=1 Target: x86_64-linux-gnu Configured with: ../src/configure -v --with-pkgversion='Ubuntu 9.2.1-9ubuntu2' --with-bugurl=file:///usr/share/doc/gcc-9/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++,gm2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-9 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none,hsa --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu Thread model: posix gcc version 9.2.1 20191008 (Ubuntu 9.2.1-9ubuntu2) I wanted to learn, in printf(""Hello%cWorld\n"", 0) here %c expects matching int argument. Hence I gave 0, this turns out to be NULL with respect to ASCII characters. I need to know why there are two different outputs i.e. compiler prints HelloWorld & chat-gpt says only Hello should be printed. Please help to explain this behavior ? Thanks.",Stack Overflow,https://stackoverflow.com/questions/78021878/compiler-prints-helloworld-whereas-chat-gpt-says-it-should-only-print-hello,0
"I asked Microsoft's Copilot a query to write a sample web-service accepting employee number and return name of the employee against the supplied employee number from database. Here is script of the sample table in database. I have added a few records in the database for testing. CREATE TABLE employees ( EmpNumber INT PRIMARY KEY, EmpName VARCHAR(100) ); Here is the code for service.php, a file which is supposted to connect with databse and returns output to the client: <?php require_once('nusoap/lib/nusoap.php'); // Database credentials $host = 'yourHostName'; $db = 'your_db_ name'; $user = 'your_database_user'; $pass = 'your_password'; // Create a new soap server $server = new nusoap_server(); // Configure the WSDL $server->configureWSDL('employeeService', 'urn:employeeService'); $server->wsdl->schemaTargetNamespace = 'urn:employeeService'; // Register the method to expose $server->register('getEmployeeName', array('employeeNumber' => 'xsd:int'), // input parameters array('return' => 'xsd:string'), // output parameters 'urn:employeeService', // namespace 'urn:employeeService#getEmployeeName', // soapaction 'rpc', // style 'encoded', // use 'Returns the name of the employee' // documentation ); // Define the method as a PHP function function getEmployeeName($employeeNumber) { $name = ''; global $host, $db, $user, $pass; // Connect to the database $conn = new mysqli($host, $user, $pass, $db); // Check connection if ($conn->connect_error) { return 'Database connection failed: ' . $conn->connect_error; } // Prepare the SQL query $sql = ""SELECT EmpName FROM employees WHERE EmpNumber = ?""; $stmt = $conn->prepare($sql); $stmt->bind_param(""i"", $employeeNumber); $stmt->execute(); $stmt->bind_result($name); $stmt->fetch(); $stmt->close(); $conn->close(); // Check if the employee was found if ($name) { return $name; } else { return 'Employee not found'; } } // Use the request to (try to) invoke the service $server->service(file_get_contents(""php://input"")); ?> Here is the code for client.php. I have added try-catch block to capture any exceptions. <?php # URL of the web service $url = ' // SOAP request JSON $empID = 1; $data = '{ ""employeeNumber"": '.$empID.' }'; $options = array( CURLOPT_URL => $url, CURLOPT_RETURNTRANSFER => true, CURLOPT_ENCODING => '', CURLOPT_MAXREDIRS => 10, CURLOPT_TIMEOUT => 0, CURLOPT_FOLLOWLOCATION => true, CURLOPT_HTTP_VERSION => CURL_HTTP_VERSION_1_1, CURLOPT_CUSTOMREQUEST => 'POST', CURLOPT_POSTFIELDS => '{""data"": '.$data.'}', CURLOPT_HTTPHEADER => array( ""Content-Type: application/json; charset=utf-8"" ) ); try{ $ch = curl_init(); # cURL initialization curl_setopt_array($ch, $options); $response = curl_exec($ch); # Execute the request curl_close($ch); # Close cURL echo $response; # Display the response } catch (Exception $ex) { echo ""An exception has occured: "".$ex->getMessage(); } ?> And finally, below is the XML for employeeService.wsdl. <?xml version=""1.0"" encoding=""UTF-8""?> <definitions name=""employeeService"" targetNamespace=""urn:employeeService"" xmlns:tns=""urn:employeeService"" xmlns:soap="" xmlns="" <message name=""getEmployeeNameRequest""> <part name=""employeeNumber"" type=""xsd:int""/> </message> <message name=""getEmployeeNameResponse""> <part name=""name"" type=""xsd:string""/> </message> <portType name=""employeeServicePortType""> <operation name=""getEmployeeName""> <input message=""tns:getEmployeeNameRequest""/> <output message=""tns:getEmployeeNameResponse""/> </operation> </portType> <binding name=""employeeServiceBinding"" type=""tns:employeeServicePortType""> <soap:binding style=""rpc"" transport="" <operation name=""getEmployeeName""> <soap:operation soapAction=""urn:employeeService#getEmployeeName""/> <input> <soap:body use=""encoded"" namespace=""urn:employeeService"" encodingStyle="" </input> <output> <soap:body use=""encoded"" namespace=""urn:employeeService"" encodingStyle="" </output> </operation> </binding> <service name=""employeeService""> <port name=""employeeServicePort"" binding=""tns:employeeServiceBinding""> <soap:address location="" </port> </service> </definitions> When I execute the script in the browser, the script displays web-page whose screenshot is as below. There is neither any error log entry nor any exception thrown. Is there anything wrong with the script or with the way I am calling the service? PS: I have changed input from XML to JSON. This time, an error log has exactly six entries of the line as below: [17-Feb-2025 19:32:47 Asia/Kolkata] PHP Warning: Cannot modify header information - headers already sent by (output started at /path/to/service/api/demo/service.php:1) in /home2/kbcsurve/public_html/cdn/classes/nusoap/lib/nusoap.php on line 4272 Can the script be fixed now?",Stack Overflow,https://stackoverflow.com/questions/79442101/unable-to-consume-php-nusoap-web-service,6
"they are violating my licenseit's much more likely the generated code fragments violate some patents. Being a paid service while training on free code is unethical in my opinionon the other hand everyone seems to take it for granted that they provide free services for developers.EDIT: I spend all of my spare time to open source projects (see and really don't see why something like Copilot shouldn't use my code; and the free services Github provides are really helpful for open source.EDIT 2: The comments in this discussion suggest that community in this subreddit suffers from a frightening delusion and ignorance regarding licensing and copyright, combined with an almost presumptuous attitude of entitlement; people seem to take it for granted that others provide them code or services for free; but at the slightest suspicion that they should give something away, all hell breaks loose. I can only hope that this is not representative of a new generation of open source developers.",Reddit,https://www.reddit.com/r/opensource/comments/y75jy1/github_copilot_investigation/,2
"Just to clarify: I appreciate that they provide the service for free, but at the same time this doesn't give them the right to violate licenses.If using copilot is not violating licenses, why didn't they use their proprietary software in the training?I still can't make my mind on copilot, I'm actually more on the against side.",Reddit,https://www.reddit.com/r/opensource/comments/y75jy1/github_copilot_investigation/,1
I think if copilot was also free and only used open source free code that allowed it to train off of it it would be different.It's a paid service that violated licenses so that's the issue....,Reddit,https://www.reddit.com/r/opensource/comments/y75jy1/github_copilot_investigation/,1
"Exactly what primacora said. With Dalle2 and OpenAI people are entering hyper specific terms to get hyper specific output. For example ""make me this , in the style of "". While copilot and dalle might claim that the output is generative, and not derivative...with the right input, you can force the system into producing a derivative output.What i'm saying is the same tactic could be used to subvert the GPL. If you can use the defense ""copilot wrote it, I didn't"" then if you then you can use copilot to launder any code regardless of license. Do you dislike the GPL?The level of like or dislike of a specific license should have no bearing on the impacts of subverting it. I chose GPL because people are familiar with it in this sub, especially when it comes to thinking of how a corporation might want to violate its license.",Reddit,https://www.reddit.com/r/opensource/comments/y75jy1/github_copilot_investigation/,5
"In my opinion, it violates most licenses (violates as in not comply to the license). Even licenses like MIT require to give attribution, which copilot isn't doing.The GPL requires that you license under GPL if you include any part of the code in your code, but copilot uses GPL code without indicating its origin.",Reddit,https://www.reddit.com/r/opensource/comments/y75jy1/github_copilot_investigation/,5
"The trouble is, something is legally a copy if a jury can be persuaded that it is. A jury, usually, of people who know nothing about software at all.There have been a number of cases of alleged music copyright infringement, where the case has relied on ""the melody of song X so closely resembles that of song Y that it must be a copy"". Some examples In many of these cases, no solid evidence is presented that the writers of X heard or even knew about Y. And the trouble is that there's really only so many ways to arrange notes and chords and beats (and still produce something ordinary people will find enjoyable). cf songwriters have taken to recording everything they play as they write songs (which can involve weeks of messing around, jamming, experimenting, and iterating). So they can show some future jury all the illformed prototypes, and they're not relying on the claim that they magically sat down and the melody just poured out of their fingers.Software has a similar issue, at least in the small scale. There's only so many ways to implement a hash table or an LRU cache or calculate the number of seconds between two dates. Doubly so when you're implementing a standard or specification (cf the SCO/Linux errno.h issue is the worry about Copilot. I'd really want to be able to swear to a jury that I'd written all the code myself, and show them all the git deltas for all the broken and halfdone versions. But if some chunk of software, even a handful of lines, as been ""invented"" by Copilot, it has magically appeared (as far as the jury is concerned) from somewhere. Then an expert witness says that Copilot learns (by copying) from other code. The plaintiff's code, they'll say. So even though I didn't know anything about the plaintiff's code, and even if there's no evidence that it's the code that specifically influenced Copilot to emit the problematic code fragment, I'd run the risk that the jury (who have no idea how to implement a hashtable or how similar one person's implementation might be to another) will believe that Copilot just copied the code.",Reddit,https://www.reddit.com/r/opensource/comments/vidiq2/github_copilot_legally_stealingselling_licensed/,4
"I believe GitHub itself is in violation when they share code verbatimAlso, the code Copilot inserts is in your project, you accepted it (and the copyright of your project is yours) and you have to prove it was written by Copilot (if it even changes anything...).If a large body of code is inserted by Copilot instead of just a line or two it may be subject to copyright issues... I won't use Copilot or other similar AIs in my code. Maybe an AI search tool that instead of inserting the code it shows you the original code on the web (alongside the used license) to allow you to take a cue would be a better product...",Reddit,https://www.reddit.com/r/opensource/comments/vidiq2/github_copilot_legally_stealingselling_licensed/,-1
"I release code as open source so people other than me can benefit from it. If Copilot learns a pattern from my code and it helps someone else get something done just a bit more quickly or easily, that's terrific and I'm delighted that Copilot has made my code useful outside of its intended niche.And if they make money on it? Cool, I've helped contribute to a net increase in the amount of value in the world. I'm not any worse off than I would have been if Copilot hadn't scanned my code. Open source isn't a zerosum game and I've benefitted from other people's code too.That said: Copilot should not violate licenses and if it does that, there's a real problem. I choose to release my code with very permissive licenses that it'd be more or less impossible for Copilot to ever violate, but other people choose to restrict usage of their code in various ways. That's each author's decision to make, and Microsoft has both a legal and an ethical obligation to respect the licenses people choose.",Reddit,https://www.reddit.com/r/opensource/comments/skpwue/is_github_copilot_causing_people_to_consider/,7
"My code is mostly GPLed. Even if the legalities are TBC, copying my code when I don't give permission to do so is an ethical violation, and I hope that people using Copilot take this onboard, as well as the potential legal risk associated.No plans to move to anything else, GitHub provides a lot of value.",Reddit,https://www.reddit.com/r/opensource/comments/skpwue/is_github_copilot_causing_people_to_consider/,-1
"Yes and no, as to not safe.If you have a contract with Microsoft / GitHub then the version of Copilot( enterprise or for business) means they will not be storing your data and that the data cannot be used for training.If you’re using it for and connected to your personal account (aka free) then that is where the risks comes into play.But the plugin risk here, is the same with any other plugin a software engineer can install anyhow. It is just a question of plausible deniability (indemnification and approval) that you need to cover.A security policy such as “do not store sensitive information like credentials or passphrases in files, when working with sensitive information then disable plugins” can help. Your company should also be using an intermediate network scanning software (like with your VPN) to monitor that people aren’t sending your credentials to third parties anyhow.And this isn’t just a copilot issue, but any and all GenAI plugins are doing the literal same thing. There is not a single one which allows you the ability to control (based upon string entropy) what can and is not allowed to be sent.The plugin is going to tokenize the content of the open tabs, and in other cases the referenced files (eg C using includes) available in order to give the AI model context outside of the chat and autocomplete.So if you have a token, cert or credentials, then this will be sent over the line. If you are using Copilot for Business or Enteprise, then there are filters which supposedly work and block certain files (eg secrets.txt) from being tokenized. Doesn’t work though, lol.",Reddit,https://www.reddit.com/r/github/comments/1gnagaa/is_integrating_github_copilot_in_my_organisation/,-1
"I don't understand why people see this as different than opening your ""confidential code base"" in vs code. If MS wanted to steal your codebase, they already could have by using VS Code as the conduit (or obviously GitHub).Even if they train their model based on your code (which is OFF by default), do you actually think somebody could figure out what your software does by asking copilot prompts? I don't know maybe, but I highly doubt it. Prove it to me.If you need that level of confidentiality, you should enforce it on a network level (like defense contractors).",Reddit,https://www.reddit.com/r/github/comments/tto314/would_you_use_github_copilot_with_confidential/,6
"Yea most devs during the dev process won’t do that, they won’t do that until they are ready to deploy to an environment. So while they are using copilot their snippets can make it back in the model.With that being said, you should be using keys you plan to rotate and are only test keys during that stage anyways, but again, most devs won’t do that, let alone rotate out keys or disable them afterwards. So to answer your question yes it often feels like we work with children.I’m glad you consider security first though, makes people like me happy.",Reddit,https://www.reddit.com/r/github/comments/tto314/would_you_use_github_copilot_with_confidential/,6
"Thank you! It seems like they've stopped doing those webinars unfortunately, so it may not be possible to do it for free anymoreFor context, I've already done the GitHub Foundations, Advanced Security and Actions certs, and your advice is spoton. I'm just a bit annoyed that I paid full price for AS and Actions (I got Foundations for half price), and I was looking for a way to get the Admin and Copilot ones cheaperThere does seem to be a webinar on Copilot which I've signed up for, so we'll see what happens. Thanks again mate!",Reddit,https://www.reddit.com/r/github/comments/1f4g2td/passed_all_5_github_exams_in_1_month/,10
"1. Yes, although for most situations this requires an Advanced Security License. Doesn’t even need to be a git repo.2. The CodeQL team often writes and tests queries by running them against open source repos to verify they find real world vulnerabilities (“variant analysis”). The article is about how they used AI to speed up that process. It’s not a userfacing feature AFAIK.3. See 2 😀4. CLI, no. However Copilot tech is used to generate suggested fixes for alerts within the GitHub website. It’s licensed as part of Advanced Security.",Reddit,https://www.reddit.com/r/github/comments/1gpz2zg/confused_by_codeql_cli_copilot_and_aiassisted/,3
"This is why I set up an organization on GitHub and pay for GitHub Copilot Business. I have more faith that GitHub will not use my code for training or for accidentally leaking actual internal company secrets. (My job doesn’t pay for copilot, so I do myself)",Reddit,https://www.reddit.com/r/github/comments/1hh8krf/friendly_reminder_to_turn_off_this_option_if_you/,10
Mostly because there is a “guarantee” code will not be leaked or used for training. There isn’t even a checkbox like in your screenshot to check/uncheck when using Copilot Business. Which is important for work I do for my employer or any freelance work I might do.,Reddit,https://www.reddit.com/r/github/comments/1hh8krf/friendly_reminder_to_turn_off_this_option_if_you/,1
"It's always surprised me how popular Copilot has become.There's a popular joke that goes something like ""technically literal people only have one computer in their house, and they keep a shotgun next to it in case it goes rogue"", and yet when Microsoft (of all companies) introduces a neural network powered snippet engine, everyone goes ""yass inject it into my veins"".",Reddit,https://www.reddit.com/r/github/comments/vhhmi3/end_of_free_github_copilot/,1
"If it was bundled with Advanced Security it would have been a nobrainer but the current license structure. For now it is a nogo for me as Copilot got more and more stupid over time during the beta.The license structure really needs to be reviewed as there is nothing between team and enterprise. Then again, GitHub is way cheaper than GitLab.",Reddit,https://www.reddit.com/r/github/comments/vhhmi3/end_of_free_github_copilot/,10
"OpenAI Codex was trained on publicly available source code and natural language, so it works for both programming and human languages. The GitHub Copilot extension sends your comments and code to the GitHub Copilot service, and it relies on context, as described in[Privacy]( i.e., file content both in the file you are editing, as well as neighboring or related files within a project. It may also collect the URLs of repositories or file paths to identify relevant context. The comments and code along with context are then used by OpenAI Codex to synthesize and suggest individual lines and whole functions.This?I don't see anything here that says works developed with copilot must use an open source license.",Reddit,https://www.reddit.com/r/github/comments/vhhmi3/end_of_free_github_copilot/,-1
"There's an enterprise version of Copilot that segregates an organization's data from all other customers. Microsoft uses this internally to protect their data from leaking out through Copilot. If your employer doesn't pay for the data segregation, which I think is just the default for Copilot for enterprise, then its data could be comingled with public data.Here's the [Copilot For Enterprise site]( if you want to look through the marketing materials for yourself.",Reddit,https://www.reddit.com/r/programming/comments/1gijbgw/is_copilot_a_huge_security_vulnerability/,1
"Yes, just watch this more about a copilot integrated within o365 but still very valuable lecture. Targeted and automated phishing attacks has gotten much easier and I don't see any way to make this secure without rendering this systems to being borderline useless.People are coming with new and new jailbreaks each day and it looks like it's not entirely possible to get rid off all of the options to persuade LLM somehow to do what malicious agent wants.People used to be the weakest link in the security. Now it's going to be LLM and People, what's worse, LLM can be persuaded easier in some circumstances because they don't have any reasoning abilities.",Reddit,https://www.reddit.com/r/programming/comments/1gijbgw/is_copilot_a_huge_security_vulnerability/,1
"Big difference between individual and enterprise. Do not use personal license unless it is truly personal and not company/confidential. Just had a meeting with GitHub copilot and that is the biggest takeaway is that it is a private instance that will in no way, shape or form use our own code to train their models. Individual doesn’t have that option!",Reddit,https://www.reddit.com/r/programming/comments/1gijbgw/is_copilot_a_huge_security_vulnerability/,-1
"We had an initiative to get rid of all passwords and token from the code in order to prepare codebase for copilot, so there's something to it. Weirdly enough I got access to copilot in order to evaluate it before we got rid of everything lol.",Reddit,https://www.reddit.com/r/programming/comments/1gijbgw/is_copilot_a_huge_security_vulnerability/,6
"At least as per this: So, in the case that you retain untracked files like a .env that populates environment variables, when opening it, copilot will send this file to the cloud exposing your development credentials.I would not be surprised if there's some kind of default exclusion built in that sees something that looks like a key and just yeets it from the set sent to a server. People put keys out in the open, plain and simple, and it's just something you design for.",Reddit,https://www.reddit.com/r/programming/comments/1gijbgw/is_copilot_a_huge_security_vulnerability/,6
"I remember not long ago an instructor at a school I'm attending ""forced"" me and a class to use github copilot. What I didn't like was that he didn't mention there's a subscription for using the extension. Seriously, I don't think I need copilot for Github. Maybe if it's used for free on in any case, a way to ease workflow. But I've been to other teachers who didn't require to have copilot. If Github copilot is a security risk, then probably I won't be thinking of using it.",Reddit,https://www.reddit.com/r/programming/comments/1gijbgw/is_copilot_a_huge_security_vulnerability/,10
"My company built on premises azure to move our sensitive stuff onto our own hardware. We have onpremises GitHub and copilot.Despite that, I think there's still projects you can't use copilot with.Research showing how to get unique sensitive data embedded during model training hasa lot of companies walking the edge between the hype of AI reducing costs and the massive expense of a security breach.",Reddit,https://www.reddit.com/r/programming/comments/1gijbgw/is_copilot_a_huge_security_vulnerability/,1
"Yea, where I work (enterprise SaaS company) we were not initially allowed to use any LLM AI, then we signed commercial licenses for copilot and chatgpt. Both of these contracts confirmed that our internal data would not be used for training or otherwise reflected externally.One nice thing about copilot licensing is that it's based on the github organization for the company being associated with the github user account, so it works on my personal PC / vscode setup too, not just my work computer or work project repos.",Reddit,https://www.reddit.com/r/programming/comments/1gijbgw/is_copilot_a_huge_security_vulnerability/,-1
Came to say this. GitHub Copilot is the most secure tool on the market. If people are worried about security vulnerabilities they should immediately stop using other AI code gen tools…,Reddit,https://www.reddit.com/r/programming/comments/1gijbgw/is_copilot_a_huge_security_vulnerability/,0
"Nobody should put credentials in their codebase, no matter copilot or not",Reddit,https://www.reddit.com/r/programming/comments/1gijbgw/is_copilot_a_huge_security_vulnerability/,6
"Isn’t this just a hack to say that Copilot shouldn’t be allowed to be in the same directory as those files? If I need to use another program to open a file I don’t want an application to have access to, then I’m essentially treating Copilot as malware at that point",Reddit,https://www.reddit.com/r/programming/comments/1gijbgw/is_copilot_a_huge_security_vulnerability/,-1
"Your data is encrypted with before being sent over the wire. Security in flight isn't the issue. If microsoft themselves wants to steal your data, there are other big concerns to look at, like onedrive, first.Why do I have to pick just one? Don't put your data on onedrive either.Definitely they are training Copilot with it (even if not ChatGPT), it wouldn't work otherwise. You have to hope they somehow isolate the effects of training on your code from other Copilot instances. I didn't see if they guarantee that, did you? I have to imagine unless you are a medium to large customer you can't get your own separate instance at a cost that makes sense.",Reddit,https://www.reddit.com/r/programming/comments/1gijbgw/is_copilot_a_huge_security_vulnerability/,1
"Eh, I've worked on more than one project where I needed clearance and had to go into SCIFs to the support the project, but the actual codebases were entirely open sourced. The code I committed every day lived on a publicly accessible GitHub page. Copilot wasn't available at the time, but I have no idea if I would've been technically allowed to use it for that code. Asking is the only way to find out. (As far as I understand, Copilot is now explicitly trained on this code as it's public on GitHub!)And I'm not sure I agree with your characterization of clearancetype positions. Your number one priority is always supporting the mission. You can't support the mission if you damage national security and spill data, but you're also doing a poor job supporting your mission if you're not communicating and working efficiently. Working efficiently doesn't mean working without care, either. If you know there's a tool that will help you work better, and never ask if you can use it, you're doing something wrong, unless you have been explicitly informed that you can't.Point being, even in cleared positions things aren't always cut and dry, and it's not always obvious what is permitted or is considered taboo. The number one rule in security is if you're not sure about something, ask! Teams exist for this reason, and anybody responding to a teammate like the above commenter is frankly just being a bad teammate (and for why????)If somebody on my team ever responded to a question in that way, they're getting immediately chewed out, and I'm not normally one to chew anybody out. Mistakes happen, but that behavior is a decision. All that to say, I am squarely against anybody that puts anybody down for asking questions.",Reddit,https://www.reddit.com/r/programming/comments/1gijbgw/is_copilot_a_huge_security_vulnerability/,10
"As far as I understand, Copilot is now explicitly trained on this code as it's public on GitHub!Which opens up another attack vector. Just upload loads of subtly malicious code, `ifdef`'d out so it doesn't cause visible issues but still readable by the AI.",Reddit,https://www.reddit.com/r/programming/comments/1gijbgw/is_copilot_a_huge_security_vulnerability/,0
"Basically, boilerplate = patterns repeated in multiple places in your project.Sorry, you're stating that LLMs are useful because they violate the DRY principle. This may explain why research is showing that tools like [GitHub Copilot are increasing bug rates and that's leading to a loss in all of the perceived productivity gains](",Reddit,https://www.reddit.com/r/programming/comments/1gijbgw/is_copilot_a_huge_security_vulnerability/,0
"This is an interesting take. What language are you writing in where you don't have boilerplate, or otherwise simple code that you need but would rather not type? Copilot is auto complete but just better, and more. My impression based on your comment is that... you've just never used AI tools. They're good!If in C I write out: public int XCoordinate;Regular auto complete isn't doing anything to help that. Copilot is going to correctly guess I want YCoordinate next. And guess what, it's probably going to guess that I want Z after that. Is that a huge time save? No. But do that 100+ times a day with random little things, for 40 hours a week, over years, and you have massive time/mental savings.Also, if you move between languages/frameworks frequently, you don't have to waste as much time remembering the exact syntax you need or the name of the math function you want to call. I'm not a genius, I don't have infinite mental bandwidth. I know what I need my code to do, copilot can predict how I need to type it. I can type out a comment in English, hit enter, and copilot will 99 times out of 100 have exactly the line I needed, and my code has the added benefit of being rife with descriptive comments, explained in plain English. If you try to use copilot to generate entire functions, you're probably going to have a bad time. But if you're using it to speed things up, it's very, very effective. There are security concerns with the concept, but if you take those away and still think it's not a great tool, you're being deliberately dismissive. I've been using copilot essentially since it's been available and it has been nothing but a productivity boost for me. I can't use it professionally as much because I work on secure projects, but in personal projects or when I'm prototyping things? Huge benefit.",Reddit,https://www.reddit.com/r/programming/comments/1gijbgw/is_copilot_a_huge_security_vulnerability/,0
"You said ""not train GPT"". That isn't exactly clear when we're talking about Copilot.The issue here more seems to me that you are concerned about the difference between training and creating a vector embed and I (perhaps not usefully) am not. I am not because this doesn't matter at all when it comes to this threat model.Even if creating a vector embed is not training it's still MS ingesting all your code so that it can give reasonable responses based upon it. So much like the faulty library example you gave the results that you show us when queried what you have ingested.It's not like MS is creating a generic model and that produces a result and then it is translated to your symbols in what we hope to be a safe fashion. Instead your data is ingested, it's turned into a database that characterizes your data and MS holds onto that and that's used with the other model to form (even if only for a moment) a model which produces the responses.So even though I'm wrong to use the term training it doesn't really change the concern. MS has everything about your code, including a ""gist"". And you have to both hope they don't use it for anything but that also they don't lose your data. I admit MS is hacked a whole lot less than other companies, but it's still a risk that must be considered.I also don't see how this is simply file search. Maybe I'm wrong on that, but to me it's closer to autocorrect than just search. It's using context for the search. Maybe I'm dicing that too finely. the intentional choice to remain ignorant come back and let me know. No need to be assy. It's not like you have an actual explanation for how your faulty library scenario is actually parallel. You have to ""read"" (ingest) that entire library before your contextual suggestions will be useful enough to try to sell as a product. You made a misstatement. I did also. Is it all that helpful to try to characterize the other for these things?",Reddit,https://www.reddit.com/r/programming/comments/1gijbgw/is_copilot_a_huge_security_vulnerability/,9
"What part of capturing 20 TB per second of global signal communications as of last century do you not understand?What part of the Government planting evidence obtained unlawfully to obfuscate that the Govenment already has your signal communications do you not understand?GitHub Copilot is ""AI"". ""AI"" is just a racket. It's just fuzzylogic with marketing and branding (""AI"") with human biases baked in.If you want ""security"" keep your mouth shut and a big stick on your side at all times.",Reddit,https://www.reddit.com/r/programming/comments/185ie7r/github_copilot_security_scanning_does_not_work/,1
"Copilot was trained on code without any quality metric. So it will happily reproduce all bugs, security issues and deprecated API usage found in any dead github project. Now here is the problem to reach ""tucker jerbs"" quality the code copilot is based on would need to be accurately labeled as good/bad/buggy/outdated etc. . They would have to hire an army of programmers in order to do this, which probably defeats the purpose of basing it on all that freely available (licenses are for suckers) code on github.",Reddit,https://www.reddit.com/r/programming/comments/qdtr05/github_copilot_quietly_releases_support_for/,3
"GitHub Copilot's new Codex model, context comprehension, and lightweight clientside model improve code suggestions and latency. The new AIbased vulnerability checking solution blocks dangerous code patterns in real time to secure proposals.",Reddit,https://www.reddit.com/r/programming/comments/113qu5a/github_copilot_update_new_ai_model_that_also/,0
"Also: A good chunk of developers are likely using freely available tools without mentioning anything to upper management. Which is becoming a bit of a problem for companies that are expecting some sort of data protection on their proprietary software's internal code, because free ChatGPT is not silo'd and the data may be used for retraining.Microsoft is betting on the dam breaking and these companies eventually giving in and paying for enterprise Copilot/ChatGPT to prevent developers from accidentally using personaluse LLM products. But that concern might not be registering on anyone's radar because developers have been posting StackOverflow questions that reveal internal product information for a while now &mdash; and StackOverflow usually only gets banned at companies that are handling particularly sensitive info, because it would kneecap developers to take it away.",Reddit,https://www.reddit.com/r/programming/comments/1hh8c4x/github_copilot_is_free_in_vs_code/,1
"or move towards offering a system that strictly stays within the boundaries of their network I imagine they're going to continue to get push backThis is essentially what they're offering with the enterprise version of Copilot/ChatGPT. They're selling the ability to run AIs on separated Azure datacenters that adhere to stricter data handling policies, so that you can treat OpenAI as just another vendor and not an information leak risk. The argument I'm making is that not buying in is more of a risk than buying in, because if you haven't adopted the mindset that it's another vendor, individual employees will use whatever public tools are not blocked on the company's internet filters. But that despite this, interest has still been low.",Reddit,https://www.reddit.com/r/programming/comments/1hh8c4x/github_copilot_is_free_in_vs_code/,1
"Apparently nobody read the article. The court’s dismissal primarily focused on the accusation that GitHub Copilot violates the Digital Millennium Copyright Act (DMCA) by suggesting code without proper attribution. An amended version of the complaint had taken issue with GitHub’s duplication detection filter, which allows users to “detect and suppress” Copilot suggestions matching public code on GitHub. The developers argued that turning off this filter would “receive identical code” and cited a study showing how AI models can “memorise” and reproduce parts of their training data, potentially including copyrighted code. However, Judge Tigar found these arguments unconvincing. He determined that the code allegedly copied by GitHub was not sufficiently similar to the developers’ original work. The judge also noted that the cited study itself mentions that GitHub Copilot “rarely emits memorised code in benign situations.”They claimed that Microsoft was violating DMCA. They didn't provide evidence of this claim, and in fact their evidence showed the opposite of what they were trying to prove. Therefore those claims were dismissed. The copyright infringement claims were not dismissed.",Reddit,https://www.reddit.com/r/programming/comments/1f360xd/judge_dismisses_majority_of_github_copilot/,-1
"open source doesn't mean free. they're being trained on code which is licensed under the GNU GPL and other strong copyleft licenses, and the result ends up in closed source software which violates the terms of these licenses copilot doesn't discriminate! everyone's work gets stolen",Reddit,https://www.reddit.com/r/programming/comments/1f360xd/judge_dismisses_majority_of_github_copilot/,7
"Irrelevant, this doesn't excuse the fact that the tooling can't exist in its current form without stealing from people. Using GitHub copilot isn't a skill.",Reddit,https://www.reddit.com/r/programming/comments/1f360xd/judge_dismisses_majority_of_github_copilot/,3
"Technically speaking, all code snippets on StackOverflow fall under CC by SA, meaning everyone here who has ever cut and pasted from StackOverflow should be publishing an attribution for each code snippet. No one does this. It's impractical.I've been at companies that very much track this, and add it to their open source disclosures. There's a whole industry built around tracking usage of open source code, making sure that license attribution is done correct, and making sure your products aren't in violation.In this case, the burden to detect and complain about this copyright infringement lies on StackOverflow, as ultimately it's their copyright being violated by these offenders. StackOverflow apparently doesn't care, as it hasn't sued anyone over it yet. Therefore, this entirely fraction of the argument is moot. (It's the same thing about pirating content online just because the companies don't care enough to track you down and make you pay, doesn't mean it's not violating their copyright. The big content creators mostly care about the big vendors of this copyrighted data in this analogy, Microsoft than the little guys. Just because you didn't get caught doesn't give you the right.) In fact, from a commercial sense, you may be better off telling your developers to use github copilot over using StackOverflow to get snippet type code generated because there are (currently) no legal issues with the generated code.Except, you know, the very likely possibility that you get sued for copyright infringement right after Microsoft loses this case. The amount of emergency lawyering that would have to be done, the ridiculous degree of code auditing, the fucking five alarm panic this would cause at most companies means that this advice is the worst fucking advice I've literally ever heard. ""Might as well commit copyright infringement, it's not yet been settled it's copyright infringement."" No fucking thanks.",Reddit,https://www.reddit.com/r/programming/comments/1f360xd/judge_dismisses_majority_of_github_copilot/,2
"I agree with you, I am supporting copy left and GPL and against closed source companies. But this is like I am forbidden to read GPL code and then write MIT license code based on that knowledge. Maybe our licenses need to catch up with time and add clause about AI.And about copilot, what did people think when M$ bought github, that it was for altruistic reasons? We all knew they were training models and that's why some projects moved away. They must have consulted dozens of lawyers and judges before doing it, my opinion is not important.Similar with art and AI, do I need to pay just to see some image? If it appears in my browser and in my disk cache and backup and I didn't forget about it and create art based on that image. Is that illegal? Yeah right.",Reddit,https://www.reddit.com/r/programming/comments/1f360xd/judge_dismisses_majority_of_github_copilot/,2
"Yes, when I was using GitHub Copilot it wrote code that is insecure and deprecated.",Reddit,https://www.reddit.com/r/programming/comments/zwpqiv/do_users_write_more_insecure_code_with_ai/,3
"The results of [Authors Guild v. Google]( seem relevant. In that case, the Authors Guild argued that Google's unauthorized training of ~~an AI~~ a machine learning model on their (the Guild's) authors' copyrighted works was a copyright violation. The US District Court and Second Circuit Court both ruled in Google's favor. Here's a specifically relevant section of the decision: Google’s unauthorized digitizing of copyrightprotected works, creation of a search functionality, and display of snippets from those works are noninfringing fair uses. The purpose of the copying is highly transformative, the public display of text is limited, and the revelations do not provide a significant market substitute for the protected aspects of the originals. Google’s commercial nature and profit motivation do not justify denial of fair use.(Emphasis mine.) It's not exactly the same as Copilot, of course, but the question of whether training an AI on copyrighted works violates copyright has been addressed before.In particular, I feel like the bit I bolded might still be relevant. One could argue that Copilot is not a substitute for the code it was trained on. That code was all written to solve problems and do work, and you can presumably only solve those problems and do that work with the code in its entirety, not whatever snippets Copilot happens to generate. Copilot solves a different problem: writing new code.That said, there is at least one gray area in that argument I can see: some of the code Copilot was trained on was intended to solve the problem of writing new code— e.g., utility libraries and code generation libraries. But a snippet still isn't a replacement for an entire library, so who knows.Edit: Replaced AI with machine learning model based on feedback in replies.",Reddit,https://www.reddit.com/r/programming/comments/og8gxv/github_support_just_straight_up_confirmed_in_an/,4
"as necessary to provide the serviceIt also does not grant GitHub the right to otherwise distribute or use Your Content outside of our provision of the ServiceThis explicitly does not grant them permission to violate the GPL to reproduce your code in a way that is not necessary for hosting your repositories, which is exactly what Copilot does. This is a clear violation.",Reddit,https://www.reddit.com/r/programming/comments/og8gxv/github_support_just_straight_up_confirmed_in_an/,2
"This protects GitHub from infringing copyright by distributing code with whatever license, not consumers from infringing copyright if they as a result of Copilot get nonpermissivelylicensed code in their codebase.",Reddit,https://www.reddit.com/r/programming/comments/og8gxv/github_support_just_straight_up_confirmed_in_an/,2
"If the copilot isn't deemed a violation of copyright by law / courts, then the law / courts are morally wrong and have to change. The trust in judicial and political system is already at all time low, and this is just plain unacceptable and outrageous. If the courts don't get their shit together and try to restore trust in the system, it's not going to turn out good for anyone in the long term.",Reddit,https://www.reddit.com/r/programming/comments/og8gxv/github_support_just_straight_up_confirmed_in_an/,-1
"Copyright doesn't apply hereI think that's a bit presumptuous. There's a handful of questions here: Is the model a derivative work? I don't think there's a solid legal answer for this right now but personally I think things lean in favor of yes.A “derivative work” is a work based upon one or more preexisting works, such as a translation, musical arrangement, dramatization, fictionalization, motion picture version, sound recording, art reproduction, abridgment, condensation, or any other form in which a work may be recast, transformed, or adapted. A work consisting of editorial revisions, annotations, elaborations, or other modifications, which, as a whole, represent an original work of authorship, is a “derivative work ”.United States Copyright Act of 1976, 17 U.S.C. Section 101The model, in a sense, is the translation of source code from many sources into a series of weights and biases. By the end of training how much of the original works are still present is largely inscrutable with current analysis techniques but demonstrations such as the reproduction of the Quake III inverse square root algorithm indicate that some training code exists in retrievable form from within the model.The second question: Is the model sufficiently transformative to be protected under fair use doctrine (at least in the United States where that matters?) I think most people would look at this and say probably, I'm going to be bold and present and argument for no.Fair Use doctrine looks at 4 factors pulled here from [copyright.gov]( Purpose and character of the use, including whether the use is of a commercial nature or is for nonprofit educational purposes: Courts look at how the party claiming fair use is using the copyrighted work, and are more likely to find that nonprofit educational and noncommercial uses are fair. This does not mean, however, that all nonprofit education and noncommercial uses are fair and all commercial uses are not fair; instead, courts will balance the purpose and character of the use against the other factors below. Additionally, “transformative” uses are more likely to be considered fair. Transformative uses are those that add something new, with a further purpose or different character, and do not substitute for the original use of the work. Nature of the copyrighted work: This factor analyzes the degree to which the work that was used relates to copyright’s purpose of encouraging creative expression. Thus, using a more creative or imaginative work (such as a novel, movie, or song) is less likely to support a claim of a fair use than using a factual work (such as a technical article or news item). In addition, use of an unpublished work is less likely to be considered fair. Amount and substantiality of the portion used in relation to the copyrighted work as a whole: Under this factor, courts look at both the quantity and quality of the copyrighted material that was used. If the use includes a large portion of the copyrighted work, fair use is less likely to be found; if the use employs only a small amount of copyrighted material, fair use is more likely. That said, some courts have found use of an entire work to be fair under certain circumstances. And in other contexts, using even a small amount of a copyrighted work was determined not to be fair because the selection was an important part—or the “heart”—of the work. Effect of the use upon the potential market for or value of the copyrighted work: Here, courts review whether, and to what extent, the unlicensed use harms the existing or future market for the copyright owner’s original work. In assessing this factor, courts consider whether the use is hurting the current market for the original work (for example, by displacing sales of the original) and/or whether the use could cause substantial harm if it were to become widespread.On purpose and character: Copilot is currently noncommercial, but my understanding is that Microsoft intends to make it into a commercial product. As far as transformative as defined here, what copilot adds is a novel interface for retrieving the source code as well as the ability to remix the sources into new arraignments not found in the original works.So I would say that it is a commercial use and lightly transformative (bear in mind we're talking about the model itself and not its outputs necessarily) I think this leans neutral to gently against fair use (all leanings are of course just my opinion)On Nature of the Copyrighted Work: I think a court would likely find source code to be factual rather than creative in nature. This would lean slightly against based on the Copyright.org text.On Amount and Substantiallity: The entirety of many many works were used in the construction of the model. This factor leans heavily against a fair use claim.On Effect of the Work: This is what I think most people are referring to when they talk about ""transformation"" colloquially in regards to fair use rather than the jargon transformation of the first point. The end goal of both the original works (as licensed source code) and the copilot model aim to make available source code for future works. Copilot harms the original works by allowing authors to sidestep the copyright licensing like such as GPL. This leans against fair use.My own personal feelings: I'm generally excited for AI tools like copilot. But they have to be built with respect towards open source software developers. Rule of Cool doesn't make it right to straight up ignore the wishes of devs enshrined in licensing agreements.",Reddit,https://www.reddit.com/r/programming/comments/og8gxv/github_support_just_straight_up_confirmed_in_an/,4
"If you somehow manage to copy it across such a significant number of repositories that it completely dominates the training data for fairly common input by an inexperienced developer, and do this without Github noticing early on and nuking your account(s), then possibly. You'd probably need to replicate this more than [the most famous piece of code ever written]( as that appears to be what it takes to get Copilot to output code verbatim, and you'd have the disadvantage of needing to ""outcompete"" the legitimate code that would certainly exist for things that beginners will be trying to do (whereas the fast inverse square root is going to be exactly the same in every repository that contains the input provided in [this demo]( a lot easier to just post your malicious code on StackOverflow.",Reddit,https://www.reddit.com/r/programming/comments/og8gxv/github_support_just_straight_up_confirmed_in_an/,3
"Of course not. Copilot spits out private keys if they happen to be uploaded to the repo (yeah, I know, bad practice). They can't risk that.",Reddit,https://www.reddit.com/r/programming/comments/og8gxv/github_support_just_straight_up_confirmed_in_an/,6
"All great questions, I think one could argue that Copilot produces it's own works even if it's been trained on some GPL licensed code. It would be no different than trusting a peer to not copy some snippet from a GPL project.",Reddit,https://www.reddit.com/r/programming/comments/og8gxv/github_support_just_straight_up_confirmed_in_an/,5
"Even without directly monetizing Copilot, it seems to be a new ""service."" And all of the training done for the machine learning wasn't for operating the existing service. So even if Copilot doesn't regurgitate my code for other users, IMO the training process violated my copyright on any code that was put on GitHub without a license.All it takes for this to be an absolute shitshow is one dev with deep pockets to hire a lawyer and find out of a court will agree with me. (And how sympathetic do you think a jury would be toward a megacorporation when interpreting TOS terms if they think that an independent developer has been wronged?)",Reddit,https://www.reddit.com/r/programming/comments/og8gxv/github_support_just_straight_up_confirmed_in_an/,2
1. I don't think copilot itself (meaning the trained ML model) is a derivative work of the data in the training set. So I wouldn't worry about the direct violation of the license of the code you uploaded to GitHub.It contains copies of original GPL source code encoded inside it's model. That's proven by the fact that it can produce these copies again. The ML model is a derivative work.,Reddit,https://www.reddit.com/r/programming/comments/og8gxv/github_support_just_straight_up_confirmed_in_an/,9
"I don't think copilot itself (meaning the trained ML model) is a derivative work of the data in the training set. So I wouldn't worry about the direct violation of the license of the code you uploaded to GitHub.The thing is, it doesn't really have an answer. People can make it be a copyright violation. Some people seem pretty intent on doing so!Which pointlessly cripples ML, wiping value from the world. We have seen people using the model to regurgitate entire functions from other works, which is a potential problem if that work could be considered a derivative work.Humans can also, without knowing it, ""regurgitate entire functions"" from memory.",Reddit,https://www.reddit.com/r/programming/comments/og8gxv/github_support_just_straight_up_confirmed_in_an/,9
"I don't think copilot itself (meaning the trained ML model) is a derivative work of the data in the training set. So I wouldn't worry about the direct violation of the license of the code you uploaded to GitHub.Nah. It's entirely derivative. The NN wouldn't work without the training data. It would be an empty net. If you fed it Shakespeare it'd write Shakespeare, not Byron. It will be returning answers based on copyrighted code/concepts and claiming it's theirs.",Reddit,https://www.reddit.com/r/programming/comments/og8gxv/github_support_just_straight_up_confirmed_in_an/,9
"It's my understanding that you implicitly grant them a license (ie. permission) to use your works. Any copyright therein is null and void for that particular entity.We can make guesses all we want but until an actual lawyer comes in and weighs in on the matter I find the whole thing to be in a gray area and personally would not use Copilot on any projects I was worried about getting caught up in a legal mess.What we know is that GitHub is making a claim that they can utilize all public projects to improve and provide services to the masses; whether that claim has grounds is up to the legal framework around Software asset control.What the individual did in the Tweet IMHO was a sound thing, it does mean they could potentially be restricted access to GitHub though but judging from their profile stating they hate GitHub etc. I doubt that's much of a concern. If the EFF actually gets involved they'll likely release something and they have the legal support to make provide a bit more of a valid take on the issue.A little bit of me wants to say this is why it's in Alpha, I don't know Microsoft / GitHub truly know what'll happen but their lawyers are okay with this project going into Alpha to further see where this will go.",Reddit,https://www.reddit.com/r/programming/comments/og8gxv/github_support_just_straight_up_confirmed_in_an/,2
"Copilot is a machine learning engine and not the data it is operating on. Saying Copilot would need to be GPLlicensed because it is processing GPLlicensed data is ridiculous. That's like saying I have to GPL license my tax returns because I printed them with CUPS.The real question here is who is violating the GPL, and it's a tricky one. Generally, the GPL only gets violated at the point where someone who has distributed programs that contain GPL code in binary form refuses to provide the source code upon request. Clearly GitHub isn't doing that here. What they are doing is offering GPLlicensed code snippets to other people who then may include those snippets into their code without realizing the licensing implications. If those other people then distribute their programs in binary form and refuse to release their source code, then they are the ones violating the GPL.So I think it's pretty clear that GitHub themselves aren't violating the letter of the GPL here... what they're doing with the GPLlicensed sources is fundamentally not any different than the normal hosting and code search stuff they always offered. The interesting question is whether they're committing some other crime by offering these code snippets to users without making sufficiently clear where they came from, and possibly tricking them into license violations. That's going to be one for the lawyers to figure out.",Reddit,https://www.reddit.com/r/programming/comments/og8gxv/github_support_just_straight_up_confirmed_in_an/,2
"This is my main concern with Copilot tbh, I've seen enough code on Github with obscure security flaws to be wary of any code it generates. Not sure how it would determine code quality, as [popularity]( is not a great indication of good code. As the model doesn't have any comprehension of the code itself, it's likely to suggest code because it's common rather than good.",Reddit,https://www.reddit.com/r/programming/comments/og8gxv/github_support_just_straight_up_confirmed_in_an/,0
"It is not these developers right to make a claim. Copyleft licenses extend rights. Sometimes these rights are defined in an open way, which means that albeit the fact that it opens an unknown, future right. If they didn't want this, they should have had custom licenses that do not open a potentially infinite subset of rights.These authors claim that they are in the right because it's a derivative work. There's four problems here: the size and surrounding context of use of copilot brings fair use into question. Derivative or not, chances are there are small, 10 line snippets of code that are structurally equivalent (not considering variable names, but even then) all across Github. One of those projects being GPL does not poison that same code. Hell plenty of times everyone starts off basic projects the same way. it's not necessarily a derivative work. This is a complex statistical model that has the ability to generate similar output. Chances are the model is not a derivative work simply because of scope. Scope not withstanding it will be very difficult to convince someone that a bunch of linear algebra is a copyright violation. The authors of these licenses have not made a negative comment yet. I'm not expecting a lawsuit immediately, but at least a comment about investigation. They probably agree that it's not a derivative work, that it doesn't produce derivative works / works of scale that the license is relevant. If the authors (probably) disagree, a person's legal armchairing is completely irrelevant. Github has lines in their TOS that allow them to do more or less whatever. So does every host that isn't personal. It even applies to source mirrors that other people create, in such a way that at best you don't get to sue the host, but rather DMCA. And unfortunately, I don't think a copyleft license exists that can stop someone from reuplpading source code, it would have to be copyright only (and even then, you have to protect it by not leaving it out).",Reddit,https://www.reddit.com/r/programming/comments/og8gxv/github_support_just_straight_up_confirmed_in_an/,-1
"Maybe with a rebranding, but a bad rollout could be fatal to this.I'm also skeptical that an organization would want to do this. MSFT will have just gotten sued by various parties for aggressively repurposing code given to them, and now they want these fortune 500 companies to give them all their code... What's the message there ""trust us because...""Additionally the resulting AI will only be as good as the training set. If it's garbage In (as most corporate codebases are) then the AI will spit back garbage out:If you have use after free bugs in your code copilot will helpfully suggest them to junior devs. If you have inconsistent styles copilot will suggest inconsistent styles. If you have blindspots about library APIs, copilot will be blind too.Organizations that are good enough to have good datasets to train the AI, must have controls and processes to create that good code. Why not just use those existing controls since they clearly work?",Reddit,https://www.reddit.com/r/programming/comments/og8gxv/github_support_just_straight_up_confirmed_in_an/,-1
"But you don't know if its learning or just copying. It uses openAI, sure, but its still a black box. You would be a fool to trust it to write your code. Also, if you try to copy a licenced piece of code, you would (hopefully) think before doing it. Maybe you would even consider giving credit. But you are liable.If the copilot presents you code from a licensed project, you wouldn't even try to verify if it is protected. Furthermore, if the case is made in court that copilot 'learned' snippets that are 99% similar are considered fair game and you can't be sued for using them, then what stops you from plainly stealing GPL code and saying that copilot gave it to you.",Reddit,https://www.reddit.com/r/programming/comments/og8gxv/github_support_just_straight_up_confirmed_in_an/,5
"This is quite nitpicky but where is the limit? What if for some reason I have the exact same function than a GPL'd project to instantiate a 3rd party library or service, am i violating it? If yes, how do avoid something like this that is just intuitive/documented that way? Do i add comments or extra lines just for the sake?I mean copyrighting code in general seems a pretty bad idea. Copyright ideas and abstract terms if you want but there are times when multiple people is going to get the exact same or 99% similar block of code because configuration is configuration. If i told Copilot to configure que DB driver in a Java project and it got me code copied, even if verbatim, that shouldn be a violation really. Maybe something trully unique, but not ALL code under the project. That's unrealistic.Do we copyright a div with two inputs, username and password? Do we copyright a middleware console logger? Where is the limit that separates dummy boilerplate from intelectual work??Even a CSS reset! How many projects there are in the world with a:html { margin: 0; padding: 0; width: 100%; height: 100%;}What I mean is: if I ask Copilot to instatiate Postgress connection for me, and gets some literal instatiation from some project, that shouldn't be a copyright violation. I doubt even the whole CRUD should be copyright violation.",Reddit,https://www.reddit.com/r/programming/comments/og8gxv/github_support_just_straight_up_confirmed_in_an/,-1
"I know, there’s the ethical and legal position which I don’t disagree with necessarily and then there’s the “Prove it, copper” response. Don’t forget the possible application of fair use doctrine as well, that’s proven to be pretty flexible in a lot of (court) cases.Copilot introduces a new “peril” if you will, in that it’s possible you might be put in legal jeopardy if Copilot generates code which is identifiably from a licensed product without you knowing it. I think if I were to use Copilot I’d be looking for a license from GitHub that includes indemnification against any legal issues arising from generated code. That’s likely to be a really expensive clause to have in a contract, so it would probably put the cost of Copilot beyond usable.The only way I would consider Copilot usable is if it were trained on a code base where I own the copyright, but that probably significantly decreases its usefulness.",Reddit,https://www.reddit.com/r/programming/comments/og8gxv/github_support_just_straight_up_confirmed_in_an/,7
"Copilot is not learning, it is memorizing patterns with zero understanding of why those patterns exist and attempting to figure out where it can copy/paste things it has seen before.This is not learning from someone else's code by reading it, understanding it, and applying the concepts to your own distinct code.It is a fancy copy/paste with a shiny coat of paint.This means it is the same as you stealing some code that looks like it fits via copy/paste without learning anything and hoping nobody notices.That would be the same as violating the license.",Reddit,https://www.reddit.com/r/programming/comments/og8gxv/github_support_just_straight_up_confirmed_in_an/,-1
"People have managed to get: private keys from public GitHub repos copyright/license/contact info comments from public GitHub reposIf copilot is doing transformations, it's clearly not doing enough to pull out underlying code patterns or structures, and is much closer to googling the hint comment and copy/pasting whatever looks good from the stack overflow results.",Reddit,https://www.reddit.com/r/programming/comments/og8gxv/github_support_just_straight_up_confirmed_in_an/,3
"Well you need a copilot license to sign up to the waiting list, that's not proof of anything but it implies to me the two will share the same license. But who knows",Reddit,https://www.reddit.com/r/programming/comments/11ylui4/github_copilot_x_the_aipowered_developer/,1
"I honestly don't see this at all. I mean, I get what you're saying: programming isn't just a job for me; for over 30 years now, I've programmed for fun, dabbled in competitive coding, spent my weekends playing with Project Euler or implementing some cool idea from an academic paper or from mathematics, built games and ray tracers and astrolabe simulations that talk over RS232 to synchronize with a real telescope, and a zillion other things, run open source user groups, attended and even organized weekend hacking sessions so I can solve cool problems with other people. Yes, I'm obsessed.But Copilot doesn't do any of that interesting stuff that I find attracts me to programming. It does the boring stuff that's one or more layers of abstraction below where anything gets interesting. It writes the line of code that you were definitely going to write anyway, but you didn't want to go look up the type signature for foldl' for the 200th time because seriously, who actually remembers the order of parameters to the higher order function in the first argument of some random combinator? It writes the ten unit tests that you knew you should write, but you're doing this to have fun, and why the hell should you spend your Saturday afternoon writing tests to make sure something does nothing when passed an empty list, instead of working out the interesting behaviors?When Copilot tries to solve interesting problems, it fails rather spectacularly, so you don't want to let it do those things anyway. Even if it didn't fail, you wouldn't want to let it do those things, because that's the point. You're doing this so that you can do this, not let some AI model do it for you. So just don't accept the suggestion! But especially if you establish the habit of naturally working by writing short selfcontained definitions that are defined in terms of interesting lowerlevel definitions, you will eventually reach the point where you aren't doing the interesting part any more, and the suggested completion saves you the couple minutes you would have spent writing that obvious code on your own (including looking up function names and names/orders of arguments and junk like that).For that reason, though, I don't find a lot of this Copilot X stuff very exciting at all. I have [tried working conversationally with large language models]( to solve programming problems, and honestly it's more tedious than it's worth. Copilot fits what I need pretty well: when it's already clear what I'm going to write, it lets me just fastforward past the part where I'm typing and doing tedious stuff, and get to the part where I'm making meaningful decisions.",Reddit,https://www.reddit.com/r/programming/comments/11ylui4/github_copilot_x_the_aipowered_developer/,0
"Copilot actually does understand codeCopilot doesn't understand code a tiny bit. your editor take data in adjacent files open in the editor and sends that data as context for Copilot.it is extremely dangerous to insinuate that Copilot knows what it is doing it does not. all it does is produce output that is statistically likely to be what you're looking for and while that is extremely impressive in and of itself there is no reasoning, there is no intelligence, there is no verification.meanwhile over on the stackoverflow side of things there's a human out there that does have intelligence, reasoning and verification about the things they talk about. perhaps they're wrong, that happens, but Copilot will be wrong and lie to your face about it.I like Copilot as a product, it oftentimes helps me find solutions in old frameworks that have dead forum links, but talk about it and treat it for what it is.",Reddit,https://www.reddit.com/r/programming/comments/1ac7cb2/new_github_copilot_research_finds_downward/,0
"Yeah I found this too. I had copilot save me 45 minutes the other day by it instantly creating a 95% correct unit test based off of a comment. I also had a bunch of reddit commenters choose that hill to die on by indicating it's absolutely impossible that I could be a dev that knows what he's doing, making a unit test w/ an LLM, reviewing it, submitting it to PR review by the rest of my human team etc etc. According to them if you use an LLM as a tool you're a hack, and nothing you create can possibly be robust or part of a quality system.",Reddit,https://www.reddit.com/r/programming/comments/1ac7cb2/new_github_copilot_research_finds_downward/,0
"To me its surprising this sub don’t get it (or don’t want to). Other places like hacker news are all excited about the progress. Copilot autocomplete is not great, true, but Copilot is due for an upgrade too. We will look at those tools in a couple a years laughing at how basic they were. If you believe people here the peak have been achieved and it could only go backwards from here.",Reddit,https://www.reddit.com/r/programming/comments/1ac7cb2/new_github_copilot_research_finds_downward/,0
"How else it is supposed to work? But the TOS are a legal protection so that MS can't go and do nefarious things with my code.If you are using Azure that also ""exposes"" your code to MS. And MS has root on your PC if you run Windows. There is always the possibility that a rouge employee snoopes on you. Or maybe the NSA or a secret court can force MS to hack your computer. But there are always checks and balances to mitigate that risk (both legal, and technical).My company decided their trade secrets are safe with microsoft products, but if you are a MS competitor, or work for a nation state, or are a cyber criminal ;) then I wouldn't trust them. With neither Windows nor VS code or copilot.",Reddit,https://www.reddit.com/r/programming/comments/qoz89v/everyone_is_super_exited_to_be_invited_to_use_the/,1
"I've been curious, if I have a private repo containing my personal Obsidian vault (a directory of md files), can copilot leak my personal info?Similarly, can past homework assignments/projects in private repos be exposed via common starter code?",Reddit,https://www.reddit.com/r/programming/comments/qoz89v/everyone_is_super_exited_to_be_invited_to_use_the/,6
"40% of code generated by copilot contains security issues"" injections, XSS, etc.Remember it was trained on code bases of varying quality.",Reddit,https://www.reddit.com/r/programming/comments/qoz89v/everyone_is_super_exited_to_be_invited_to_use_the/,0
"Did you forget the yuuuuge controversy when people found out it violated GPL? I don't intend to pile on you, but I think people (especially this sub) are well aware of the ethical questions regarding Copilot.",Reddit,https://www.reddit.com/r/programming/comments/qoz89v/everyone_is_super_exited_to_be_invited_to_use_the/,5
"I feel like I'm playing whackamole. You've been arguing why Copilot must train on users' proprietary code, and then it switched to why Copilot could never run clientside, and now you're arguing that Microsoft wants to protect their code. You need to think like a big corp rather than someone who likes OSSYour tone is mildly insulting... Do you work at a big corp? I do. I know for a fact that we ship binaries compiled from proprietary code to user devices. Lots of other companies' proprietary code runs on client devices, too: video games, Turbotax, MS Word, etc.Anyway, my point is that Copilot (a) has no need to train on your proprietary code because there's plenty of properlylicensed code available on the internet (on GitHub, no less) and (b) serverside processing is not the only way that Copilot can be implemented. Perhaps it requires a farm of megaGPUs for now, but I predict they'll be working to shrink the model until it fits on client devices.",Reddit,https://www.reddit.com/r/programming/comments/qoz89v/everyone_is_super_exited_to_be_invited_to_use_the/,1
"You've been arguing why Copilot must train on users' proprietary code, and then it switched to why Copilot could never run clientside, and now you're arguing that Microsoft wants to protect their code.Because it's all the same issue, my argument hasn't change at all""It doesn't need to do X"" yeah, if it's not a commercial productYou might feel like you're playing whackamole but you're just pointing out how Microsoft can release their product without making money off of it, which is unrealistic but I predict they'll be working to shrink the model until it fits on client devices.Yeah potentially but I doubt it if they want us to have uptodate suggestions and fully utilize what this could beMicrosoft are pitching this as a learning tool that keeps up to date with changes, I don't see how that's possible to do without some external back and forth going onIf you use it for free, they are going to use your code because that's how this stuff always works, you either provide data or you pay to use it (mostly both) Your tone is mildly insulting.So is yours, it's a problem all engineers have online and you're not immune to it Do you work at a big corp?I have as a freelance engineer but not right nowEDIT: I should also add, they want this tool to be editor agnostic, which gives more of an argument to having a thing client",Reddit,https://www.reddit.com/r/programming/comments/qoz89v/everyone_is_super_exited_to_be_invited_to_use_the/,1
"Personally I don't trust the data privacy policies of tools like this. It reads my code, it communicates with servers of another company. User engagement data, including pseudonymous identifiers and general usage data, is required for the use of GitHub Copilot and will continue to be collected, processed, and shared with Microsoft and OpenAI as you use GitHub Copilot. You can choose whether your code snippets are collected and retained by GitHub and further processed and shared with Microsoft and OpenAI by adjusting your user settings.It has the capability, it's just one ""oops"" away from processing my proprietary data into their models. I'm not sure I am OK with that, considering how lax the data protection laws are around the globe.",Reddit,https://www.reddit.com/r/programming/comments/vk4c2l/amazon_launches_codewhisperer_a_github/,8
"Want to answer to that side instead of entirely acting in bad faith?[Here]( an article about the topic. Admittedly, it was written by Felix Reda who was a member of the European Parliament for the Pirate Party so there's a conflict of interest.tl;dr: Scraping and processing public information from websites is legal, regardless of the code's license. Snippets generated by Copilot are not derivative works. Human input is needed to create a derivative work. Snippets that Copilot copied 1:1 from other sources don't reach the threshold for originality, which is needed for copyright protection; for it to infringe GPL license, it would need to recite an entire program. (Same reason that you can freely copy StackOverflow snippets, despite its license.)&x200B;Edit: Before you reply, please read the article above. Thanks",Reddit,https://www.reddit.com/r/programming/comments/vk4c2l/amazon_launches_codewhisperer_a_github/,5
"I mean, aside from the fact they have a privacy policy they have to adhere to, what do you expect them to do with that data? There's no one at Microsoft pouring over copilot analytics trying to figure out your name, occupation, ability and what you're working on so they can steal it for themselves. They've got much more profitable things to be doing.",Reddit,https://www.reddit.com/r/programming/comments/vk4c2l/amazon_launches_codewhisperer_a_github/,1
"Scraping and processing public information from websites is legal, regardless of the code's license.So if I scrape a repo's codebase...I can completely ignore its license? That makes no sense. Snippets generated by Copilot are not derivative works. Human input is needed to create a derivative work.That sounds like a very flimsy way of washing one's hand of responsibility. Just write a program that ~~memorizes~~ learns input, let it loose somewhere and then use that program to ~~reproduce~~ generate code for you and you've subverted its license altogether. for it to infringe GPL license, it would need to recite an entire program.So I can ~~steal~~ fork a GPL licensed repository and remove a bit of code, and I would no longer need to follow the GPL since it's not ""the entire"" program?",Reddit,https://www.reddit.com/r/programming/comments/vk4c2l/amazon_launches_codewhisperer_a_github/,5
"They gather every keystroke of tens of thousands of developers. Of course they analyze that data in every possible way to gather more information about usage patterns, structure of source code, individual competence of developers, psychological effects of using AI tooling, and hundreds of other corporate interests. I'd think it would be rather naive to assume otherwise. There's no one at Microsoft pouring over copilot analytics trying to figure out your name, occupation, ability and what you're working on so they can steal it for themselves. They've got much more profitable things to be doing.I'd argue that they actually do steal your source code, your data. Of course they don't copy it one to one, but they analyze it and use it to feed their machine learning projects. That are the profitable things they're doing.And yeah, they're most probably not after single individuals (although it would be in their interest to identify hidden talent), but I really don't care about that at all. I just don't agree with sharing my usage patterns, my data, my source code with Microsoft, so that they can analyze and profit from it.",Reddit,https://www.reddit.com/r/programming/comments/vk4c2l/amazon_launches_codewhisperer_a_github/,1
"Yeah, I somewhat agree with you. I try to avoid unnecessary data collection where I can though. And it's doable without moving to the woods. Use Linux, use a linux phone or a dumb phone, avoid Facebook and other social networks, use Firefox and uBlock Origins, be a bit mindful where you share your data, and you're already avoiding a massive amount of tracking and data collection without too much hassle. One just has to care about it.It's this mindset that makes me avoid CoPilot, too. I'm all for better tooling, and I would use AI tooling, if it ran locally, or was made available by an organization that believably cares about its users privacy. But MS is not that. But, yeah, it's a very personal decision whether people care about all this or not.",Reddit,https://www.reddit.com/r/programming/comments/vk4c2l/amazon_launches_codewhisperer_a_github/,1
"This is not just an annoyance, when working in corporate environment you might need to disable Copilot for security reasons",Reddit,https://www.reddit.com/r/programming/comments/1d2xtpe/github_copilot_reenables_itself_after_being/,6
"\\\\\\\ It's very simple, Microsoft is simply adding it to updates. \\\\\\\\So for now disable it with registry hack, turn off automatic updates. Realize when you update your PC you must reinstall the registry hack below. Takes like 5 seconds.Turn off automatic updates, MS is scanning your system to see if Copilot is there, functional. If not you're going to get updated.Turn off automatic updates. You can open this reedit file below in notepad (open with once) so you can see that it's safe only targeting Copilot entries. Then click and install, easy as that. Enjoy, for now this will fit it![",Reddit,https://www.reddit.com/r/programming/comments/1d2xtpe/github_copilot_reenables_itself_after_being/,6
"Ok, again, I don't think this current topic is really about AGI. Copilot is not AGI, nor is it trying to be. The possibility of AGI, how far away it is, whether it is good or bad, is all up for debate. But I can say a few things to respond to your ideas about data sharing anyway.Many people are happy to post their images, words, or code to be read by any interested person but yet do not give permission for their data to be exploited for commercial gain by a company using it to train an AI. So 'public data' is a grey area. But Microsoft (and reddit and others) are in a position where they control the platforms that people are posting to, and so they are able to demand that the data is available for them to use while it remains 'grey' or perhaps even illegal for others to use.It is easy to dream up endstate utopias for where AI might take us, but only if we ignore the current obvious reality that AIfocused companies are aiming to increase their own wealth rather than improve the world. Their aim is to consolidate wealth and power, not spread or share it. We can wish for everyone to have access to the same data; but that is not the case. Nor does everyone have access to the expensive and significant resources to make use of that data. And those who do will clearly be trying to block & slow down others. Again, our current trend is towards evertighter wealth concentration. In our current world, money is all powerful. As an example, just consider that we've all known for decades that our current actions are putting all live on earth at great risk and yet we continue pretty much business as usual, because the the profits of corporations is nonsensically considered more import than the environment such is the power of money. So now, as we see corporations scrambling to grow their profits using AI, do you really think they are going to aim to make the world a better place? I certainly wouldn't expect so.",Reddit,https://www.reddit.com/r/programming/comments/1d2xtpe/github_copilot_reenables_itself_after_being/,1
"My experience is the same as the author's. Copilot saves typing obvious stuff, but that's about it. Otherwise it is unpredictable and untrustworthy. Which means it just becomes a distraction.In many situations where I really wanted it to type the code for me because I thought it was boring and obvious, it failed utterly and I had to write it myself anyway, probably taking longer.Even a 99.9% tool will always have the same issues. If you're responsible for the code, you have to review it and find the .01% of bugs. Fun! What a productivity boost!",Reddit,https://www.reddit.com/r/programming/comments/1cmy2x4/did_github_copilot_really_increase_my_productivity/,0
"Anything remotely complicated is useless, and frankly dangerous for anything that is actually critical. I've found I can do it just as quickly, if not more so, because it's largely done right the first time instead of having to unpick copilots gibberish. People who have used copilot are seemingly forever fixing bugs and having to deal with pissed off users, and so they banned it completely at work for all uses. The truth is that for serious work it is untrustworthy. I've found those who are using copilot successfully are not on serious or complicated projects.",Reddit,https://www.reddit.com/r/programming/comments/1cmy2x4/did_github_copilot_really_increase_my_productivity/,0
"One key lesson is not to trust copilot for large blocks of code, unless those blocks are dead simple.It's a lot better at doing a single line though. Start a new line, type in the first two or three letters, and a lot of the time it'll actually get you really close to what you want.If you want a large block of code, go have a conversation about it with a more powerful bot.",Reddit,https://www.reddit.com/r/programming/comments/1cmy2x4/did_github_copilot_really_increase_my_productivity/,0
"however, it's really difficult to predict what it will get right, and what it won't. That's on the author, and it blows my mind (not really, this is expected) that people who should know better still don't know better. The point isn't to predict success or to be wary of AI generated code. The point behind looking up solutions elsewhere, is that you adapt what is close enough to be the valid solution for you. Treating solutions from Copilot or from whatever differently then from stackoverflow is the problem, they are the same, they are both not to be trusted, and in both cases you need to adjust, fix, and improve it to work for you. It is just in one case, the untrustworthiness is implicit (stackoverflow), and in another is explicit (copilot or other tools).",Reddit,https://www.reddit.com/r/programming/comments/1cmy2x4/did_github_copilot_really_increase_my_productivity/,0
"One of the biggest takeaways I have about copilot usage is that a lot of people want to see it output the code that they would've written given the time. IMO that's treating Copilot like a ""do my job"" servant, not the tool that its useful as. I don't trust copilot to output exactly what I was thinking.But as a tool for getting hints and lookups quicker, and generally giving me a direction of where to go next it excels. That's the productivity gain IMO. And autocompleting the simple obvious stuff. I've also used chat to ask it stuff like ""how could I connect this to that"" or whatever, and it will spit out a bunch of ways to go about it. While it gives me sample code, I don't just copy/paste it since I know its either going to have some nonsense in there or might even be totally wrong about the approach. Maybe some people don't deal with mental blocks or haze as much as I do, so they have higher expectations. But its been a real big productivity gain for me.",Reddit,https://www.reddit.com/r/programming/comments/1cmy2x4/did_github_copilot_really_increase_my_productivity/,0
"I'm worried it can be used maliciously e.g., we already see many weird / random robot user accounts with really random looking content. I would imagine a malicious entity could make enough of these (with some pattern that introduces a security flaw) so they are used by copilot and then the unsuspecting Copilot user would just use the code verbatim. Hopefully GitHub is doing some kind of quality filter over the code being used.I haven't used it yet, and I don't plan to. I don't really have issue if snippets of my code are used for training, because (for now) it doesn't impact me at all.",Reddit,https://www.reddit.com/r/programming/comments/x61xo0/is_there_any_way_to_opt_out_of_githubs_copilot/,3
"Not a fan of copilot. I have several small open source projects that are unlikely to be copied directly, but I do not want my work to be used to train a machine like copilot. How do I opt out? Or is this going to be like when Google mined free public translations to improve their paid autotranslating service without compensating the people who did the work they exploited?",Reddit,https://www.reddit.com/r/programming/comments/x61xo0/is_there_any_way_to_opt_out_of_githubs_copilot/,7
"GitHub's ToS require their users to give them a free license to use your code for their products, which include Copilot. And if you did not have authority to give them that license, then the ToS blame you for it.If you published code online without a licence allowing that and someone else mirrored your code on GitHub, you probably want a lawyer because there isn't much legal precedent on AI usage.",Reddit,https://www.reddit.com/r/programming/comments/x61xo0/is_there_any_way_to_opt_out_of_githubs_copilot/,-1
"The latter. If it's public, they train on it, as they believe it is permitted under copyright law no matter the license.The best protection schemes you can likely adopt is: A: remove the code from the public internet. No one says an open source project's code much be online, it's just it must be available to others somehow.Or B: apply a license whose custodians are taking Copilot etc to court over their interpretation (the FSF are doing so, I think). If they win, you should then be protected.",Reddit,https://www.reddit.com/r/programming/comments/x61xo0/is_there_any_way_to_opt_out_of_githubs_copilot/,7
"TL;DR: Don't use Github and don't write open source code, so no one else can put your code there either.You can't really use a permissive license repository or free hosting service like Github and then start getting huffy about what people are doing with the code. Are you going to object if someone in the sex work industry uses your code? What about someone in the Taliban? Why is Copilot specifically the target of your ire?And why is it ""exploiting"" your work to train a machine to help others write code vs just someone using your code directly? Like, if I cloned your repository and used your code for part of my project, am I exploiting you then? I think the real issue here is that developers are stunned by how much of what they do that they feel is an extremely high level skill is so easily filled in by machine learning. But it's not like we weren't [warned](",Reddit,https://www.reddit.com/r/programming/comments/x61xo0/is_there_any_way_to_opt_out_of_githubs_copilot/,2
"Because it monetizes opensource code, without complying with opensource's clearly stated requirements. Not to put too fine a point on it, but Copilot constitutes systematic corporate theft.When you use GitHub, you enter an agreement with them to license your code for them to use. This is a different and parallel agreement to your license like GPL, MIT etc. It is no different than someone offering a GPL and commercial license, both are parallel licenses. In the agreement you comply with GitHub using your code for training Copilot without attribution, just as in a commercial license you can add that you don't require virality, because you are not making them use the GPL, you are making them use a separate license. If you don't want them to use your code, don't use GitHub.",Reddit,https://www.reddit.com/r/programming/comments/x61xo0/is_there_any_way_to_opt_out_of_githubs_copilot/,2
"TL;DR: Don't use Github and don't write open source code, so no one else can put your code there either. You can't really use a permissive license repository or free hosting service like Github and then start getting huffy about what people are doing with the code. Are you going to object if someone in the sex work industry uses your code? What about someone in the Taliban? Why is Copilot specifically the target of your ire? And why is it ""exploiting"" your work to train a machine to help others write code vs just someone using your code directly? Like, if I cloned your repository and used your code for part of my project, am I exploiting you then? I think the real issue here is that developers are stunned by how much of what they do that they feel is an extremely high level skill is so easily filled in by machine learning. But it's not like we weren't warned.Sounds like you are unaware of the ongoing issues between Elastic and Amazon. Just because code is open source doesn't mean that the authors lose ALL rights. If companies can require us to agree to terse legal documents to use their software to listen to music, I can enforce terms on my code. I am not a fan of GPL3, but it at least aims to give developers a choice to deny cloud providers from selling thousands of instances of the software in a server room.I have no issue with sex workers using my code, but a lot of people would not be happy if the Taliban used their code. Unfortunately, you can't really enforce software licenses against rogue militias.Part of why I asked this is to find out if there is any license I can use to disable CoPilot from hoovering up my information. If the MIT/etc license is too permissive, or if I have to leave github, I can change that. Not sure why you are being so aggressive about someone looking for a way to block feeding all their information into a giant AI.As for exploiting, sucking up everyone's work for a proprietary product that generates that work without any compensation to the people who actually did that work is... kind of obviously exploitation. If it isn't to you, I don't know what to say. People can look at work from others and take inspiration, but there is clearly a difference between someone borrowing my code from github/stackoverflow (still can't violate software licenses) with maybe an attribution, and feeding it into a replicator to accelerate copyandpaste coding.You may be right about some of the backlash to CoPilot. There are a lot of developer roles that only require blindly gluing code snippets together. This type of coding practice often leads to quick company growth for a few years until the code is so rickety, taped together, and full of security holes that it has to be rewritten (something I've done multiple times). We wouldn't want to build bridges this way, and we should not encourage software to be written like this. Use well supported libraries with well known interfaces over blindly copypasting tons of unverified code into your project.",Reddit,https://www.reddit.com/r/programming/comments/x61xo0/is_there_any_way_to_opt_out_of_githubs_copilot/,2
"If I modified the cash register software used by a chain of stores to round in my favour, costing each customer an extra cent that they'd hardly notice or care about, then half a billion transactions are processed by my code, is it fraud, and am I now 5 million dollars richer? Alright, now the AI takes an insignificant sliver of abstract creativity from each training source...My personal feelings are that they'd be fine if they trained a ""GPLcompatible"" AI, a ""MITlicensecompatible"" AI, an ""everythingcompatible"" one trained only on unlicense/CC0, etc. code, and so on.But by mixing in all sorts of proprietary and copyleft data into a single AI, they're taking a risk that no country will ever fill in the gap in their copyright laws. What if, 5 years down the line, it's ruled that as far as France is concerned, the output had been GPL all along, so any company that committed Copilotgenerated code to their products must cut those lines out and then spend a mandecade of effort patching the new holes in their source, stop doing any business in France, or else open the whole thing for the public to see and use?",Reddit,https://www.reddit.com/r/programming/comments/x61xo0/is_there_any_way_to_opt_out_of_githubs_copilot/,4
"Copyleft is about preventing you from adding restrictions to my code, which is what ""stealing code"" is. Copilot is a paid feature, and it claims the code it outputs may have restrictions added to it. How exactly are we not consistent with ourselves?",Reddit,https://www.reddit.com/r/programming/comments/x61xo0/is_there_any_way_to_opt_out_of_githubs_copilot/,-1
"On the flipside, if the code is factored out into a reusable library, people contribute back, allowing all downstream users to further benefit from the collaboration. Copilot sets it in stone, a onetime imitation that receives no patches, and has no path to pass knowledge back. It's also a commercial product, generating profit for Microsoft and sharing none with the users it learned from.Finally, the point of copyright is to help people feel comfortable publishing their works, as the law will protect their inherent value from being taken and reused by others without consent. People will be hesitant to put code up on Github if Copilot might nab parts of it without respecting its license terms. So other humans don't get to benefit from the code being shared, just the bots that mash it up, separate insight from context from attribution. Your two paragraphs explaining the mathematical trick you pulled off to amortize the cost of a search? Very unlikely to be reproduced without error right before the clever code that implements it.",Reddit,https://www.reddit.com/r/programming/comments/x61xo0/is_there_any_way_to_opt_out_of_githubs_copilot/,-1
"How so? It says on the Copilot page that it uses data from public repositories and internet text. Unless that isn't true, I don't see a problem with it giving you ""secrets"" that are already public. If you don't want your secrets leaked, put them elsewhere.",Reddit,https://www.reddit.com/r/programming/comments/oe5pi8/github_copilot_generates_valid_secrets_twitter/,6
"I feel the ""Don't steal my code!"" perspective fundamentally doesn't understand how these AIs work. They don't memorize specific sequences of words/code. Rather, they see example after example and learn general statistical relationships between tokens.The only time entire blocks of text are copied verbatim are when they are endlessly replicated in the training set. People were freaking out about Copilot replicating fast inverse square root exactly as it was in Quake 3, and the reason being because it has been endlessly copied time and time again, and those who copy typically leave it asis. So when someone starts a string `float Q_rsqrt` the statistical prediction of the following text is very very strong. Just as when someone starts a phrase ""To be or"", you know what will follow because you've seen it so frequently.",Reddit,https://www.reddit.com/r/programming/comments/13fbnte/github_and_openai_fail_to_wriggle_out_of_copilot/,4
"They did not ""steal"" code in any sense of the word. They trained a neural net on open source code.In some sense that is stealing code.These neural nets function almost identically to the wetware neural nets inside our brain. The more code we read, the better we get at writing code.And it's still illegal for your wet brain to steal code. When I write new code, am I stealing from every author whose work I have previously seen? No, I am producing new code, but it is influenced by most every author I have ever read. It is exactly the same with neural nets.Copilot actually copies code though and if you did that you would be in violation of copyright too.These neural nets can not even remember specific things, only statistical weights. They don't have brains so no they don't remember anything. The data they ingested is still there though.",Reddit,https://www.reddit.com/r/programming/comments/13fbnte/github_and_openai_fail_to_wriggle_out_of_copilot/,4
"For people who want actual information instead of garbage clickbait headlines:DMCAA. Plaintiffs claim that copyrighted works do not need to be exact copies to be in violation of DMCA based on a nonbinding court ruling. Judge disagrees and lists courts saying the contrary. This seems like a screwup on the plaintiffs as it's 100% possible to get AI chat bots / code generators to spit out 1:1 code that can be thrown into a search engine to find its origin.B.they “do not explain how the tool makes it plausible that Copilot will in fact do so through its normal operation or how any such verbatim outputs are likely to be anything beyond short and common boilerplate functions.”Nearly everything could be categorized as ""short and common boilerplate functions"". Unless you create some never heard before algorithm, you're code is free for the taking according to this judge. This is nearly an impossible standard.C. In addition, the Court is unpersuaded by Plaintiffs’ reliance on the Carlini Study. It bearsUnited States District CourtNorthern District of California emphasis that the Carlini Study is not exclusively focused on Codex or Copilot, and it does not concern Plaintiffs’ works. That alone limits its applicability.Most AI stuff works the same and has the same issues.D. Accordingly, Plaintiffs’ reliance on a Study that, at most,holds that Copilot may theoretically be prompted by a user to generate a match to someone else’s code is unpersuasive.AI is sometimes unreliable, therefore is immune to scrutiny? Unjust enrichmentA. The Court agrees with GitHub that Plaintiffs’ breach of contract claims do not contain any allegationsof mistake, fraud, coercion, or request. Accordingly, unjust enrichment damages are notavailable.Failure on the plaintiffs again.B.Put differently, the unjust enrichment measure of damages was explicitly written into the parties’ contract.Previous court cases justifying unjust enchrichment onlt went through because there was a clause in the license(""contract"").C. Didn't defend a motion to dismiss, abandoning the claimTL;DR: Not as dire as the article title makes it sound like but plaintiffs have garbage lawyers and California laws suck. Include unjust enrichment in your software licenses.",Reddit,https://www.reddit.com/r/programming/comments/1dzjt2d/judge_dismisses_lawsuit_over_github_copilot/,4
"microsoft won it's war on the GPL with copilot. Now anybody can violate any license just by asking copilot to copy the code for them and copilot will gladly spit it out verbatim.Keep in mind as time goes on copilot will only ""improve"" in that it will be generating bigger and bigger code ""snippets"" eventually generating entire applications and some of that code will absolutely violate somebody's copyright. Also keep in mind there is nothing preventing you from crafting your prompt to pull from specific projects either. ""write me a module to create a memory mapped file in the style of linux kernel that obeys the style guidelines of the linux kernel maintainers"" is likely to pull code from the kernel itself.This judge basically said copyrights on code are no longer enforceable as long as you use an AI intermediary to use the code.",Reddit,https://www.reddit.com/r/programming/comments/1dzjt2d/judge_dismisses_lawsuit_over_github_copilot/,1
"I don‘t think that this is what it means. There‘s a difference between Copilot having been trained on GPL code (and thus Microsoft being liable) and using Copilot to copy GPL into ones project (and thus you being liable).There was never a real chance for Microsoft being liable anyway, because you explicitly grant Microsoft a separate license when uploading your code to GitHub. And they are a DMCA safe harbor.",Reddit,https://www.reddit.com/r/programming/comments/1dzjt2d/judge_dismisses_lawsuit_over_github_copilot/,2
"microsoft won it's war on the GPL with copilot. Now anybody can violate any license just by asking copilot to copy the code for them and copilot will gladly spit it out verbatim.Better! Copy/Paste it yourself, but say Copilot did it.",Reddit,https://www.reddit.com/r/programming/comments/1dzjt2d/judge_dismisses_lawsuit_over_github_copilot/,1
"They got away with training Copilot on everybody elses code, but in doing so they destroyed their credibility as a private repository for enterprises. There is a market for startups that has been torpedoed from the privacy and security point of view.",Reddit,https://www.reddit.com/r/programming/comments/1dzjt2d/judge_dismisses_lawsuit_over_github_copilot/,1
"I don‘t think that this is what it means. There‘s a difference between Copilot having been trained on GPL code (and thus Microsoft being liable) and using Copilot to copy GPL into ones project (and thus you being liable).This statement is nonsensical. I am not copying the code, the AI is. The code appears on my screen and I have no idea where it came from. I don't know which project the code was copied from and I don't know the license that code was released under. Microsoft does know what source code was used to train the AI and what the license was though.There was never a real chance for Microsoft being liable anyway, because you explicitly grant Microsoft a separate license when uploading your code to GitHub.Not a license to copy your code and give it to somebody else.And they are a DMCA safe harbor.That's not relevant to this subject.",Reddit,https://www.reddit.com/r/programming/comments/1dzjt2d/judge_dismisses_lawsuit_over_github_copilot/,-1
"I am not copying the code, the AI is. The code appears on my screen and I have no idea where it came from.You said:Now anybody can violate any license just by asking copilot to copy the code for them and copilot will gladly spit it out verbatim.And now you're really gonna pretend that you have ""no idea where it came from""? And you think that argument will hold up?""Gee your Honor I typed 'the code for GNU EMACS' into Google and some words appeared on my magic light box. I don't have any idea where it came from, though. I had no clue I was infringing copyright!""",Reddit,https://www.reddit.com/r/programming/comments/1dzjt2d/judge_dismisses_lawsuit_over_github_copilot/,-1
"But the person you replied to pointed out the difference between suing Microsoft and suing someone using their product. You said that difference is nonsensical, but I don't think it is. Someone could take a GPL project and put it on Stackoverflow, and I could copy it from there and that would ""defeat"" GPL in the same way. Just copy it, upload it somewhere anonymously with an altered license agreement, and BAM you've cheated it! You didn't write the code after all, someone on the Internet shared it with you, so it's not your fault, right? But I don't think it works like that? Because you can violate a copyright without intending to. So you should still be responsible for what code you use. At the very least, this court case wasn't about that scenario at all, so you can't say that a judge has said it's okay to use GPLed code if CoPilot spits it out for you.",Reddit,https://www.reddit.com/r/programming/comments/1dzjt2d/judge_dismisses_lawsuit_over_github_copilot/,-1
"According to this judge yea.This judge is saying that Microsoft isn't violating copyright. But if you:violate any license just by asking copilot to copy the code for themthere is nothing in the judge's statement saying that you're protected. Just like if you asked Google to find the code for you. What Google is doing is considered fair use. But just because they put the code in front of you doesn't mean you can copy it.Nothing about this allows you as the user to circumvent copyright. Just like Google's ability to show you someone else's code doesn't allow you to circumvent copyright.If your codebase ends up with large swaths of effectively identical code to someone else's copyright, and they sue you, it's not gonna matter where you got it. Copyright infringement does not require either a knowing or willful act. You simply have to have enough of someone else's code in your codebase.",Reddit,https://www.reddit.com/r/programming/comments/1dzjt2d/judge_dismisses_lawsuit_over_github_copilot/,2
"It's not we that did that. It's microsoft and other corporations. They fought against the GPL for decades and this is Microsoft's most successful attack on the GPL to date. They can now freely infringe on GPLed code without consequence and they have given this ability to everybody in order to destroy the GPL once and for all.If microsoft wants information to be free they should open source github, they should open source copilot, they should open source windows and SQL server and office365.The GPL uses the copyright to fight copyright. This is Microsoft destroying the GPL so any corporation can freely steal GPLed code without opening their source code.",Reddit,https://www.reddit.com/r/programming/comments/ye5et2/github_copilot_may_steer_microsoft_into_a/,1
"But even if it is licensed, the data is still on their server and we are agreeing to share it with them, by uploading it to said server. If anyone has super confidential code or material, then it should be saved on their own hardware. Hell if github had a self hosted option, i can understand that, then they would be crossing a line imo. But we literally send the entire codebase for projects to them, explicitlyIt's not like they are sharing your code publicly. They are just using it to create something (internally), and copilot is the result, and the only thing being shared in this situation",Reddit,https://www.reddit.com/r/programming/comments/ye5et2/github_copilot_may_steer_microsoft_into_a/,-1
"I mean if the repo has no license copilot also shouldn't use it because the property is automatically protected in most jurisdictions upon creation (if it doesn't violate existing IP). Only by adding a license stating that you forgo your copyright the code is set free to be used by others.Not that anyone in the history of open source ever cared about this technicality (also the act of putting the code onto a public repo might be seen as enough intent to give up copyright, admittedly).",Reddit,https://www.reddit.com/r/programming/comments/ye5et2/github_copilot_may_steer_microsoft_into_a/,7
"I load some code to GitHub and declare I own the copyright to it. Then somebody uses the Copilot and a large section of my code appears in ""their"" code produced by Copilot.Yes, but then the guy violated your copyright, not the tool he used.",Reddit,https://www.reddit.com/r/programming/comments/ye5et2/github_copilot_may_steer_microsoft_into_a/,-1
"It's not how software license works. When I post my code online, I don't mind people getting inspired and making their own independent thing, but I expect people to be respectful and follow my license. They can't blindly copypaste my code without acknowledgement in case of my MIT projects or without giving their version back to the community in case of my GPL projects. If they violate this, we go to court and they WILL have to pay up by law. What Copilot is, is a fancy copypaste tool. It was trained on all public Github repos regardless of their license, and obscures where it takes its output from. It is hypocritical because Microsoft sued many businesses out of existence due to patents and other copyright claims. It is unethical because Microsoft didn't ask the Github devs if they're ok with this, and didn't acknowledge or share profits with them in any way. It is bad for the industry, because what happens when new devs and students start using Copilot to write their assignments and learn nothing themselves? It may also be illegal, and hopefully the court case sees it that way.",Reddit,https://www.reddit.com/r/programming/comments/ye5et2/github_copilot_may_steer_microsoft_into_a/,-1
"First, you've given me a lot to think about, and I appreciate that. Please don't take my comments as being kneejerk arguments. How would you define storage in a way that includes all methods of storage that you agree are storage, without arbitrary specialcasing?I think storage should be deterministic. If I put the deed to my house or a photograph of my kids in storage (even digital storage), I expect to be able to retrieve that item or a highfidelity copy of it on demand.If I ""stored"" a picture in an AI and I have to keep hitting generate over and over until it randomly gives me my picture, that's not storage. If I store a picture of my uniquelooking kids and my neighbor stores a picture of his generic looking kids, and he is able to easily regenerate pictures of his kids while I never can, that's not storage. Those aren't even pictures of his kids, they are just an average of what pictures of kids look like and his kids look so very average. That's not storage.I should also be able to know I've gotten the whole thing. If one of the kids looks like mine and the other looks totally different, did it draw my kid or did it just draw a kid that happens to look exactly like mine? If it's my kid, how come the kid standing next to her looks completely unlike my other kid?Compare this to a ZIP file. Any time I want I can run the algorithm on my file (which is really close to what the AI does) and get back the exact contents. I don't have to wonder if anything was changed in the process.I feel like this could be a whole philosophical discussion. I'm not saying this definition of storage is correct or complete, but it highlights where I disagree that training an AI is storage. by this definition, the network is clearly storing the code that it replicated.You ask infinite monkeys with typewriters to write literature and eventually get back the complete works of Shakespeare. That doesn't mean the monkeys know it, it's just a function of there being a finite number of ways letters can be arranged on a page. If you could made the typewriter generate a ""best fit translation"" to actual English words, it would be even more likely. And if you further made it so the ""best fit"" algorithm would transform the words by rhyme and meter, it would be more likely still.But at no point do the monkeys know Shakespeare or attempt to reproduce it. This is not about storage, or even reproducing Shakespeare's works; this is about designing a system by which monkeys can write literature. And the more helpful the system, the more likely it is you'll get a copy of Shakespeare. Or at least an exact replica of a sonnet or two. you know, they're copying the code because it is free software released under the LGPLYeah, I'm a little sheepish I didn't think of this. You're right that freeuse code is going to be very common to the point of approaching determinism. if copilot was trained exclusively on free software and required that its output include copies of all the licenses used in the training data, there might still be an ethical question about the impact on communities, but legally it would be in the clear.That assumes copilot is even possible or good if trained on the limited set of data. It could be the licensing requirement would render any conversation moot. The more good training data you input, the better the AI will be. one can avoid infringing by examining the contents of one's own mind [...] either you remember their code, in which case you know not to infringe it, or you don't remember their code, in which case you are incredibly unlikely to reproduce it nearverbatim down to the specific phrasing used in the comments.This instinctively feels true, but I wonder if it really is. A person who strictly follows a certain pattern wouldn't be at all unlikely to have developed infringing patterns. The example I see everyone point to has very generic structure and comments. If you asked me to write that function, it might turn out very much the same. If you then required me to comment it using best practices, I might well come up with similar comments. Particularly if I'd see that pattern before.And if you consider that a lot of the personalization we might do is actually due to laziness or not following best practices or ""trying something new"" then I wonder if a lot of that is just crappier versions of what we'd like to be writing but my blood sugar is a little low or my boss is breathing down my neck and I don't have time to dot every last 'i' and write every last unit test. Having an AI might not be too far off from just having something to remind you of best practices and weird edge cases that you always forget or haven't run into, all of which would tend to make us produce much more homogenized code. you're essentially eliminating the use case for copilot because that takes more effort than just writing the code yourselfNah. Someone (you?) suggested just automated scanning for infringement. You'd have to do something like that, and frankly I'm not sure why code written by humans should get a pass. And you'd have to have an algorithmic definition of infringement (requiring a court of law to make a determination isn't something you can automate in code). But then, if you had that you'd have all the tools necessary to change any code just enough that it no longer infringes. But if you can copy someone else's code verbatim and then run a ""deinfringifier"" on it to be legal, that is a complete violation of the spirit of copyright and licensing.I could go on for quite a while and I find the conversation really interesting and stimulating. But I'm off to Halloween festivities, so I'll leave it at this and wish you a happy weekend.",Reddit,https://www.reddit.com/r/programming/comments/ye5et2/github_copilot_may_steer_microsoft_into_a/,-1
"If you know how the AI works, I’m not sure what your expectation is.A. get explicit permission from each individual project before running an AI on it. B. Add a hidden ID to each line of code the AI reads.(or multiple ID's for lines that are repeated in multiple projects) When multiple lines of code are repeated from a single project while using Copilot, give a popup of the license that is attached to that code, so it's up to the user to see how and whether they can use it.C. after writing code with Copilot, copilot runs a 'fraud checker' (which already exists for grading papers) or similar, checks for similarity against OS projects, and hands you the license if needed.D. Copilot becomes a non profit project with it's own Open source license attached to any code it generates.That's 4 reasonable options just on top of my head. I'm sure microsoft has plenty of resources to think of more reasonable resolutions.",Reddit,https://www.reddit.com/r/programming/comments/ye5et2/github_copilot_may_steer_microsoft_into_a/,-1
"CoPilot is dangerous. It hands you big blocks of syntax with tiny bugs in them. Mistakes that you wouldn't have made if you had typed out the source yourself but are hard as hell to fix because they're deceptively hidden the AI didn't spot them, so why should you?",Reddit,https://www.reddit.com/r/programming/comments/z02zhu/who_needs_github_copilot_when_you_have_cheatsh/,0
"Oh, I’m not knocking your project at all, trust me. I haven’t tested it yet but I will. Copilot being $10 a month is absurd. I actually abandoned it and am using Amazon Codewhisperer at this point, although it’s not quite as good.I definitely don’t recommend Copilot to new programmers. It does way too much of the work for you and it would be easy to become reliant on it and not learn syntax.",Reddit,https://www.reddit.com/r/programming/comments/z02zhu/who_needs_github_copilot_when_you_have_cheatsh/,0
"Apologies, this is a programming forum, so I presumed a level of technical knowledge. To train a supervisedlearning AI, like Copilot, requires a corpus of knowledge to reproduce when prompted. Microsoft, without the consent or knowledge of authors, ingested all of GitHub into the training corpus for Copilot. That is why I used the term 'coercion', because our ( my ) code was forced into a commercial utility, and no author had any say, or share in the profits.The same argument is being hashed out in the publishing realm, with GPT3 being trained from the unwilling efforts of the worlds authors, and the same in the visual arts, with the models behind DALLE and Stable Diffusion ingesting the entire human effort at making pretty things.Legally, this is a big, interesting deal, b/c what does copyright mean when an individual or a business cannot profit from derivatives of copyrighted work without licensing, but AI can produce derivative works for profit, and not only are the original works unlicensed, but they often times have no providence at all? Arguably, copyright in the post Mickey Mouse era has itself been twisted into a tool that doesn't protect creators, so legal arguments which force more attribution and compensation certainly will help creators.Lets say, hypothetically, you made a bot to make stupid comments on reddit and get into pointless arguments with people. AND you used an open source license which prohibited commercial reuse without attribution. If someone took that code and made $10/month per user on it, wouldn't you feel entitled to some part of that commercialization of your innovation? Wouldn't you believe that someone acted against your expressed terms of license, and that your work was wrested out of your control, without consent? Hence, the term 'coercion'. You might want to look things up in a dictionary before Princess Bride memeing as a stimulusresponse. Of course, you might have been programmed that way, so it's not your fault. I blame Copilot.",Reddit,https://www.reddit.com/r/programming/comments/z02zhu/who_needs_github_copilot_when_you_have_cheatsh/,4
"Copilot should just take the license of the project into account and filter out incompatible snippets. In other words, they need to tag their internal data with the corresponding license. That might be too late at that point, but they should have thought of it first (doesn't GitHub have an ethic committee, the same way universities validate a project/thesis before publication?).IANAL but I had another thought: given that Copilot potentially produces (pastes) GPLlicensed code, it could be considered to be itself a derived work, hence the code of Copilot itself should be released under GPL.",Reddit,https://www.reddit.com/r/programming/comments/ocx11p/github_copilot_research_recitation_analysis_on/,-1
So how do I poison CoPilot into producing wrong answers or exploits?,Reddit,https://www.reddit.com/r/programming/comments/ocx11p/github_copilot_research_recitation_analysis_on/,0
"Copilot should just take the license of the project into account and filter out incompatible snippets.Other than public domain, what license is compatible?Almost all licenses require at least attribution, and this violates that.",Reddit,https://www.reddit.com/r/programming/comments/ocx11p/github_copilot_research_recitation_analysis_on/,5
That’s so much hassle. It’d really ruin the value of copilot.Ignoring licenses and required copyright attributions if far easier. Why would Microsoft follow the rules?,Reddit,https://www.reddit.com/r/programming/comments/ocx11p/github_copilot_research_recitation_analysis_on/,1
"Why poison? There are probably thousands of known exploits already in the code used as training set, probably in many cases already with a ticket pointing the problem out. Now we just need an AI to parse Github tickets for unfixed exploits and to check if copilot reproduces them.",Reddit,https://www.reddit.com/r/programming/comments/ocx11p/github_copilot_research_recitation_analysis_on/,0
"Pick a very rare and/or obscure string that you will expect to actually come up in your victims copilot prompts at some point. Create a lot of github repositories, ideally under many different users. Include the chosen rare string directly above the exploit code you wish to inject. Bonus points for adding this as part of an unused/inaccessible function in larger repositories of very useful code.Then wait, and hope copilot retrains, and that your victim uses your chosen string and asks copilot for a suggestion shortly afterward. This gets easier if you just want to inject an exploit somewhere rather than against any particular targets.",Reddit,https://www.reddit.com/r/programming/comments/ocx11p/github_copilot_research_recitation_analysis_on/,0
"To create a strong incentive for GitHub to take the issue of licensing seriously. I disagree with the approach they took with Copilot training data, where it looks like they took public projects on GitHub and blindly trusted the project license with little to no due diligence.I don't believe companies should be permitted to scrape public codebases to create a commercial product that has a serious risk of spitting out said code verbatim (""license laundering"").Either they do their due diligence in sourcing their training data, or they improve the model so it focuses exclusively on the noncopyrightable structure/intent of the training data rather than the specific protected expression of it.Of course, this doesn't even get into the patent question. Just because code is publicly available doesn't mean it's not patentencumbered.",Reddit,https://www.reddit.com/r/programming/comments/ocx11p/github_copilot_research_recitation_analysis_on/,2
"copilot has been confidently wrong for me at work so many times, the only use i’ve found for it so far is basic scaffolding of something like terraform, but i often have to fix mistakes it makes so i’ve learned not to trust it",Reddit,https://www.reddit.com/r/programming/comments/17s1iyt/microsofts_github_announces_copilot_assistant/,0
"Oh boy, the inevitable reading comprehension fail. The headline implied MS would use private GitHub repositories to train CoPilot. This isn't the case, but it was easy to misread thus. This is entirely possible because MS own GitHub and GitHub repositories are not encrypted at rest. The only technical provision separating private from public repositories is the GitHub security layer. I replied to a poster who was saying that if this was true, so what? Just don't use CoPilot. I pointed out that this was not a solution, since MS would still be training CoPilot on your code whether you used CoPilot or not (which is already the situation for public repositories).In practice, MS are only training CoPilot on private repositories if those repository owners optin. Legally, of course, this is the only viable option, but since MS are as far as I know totally ignoring copyright / licence conditions in public repositories e.g. only use with attribution clauses then it certainly isn't beyond imagination that they might have sufficient hubris to try it anyway.Fortunately, as we stand today, it seems they are not.",Reddit,https://www.reddit.com/r/programming/comments/17s1iyt/microsofts_github_announces_copilot_assistant/,-1
The company develops a product to block copilot and publishes an article which doesn't have any solid argument. Bug and security risks stand for basic tutorials and stackoverflow answers as well. Are we supposed get rid of all online resources? It is the tool. An engineer copy pasting simple snippets and deploying to production doesn't make it evil.Pls stop shitting tools just to sell your product.,Reddit,https://www.reddit.com/r/programming/comments/yxyz0d/github_copilot_isnt_worth_the_risk/,7
You likely don't need to give permission to have your code used to train the model. That could be done even without your permission as training the model is probably not copying or distributing the code.Furthermore even distributing the trained model is probably not copying. If you assume it is then you have to address the question of if an individual recalling a song they heard on the radio in their mind is subject to copyright. Could the thought police arrest you for copyright infringement for thinking about a tune?The problems with copilot really kick in when someone uses it and it regurgitates some code that it recalls from its training data. The user of the tool has to make a judgement call if that code is novel enough to be subject to copyright and evaluate whether or not it should be used all without any insight into the provenance of the suggestion.If they include it in their program and distribute that program then they may have violated an unknown parties copyright.,Reddit,https://www.reddit.com/r/programming/comments/yxyz0d/github_copilot_isnt_worth_the_risk/,9
"I'm quite impressed with Copilot but not to the extent that I can trust it at all with code. Maybe this new scope suits it better as there is less wiggle room in the procedure itself, but I can only see it work in small bugfixes and maybe common feature requests.It seems the diminishing returns are coming faster than anticipated, but hey if it helps out in clearing out clerical task I'm still happy.",Reddit,https://www.reddit.com/r/programming/comments/1cgr1l5/github_previews_copilot_workspace_an_ai_developer/,0
"I actually found the unit test recommendations to be pretty decent, granted they’re fairly limited in scope and complexity by design. That’s where I found Copilot to do the best right now for me. I wouldn’t trust it with anything actual critical and lots of edge cases.",Reddit,https://www.reddit.com/r/programming/comments/wsnend/since_github_copilot_free_preview_ends_soon_what/,0
"Seems to me OpenAI/GitHub's lawyers thought that through before offering this product.I disagree with their conclusions. Unless their conclusions were simply ""don't care, it's not against license terms to create the Copilot program so we can make money selling this"". But then that's not own we function at the moment.Take a look at what happened with Blurred Lines. Unfortunately your lawyers can say you're right and the courts can disagree. What if that AI rediscovers everything without 'stealing it' from the Internet, it could still violate patents, copyrights etc right?It wouldn't matter. Patents aren't copyright. Even if you parallel construct you still are in violation. Wait, did you meant that their licensing isn't protecting you from thirdparty attacks? What kind of licensing would allow you to use it?I cannot say who created the code that was put in my file. Their program just picked it out of other people's sources. Also I can say that I didn't create those parts, something else did. So it's hard to assert ownership (the root of being able to apply a license to something).There's no fix for this. No matter under what terms or license the GitHub program is offered to me.",Reddit,https://www.reddit.com/r/programming/comments/wsnend/since_github_copilot_free_preview_ends_soon_what/,-1
"Yeah, I 100% do not trust this study.I've used copilot. It's definitely not writing code that is working even 10% of the time.",Reddit,https://www.reddit.com/r/programming/comments/1eniyo4/based_on_copilot_usage_trends_more_code_on_github/,0
"chatgpt/gemini coding is so awful. i only trust it to write a skeleton for a program. like 'i need to use this to do this and i need it like this, lay out the functions you think i'll need and the basic framework'...and it'll do it. and that's literally all it can do. everything else it just makes shit up and then you're stuck in a loop of 'you made that shit up, fix your shit' and your burn all your tokens. which is why i stopped paying for that and switched to copilot. better autofill bullshit.",Reddit,https://www.reddit.com/r/programming/comments/1eniyo4/based_on_copilot_usage_trends_more_code_on_github/,0
Are the authors of the code which Copilot steals from getting a cut of that 10$/month?,Reddit,https://www.reddit.com/r/programming/comments/vhhuun/github_copilot_is_generally_available_to_all/,-1
"Everyone talks about how this violates the GPL, but it violates just about every license but CC0 / public domain licenses.MIT, BSD, etc. require you to propagate the copyright notice, which Copilot strips. Basically if code is produced by Copilot, it's illegal to distribute in any form.",Reddit,https://www.reddit.com/r/programming/comments/vhhuun/github_copilot_is_generally_available_to_all/,5
"Wouldn’t Copilot be considered derivative work if it uses GPL licensed source code in it’s training dataset?Likely no. Using copyrighted data to train a model is likely not protected under existing copyright laws (though it’s never been tested as far as I know)The “issue” is that copilot then pasted the GPL code into new documents. However, the snippets are too short to be considered copyright infringement. There isn’t an exact minimum length, but there is existing precedent that copyright only protects a complete work. Taking small snippets from a protected work and using them in a transformative way is fair game",Reddit,https://www.reddit.com/r/programming/comments/vhhuun/github_copilot_is_generally_available_to_all/,9
"I don't get why this is upvoted. This is so wrong it's dangerous.Your company using a SaaS Git host in no way gives you permission to send company source code to another random recipient. Even if they fall under the same company as the Git host.So it doesn't matter if your company selfhosts Git, uses Gitlab or Github SaaS: make sure you have permission in writing before you use Copilot on your company's code.",Reddit,https://www.reddit.com/r/programming/comments/vhhuun/github_copilot_is_generally_available_to_all/,-1
"Which is the danger of having an open beta. GitHub just released copilot officially, and this (what should be a bug report) is from nearly a year ago. Everyone is quick to point to this specific instance, and I haven't heard anything since. GitHub's mistake for opening the beta before being certain to squash this.",Reddit,https://www.reddit.com/r/programming/comments/vhhuun/github_copilot_is_generally_available_to_all/,3
"Alright, points to unpack here:1. Fame of the code snippet.2. Fair use and the GPL.3. Hosting your code on GitHub.1: Fame it's a famous functionAnd that's the kicker that got my attention. It should be simple enough to have CoPilot generate a ton of code and then search for instances of those snippets to try and identify a specific source.Turns out Gitlab did this before releasing the beta, and looked for code that is repeated exactly in 60 words at least, and found that out of 453,780 code suggestions, only 473 (roughly 0.1%) matched some of the training code in at least 60 words. [ the paper they break down those matched instances and demonstrate why got through the prefilter and were questionable as matches (lists of primes, literal lists of alphabetic characters, etc). But instances still remained, and here's the kicker: ""Of the 41 main cases we singled out during manual labelling, none appear in less than 10 different files. Most (35 cases) appear more than a hundred times. ""In other words, the more popular a snippet is, the more likely copilot was to pick it up. And fast inverse square root is absolutely perfect for that. It's very small, takes a float and returns a float, has no dependencies, and is very famous and frequently discussed.2. GPL then yes you are bound by that codebase's licenseNot so fast there. A license is a grant by a copyright holder determining under what conditions a derivative work may be created. What does it take to produce a derivative work? Quite a bit actually.A great example is the The Author's Guild vs Google. When Google Books came out and allowed you to search copyrighted material and view several pages at a time, effectively reproducing copyrighted material. This brought a lawsuit by the Author's Guild, but they lost in court because Google's use of the copyrighted work was found to be fair use. Even though several pages of dense textbooks could be read at a time, the scope was limited enough to be within the realm of fair use.[ also applies to GPLed code. Don't believe me? Ask the authors of the GPL, the FSF:[ expected them to hem and haw about CoPilot, as the legal landscape for machine learning produced works is thin and copyright cases can have leeway depending on the judge. Nonetheless, the FSF found that ""GitHub’s use of the code repositories to train its machine learning model is likely fair use"". It's not like the FSF is unaware of the fastinverse square root example, this paper was written this February. Jump to Part B of the legal analysis for how they reached their conclusion.3. Hosting on GitHub The previous two points don't even actually matter. Because the code is hosted on GitHub. The terms of service when you use GitHub grants them implicit license to effectively do as they please with your code. They can copy and reuse to their heart's content. It doesn't matter what license you attach. Of course, this presumes you are the code's owner (again, a license is a grant by the copyright holder).This is part of why GitHub had to manually curate what repos they used to train, as they wanted to know that the actual owners of the code were the ones hosting their code on GitHub. And yes, id Software themselves chose to post Quake 3's GPLed source on GitHub, thus granting them use of that code.Licenses like the GPL do not bind the code's author from producing private derivative works. This is why you can have companies produce modified payfor versions of code that they also release under GPL. As the owners of the code, they have the authority to do so. And by being an author who choses to host code on GitHub, they're effectively duallicensing their software.",Reddit,https://www.reddit.com/r/programming/comments/vhhuun/github_copilot_is_generally_available_to_all/,-1
That is not true. You insert the code. Copilot makes suggestions. You are responsible for the code you write. I have used it for many months and I always review the code. Its basically never advanced code and I usually have to edit it because I'm not happy with the style. I would say if you manage to generate code that is not covered by prior art then its basically you that is the problem. You basically would have to try to trick it to get code that would violate anything and I think they plugged those holes. Just because a library is GPL does not mean that a function within that library that pops the first item is protected because it's prior art. You can't copyright quicksort etc..,Reddit,https://www.reddit.com/r/programming/comments/vkchoh/papers_on_github_copilot_copyright_law_questions/,5
"GitHub fixed this specific instance (inverse square root) by preventing the tool from suggesting this specific code, rather than making their algorithm not steal code. assume that nothing has changed, and that Copilot can still steal code (i.e. distribute code without attribution (license compliance)).",Reddit,https://www.reddit.com/r/programming/comments/vkchoh/papers_on_github_copilot_copyright_law_questions/,3
I don't know if you even read my comment. I wrote in most cases. Also if I wanted inverse square root I wouldn't trust copilot to generate the code I would use a lib. It would be scary stupid to trust copilot for an advanced algorithm.,Reddit,https://www.reddit.com/r/programming/comments/vkchoh/papers_on_github_copilot_copyright_law_questions/,-1
"As I've said in other threads: the Copilot model itself is nothing more than a statistical analysis of other works. As such, yes, it's a derivative work, but it's also transformative and the model itself is not a market substitute for any of the inputs. As such, it's almost certainly fair use the model is itself a collection of facts about programming data.But the output of the model could very well violate copyright.As an interesting side effect, since the model itself is just statistical information about its training data set, it actually might not be copyrightable. I have no idea if there's any relevant case law here, but copyright doesn't protect factual information. I assume that there will be confirmations that ML models are protected by copyright at some point in the future, but I don't know if there's any consensus on that yet.After a moment's research, I found [this]( case, which held that an engineering model did not merit copyright protection. Now that case was more narrow, but… I have a suspicion that an ML model wouldn't get copyright protection under current laws, and that copyright law would need modified before it could guarantee protection to ML models.",Reddit,https://www.reddit.com/r/programming/comments/od5i24/github_copilot_and_license_restrictions/,9
"One can not answer ""Is the Copilot model a derivative work?"" by ""I think the answer must be no [otherwise some business models may fail while other will continue to collect ridiculous amount of data]"".Or else I could as well answer ""should I be allowed to steal a bank?"" by ""I think the answer must be yes, because otherwise I will probably not become a billionaire, while billionaires exist and will continue to""Not to say that ""are ML models derivative work?"", generally, or ""is the Copilot model a derivative work?"", specifically, should be answered the opposite way, but I found the reasoning presented here particularly weak. Yes, there are tons of businesses that would be possible if we removed random laws and/or regulations, and yes some situations constitute some barrier to entry, but that is not enough IN ITSELF to declare that the obvious solution is to remove laws/regulation/barriers or even rule that something previously debated has a ""good"" solution because doing so could have negative side effects that must also be taken into account, and by this simplistic method are not even considered...",Reddit,https://www.reddit.com/r/programming/comments/od5i24/github_copilot_and_license_restrictions/,9
"Part of those questions are answered in patent law. However, software patents are a hot mess discussion as well. Plus, there's overlap with copyright of software, which adds to the ambiguity since both are different types of intellectual property protection. for reimplementation: there's case law already. In order to avoid copyright violations, a clean room design approach tends to be used. Do note that this approach protects against copyright infringement, but not patent infringement. first season or AMC's Halt and Catch Fire basically revolved around clean room design as a premise for the series, which was a loose recreation of how tech businesses in the 80s actually operated)So, is this legal? I think there's no clear yes/no question. Copilot is ultimately a machine, just like a xerox machine. The machine itself isn't the issue: it's the context that matters. Feeding copilot code which is in the public domain isn't an issue. Feeding it code which is sold under a proprietary license? That's a legal liability if there is any. Just that in and of itself is already reason to critically reconsider if this is one of GitHub's better ideas.The big issue is that the value of free software licenses or copyright in general is measured by the extent the protections and freedoms enshrined are affirmed and enforced in a court of law. One such example would be SCO vs IBM in which SCO asserted legal uncertainties regarding Linux due to alleged violations of IBM's Unix licenses. for the majority of GitHub users, I think you already compromise your ability to protect your copyright to an extent by agreeing to their ToS and uploading your code with them. At the end of the day, you, the individual, are at a disadvantage in dictating the terms of the relationship towards GitHub: the benefit of hosting with them is discoverability, an affordance you never get by hosting by yourself. Programs like copilot tend to externalize costs to the extent that using GitHub becomes a game of diminishing returns the more involved, business critical, publicly important your code becomes. I don't think there are easy answers as to how to approach this.",Reddit,https://www.reddit.com/r/programming/comments/od5i24/github_copilot_and_license_restrictions/,7
"The model isn't the issue, the fact that it's output can and does steal code in violation of licenses is the issue.Copilot will, unless you carefully opt out fast enough, will not only train on your code, but will happily grab entire functions wholesale and give them to other coders with zero attribution or respect for the GPL or license requiring attribution.While training probably isn't in itself illegal, when entire code functions match software that has a license requiring opening up your source code or attribution, unless you turn your code GPL or providing the required attribution, that is a violation. And Copilot cannot claim have to have discovered it independently because it DID look at the original version of the code during the training.You can say, yes my code happens to match existing code that i haven't seen. you don't get to claim that if you HAVE seen the code before. Intent to copy doesn't matter, what matters is if you saw the code before, because once you've seen it, you can't prove you didn't copy it. Copilot shouldn't get a free pass her because it's an AI.Now if you ARE making a GPL product yourself, then you can most definitely yoink other GPL code with zero issue. But that doesn't clear you of needing to providing attribution for the licenses that require it.A responsible Copilot would need to check it's output against published code, and if it significantly matches, check the license for the code at the time it was originally trained on it, and popup gpl and/or attribution notices.",Reddit,https://www.reddit.com/r/programming/comments/od5i24/github_copilot_and_license_restrictions/,9
"Microsofts codebase should be in a lot better shape than most of the public code on Github and they have a lot of it. There are so many badly coded, bad examples or even joke projects on Github, and they for sure were included when training the model. On the other hand Microsoft's code has to at least be up to some standard.As such it only makes sense to train the model on that code too. And since Microsoft claims, that the AI doesn't create derivative code, it would make their claim a lot more trustworthy if they also had something at stake.I put my code on Github, because I want the users of my programs to benefit from the ability to change the code to their liking. That's why I use the GPLv3, so that everyone will always have to provide the source code for the user to modify. If someone can use my code now to write an application, where the user does not receive the source and with a lot less effort, that puts the freedoms of my users at risk. So I disagree strongly with such a tool, that just exists to avoid using a library or to circumvent the license, when copying a snippet of code.It was already possible to efficiently benefit from the opensource code on Github. While copilot may be a bit simpler to use, the big change it brings to the table, is that you can benefit from code without having to respect its license. The only reason to write code someone else wrote before, is if you want it to be under a different license. Otherwise you would refactor the code into a library. I don't think copilot is sophisticated enough to come up with significantly different solutions to a problem.",Reddit,https://www.reddit.com/r/programming/comments/od5i24/github_copilot_and_license_restrictions/,2
"Microsofts codebase should be in a lot better shape than most of the public code on Github and they have a lot of it...I said this is my reason. I don't care what MS's reason is. As such it only makes sense to train the model on that code too. And since Microsoft claims, that the AI doesn't create derivative code, it would make their claim a lot more trustworthy if they also had something at stake.You don't understand copyright.If you hold the copyright, you have the right to do or not do something with the work as you wish, entirely. MS doesn't want to? That itself is a valid reason. Copyleft licenses are protected by copyright and extend rights. Specifically, GPL gives the extended right to create a derivative work, if the restrictions are followed, to be seen and understood (by human or AI eyes) and to be used in many other ways (but puts some limitations). ...is that you can benefit from code without having to respect its license... (and the preceding paragraph)Whether you like it or not, GPL3 lets MS use the code. It's not your fight to even make. You decided to use an open source license that you yourself did not write. Therefore while it may fit your use cases perfectly, there's things that aren't considered, because the extended rights provided in GPL3 as considered by MS, are not the derivative work clause, but rather the mere ingestion of information, which is a part of the infinite subset of rights extended in allowing others to read and understand this code.Because of this, you have no basis in court. The only chance you have is if the GNU/FSF decide to fight this as a whole. Which, they've had some time to do, and I am yet to see even a notion of disapproval, which means they probably agree with MS.Now, the solution (for you) is simple write your own license. Or a derivative of GPLv3 that explicitly disallows everything you want to disallow, but this would be a difficult feat because you (in legalese) have to find a way of inputting a concrete definition for a possibly infinite subset of an infinite set of actions.Oh, and you'd have to no longer host on Github. Because license aside, Github's TOS probably allows them to have some minimal use (including for copilot) which supersedes your license by mere fact that you decided to host on Github. Now, Gitlab and Bitbucket have the same, they just might not be saying out loud what they are doing with your code.But let's be honest, you're not going to self host. It's just too costly. But there's a saying nothing ever is truly free.",Reddit,https://www.reddit.com/r/programming/comments/od5i24/github_copilot_and_license_restrictions/,2
"The second half of this is exactly why I wrote this article: if you're unhappy with your GPLv3 code being used to train Copilot, then talk to GNU/FSF about it. At least raise it as an issue you're concerned about, in the field of FSF's expertise. If enough people do, then maybe they'll be convinced to provide a response for the benefit of the FOSS community. Most of all, don't just assume FSF ""probably agree[s] with MS."" They might be working on a response already – it's been less than a week.Also, while I did talk about how GitHub's TOS creates their license to use projects hosted with them for things like Copilot, I actually don't see anything in GitLab's TOS to the same effect, at least not as obviously. Some items in the privacy policy, and possibly section 7.8 of the subscription agreement, might be construed to grant GitLab the license to analyze your code, but it's much less convincing than GitHub's ""we can use anything you provide in whatever ways we want to improve our products"" blanket. I'd be interested to hear from a proper legal authority whether my reading is correct.",Reddit,https://www.reddit.com/r/programming/comments/od5i24/github_copilot_and_license_restrictions/,2
"You don't understand copyright. If you hold the copyright, you have the right to do or not do something with the work as you wish, entirely. MS doesn't want to? That itself is a valid reason. Copyleft licenses are protected by copyright and extend rights. Specifically, GPL gives the extended right to create a derivative work, if the restrictions are followed, to be seen and understood (by human or AI eyes) and to be used in many other ways (but puts some limitations).I'm not saying MS has to train copilot on their own code. I'm saying it calls their argument of ""copilot doesn't create derivative work"" into question, if they don't train the model also on their own code, that they are so scared of being leaked. They would have the moral high ground, if they had trained the model also on their own code, but they didn't and as such one can argue, that they are afraid of the model creating derivative work of their own code. Whether you like it or not, GPL3 lets MS use the code. It's not your fight to even make. You decided to use an open source license that you yourself did not write. Therefore while it may fit your use cases perfectly, there's things that aren't considered, because the extended rights provided in GPL3 as considered by MS, are not the derivative work clause, but rather the mere ingestion of information, which is a part of the infinite subset of rights extended in allowing others to read and understand this code.I have no issue with Microsoft using my code, as long as they respect the license. And that doesn't mean the model needs to be licensed as GPL3, but if the model reproduces code that looks like my code, it should be licensed as GPL3. Because I consider work done by cp or source obfuscators as derivative as well. Changing the way the copy and obfuscating works and throwing in the magical words ""artificial intelligence"" into the mix, does not change that. We have already seen copilot reproduce GPL licensed code verbatim.I probably don't have the resources to fight that in court, but I think there is a basis for it and someone will fight it. Oh, and you'd have to no longer host on Github. Because license aside, Github's TOS probably allows them to have some minimal use (including for copilot) which supersedes your license by mere fact that you decided to host on Github. Now, Gitlab and Bitbucket have the same, they just might not be saying out loud what they are doing with your code.I do host most of my code on my selfhosted Gitlab instance. Some of it is mirrored to Github by me, because others are contributing to it as well and it eases the process, but some code is uploaded to Github by someone else than me. They legally have the right to redistribute the source, but I don't think Github has the right to my code that way. The only clause that gives them some notion of rights is the ""improving their services"" clause, if I remember the Github ToS correctly, and I'm not sure that gives them the right to copy my code under a different license. Especially since I don't see codepilot as part of their code hosting service. But that could very well have been their reason for buying Github.If reading source code does not taint your reproductions of similar function with the license, then we wouldn't have to go through all the effort of clean room reverse engineering pieces of software. There would be no need to sign off on never having read Windows source code, when contributing to wine, since clearly any of your contributions are not affected by the copyright of the Windows source code you read.I'm interested how this will turn out, but considering how scared of the GPL Microsoft used to be, this is certainly an interesting turn of events.",Reddit,https://www.reddit.com/r/programming/comments/od5i24/github_copilot_and_license_restrictions/,9
"Not saying I'm a 10x greybeardI am very strongly of the opinion that 10xers do not exist, and that when people think they've found one, they're either the result of a better work environment, not having to supplicate to a boss, or the person speaking is just used to 0.1xers, who do exist&nbsp; I prefer using it if I'm writing a script that I need for a workflow or cron job. I can tell copilot most of what I want to accomplish and it's incredible how well it does the job when it does work. And in those cases, it's hard to not see it's value because of how much time and cognitive load it saves.The problem is, I've spent my career watching brilliant programmers write bash scripts where the problem doesn't surface for two years, then suddenly nukes everything from orbitAlso myself, a notbrilliant programmer&nbsp; This is a tool. Tools have specific use cases.I'm not saying you're wrong. I cannot be certain. However I have a different opinion.My opinion is that it's too dangerous, despite its value, because bugs can be and very often are subtle.Every time I've studied Copilot output in detail, it's been deeply dangerously wrong in subtle ways. I have no faith that I found all the problems or that I ever would.Think about humans just arguing over the right way to atomically write to disk, on stack overflow.Humans in an active factbased discussion often take years to work out which method has no subtle flawsWhat I worry about is that the user of Copilot is incorrectly gauging that risk. To me, this seems like building skyscrapers out of steel you got from organized crimeHoly shit, why take a risk like this over costs this lowSpend some time reading the IOCCC then tell me with a straight face that you can audit that code, I guess is my pithy way of saying it?",Reddit,https://www.reddit.com/r/artificial/comments/z3j9pr/are_we_ready_for_aigenerated_code/,0
"I feel like what you mean by a model doing a “good job” is basically AGI, which is when all hell will break loose. I agree that Copilot is definitely a long ways away from producing even “good” code, but what it can do is still useful in many ways.I think people use ""AGI"" as a smurf word, that just sort of smurfs its way into the smurf when they're not smurfy enough to use specific concepts and terminology in a technical discussion.AGI isn't going to happen in your lifetime. We're just excited about pretty pictures and imitations of code. You could also do this stuff with fridge magnets[These systems have had convincing output for decades]( That generator has successfully submitted ""papers"" to journals dozens of times.The goal isn't to be convincing. The goal is to be correct. Bugs are stupidly subtle. I do not trust this machine's output.",Reddit,https://www.reddit.com/r/artificial/comments/z3j9pr/are_we_ready_for_aigenerated_code/,0
Did they specify whether they collect the code for future training data if we opt to use copilot? Kinda scared about privacy and IP reason.,Reddit,https://www.reddit.com/r/OpenAI/comments/1hhmgka/github_copilot_goes_free/,1
"Microsoft Copilot analyzes data on both their server and on local devices, depending on the type and source of the data. For example, if the data is stored in Microsoft Graph or other cloudbased services, Copilot will analyze it on their server using ChatGPT version 4 and Microsoft Graphs. However, if the data is stored locally on the user’s device, such as in an Excel spreadsheet or a Word document, Copilot will analyze it on the local device using a smaller version of ChatGPT that runs ondevice. This way, Copilot can ensure faster performance and better privacy for the user’s data",Reddit,https://www.reddit.com/r/OpenAI/comments/121odvl/will_copilot_run_through_microsofts_servers_does/,1
"Yeah I'm having trouble finding any evidence that Copilot or a smaller version of Copilot would run locally.I did not think Microsoft/OpenAI would want to share an ""offline"" version of a fully trained LLM because it could be extracted by hackers/competitors/anyone.. I figured their trained LLMs would always be hosted on their own servers, accessible only through queries (for the time being, until it's limitations are truely known).",Reddit,https://www.reddit.com/r/OpenAI/comments/121odvl/will_copilot_run_through_microsofts_servers_does/,1
"I'm trying to parse Microsoft's publications for how they are specifically dealing with security/privacy issues but I'm only finding generic language such as""Copilot is integrated into Microsoft 365 and automatically inherits all your company’s valuable security, compliance, and privacy policies and processes.""and""The key to unlocking productivity in business lies in connecting LLMs to your business data — in a secure, compliant, privacypreserving way.""I can't find any specifics where they outright say that a ""smaller, local"" version of GPT is used for local data without transmitting any of that data to their servers. I didn't think they could shrink it down small enough to do that yet.. ..also, I didn't think Microsoft/OpenAI would want to allow the trained LLM itself to be accessible on a local computer.. couldn't a hacker then extract the trained LLM and do whatever it is they'd want to try to do with it? I figured Microsoft/OpenAI would want to protect their final/trained LLMs and simply host them..They mention they are ""grounding"" Copilot to specific apps, but that does not, in and of itself, mean that Copilot runs locally within that app. It just means that it is restricted/focused to think about our queries in terms of the app we are running it in.",Reddit,https://www.reddit.com/r/OpenAI/comments/121odvl/will_copilot_run_through_microsofts_servers_does/,1
"You can expect the data to be treated just like data on Azure servers. For most of cases you mentioned, the data can be stored in cloud, in Azure (that is Microsoft) servers, I don't think I've ever seen the ""legally data is not allowed to be transferred"" argument, and frankly it doesn't make sense..when you copy the data, it's being transferred to another medium (tape) and then those tapes are being transferred to a storage facility, most likely under control of Iron Mountain (a company), and any sensitive data is required to be backed up (by law in a lot of cases, in some cases multiple times a day), so a bad on transfering the data simply doesn't make sense...I know that a lot of legal entities have minimal requirements for data safety, and a lot of data has to satisfy that. Microsoft Azure already satisfies pretty much all of those requirements, so does O360, and so will copilot, as it's just an extension of O360.Basically, if your company is using Outlook, it probably is using O360, and thus it will probably be ok with copilot.",Reddit,https://www.reddit.com/r/OpenAI/comments/121odvl/will_copilot_run_through_microsofts_servers_does/,1
"Yeah, sorry what I meant about legally not transferring data was basically taking additional and unnecessary risks with the data which the controller of the data may be legally required to minimize.And I suppose you're right.. if your organization has approved the use of office 365 to handle your data then by using Copilot it's still going to the same place.But is there not a way to securely store files locally and not share them with Microsoft while still using 365? I thought you could still operate Excel offline, or at least if 365 is connected, keep files locally stored and not transmit them to another server while working on them..",Reddit,https://www.reddit.com/r/OpenAI/comments/121odvl/will_copilot_run_through_microsofts_servers_does/,1
"Ok thanks. If I understand this correctly I see that the GDPR has security/privacy standards for transmitting sensitive data that has been authorized to be transmitted in the first place.. and Microsoft/Copilot may adhere to these standards just fine.But the question really is should certain data even be transmitted in the first place, regardless of how secure or compliant the connection is?I feel like a lot of handlers of this type of sensitive data are not going to realize they are transmitting swaths of Excel data through Microsoft's servers and potentially storing that data remotely when it should remain local.Microsoft may have checked their legal boxes.. but if the nature of Copilot is online only and transmission of data is the only way to process it, then it could still be a security/privacy risks for the Controllers and owners of this data.The controllers may not realize what is happening and may think Copilot is just another internal tool in the Microsoft office suite a software they have been using and therefore trusting for decades..Microsoft is essentially saying they are providing a ""standards compliant"" conduit for the data transmission.. but Microsoft does not know whether that data should have been transmitted in the first place. It's not up to them to decide, they just provide an adequately secure service to do so.But Microsoft should be more clear about whether copilot runs locally or not and what data is stored on their servers (i.e. queries, contents of Excel sheets, etc). Because the controller of the data may have a responsibility not to transmit the data depending on how it's handled...",Reddit,https://www.reddit.com/r/OpenAI/comments/121odvl/will_copilot_run_through_microsofts_servers_does/,1
"Major nitpick: How often do you see copyleft licenses deny use in commercial products? The GPL is certainly compatible with commercial ventures.While I appreciate your position, I think you're leaping a bit here. When it comes to IP there is no such thing as the 'desires and wishes' of the creator. It's 100% about the license. If it's not stated in the license, then that desire is not at all enforceable.Of course, Copilot very well may be violating licenses. That would be a bad thing to do. To say that Copilot should somehow know the unwritten thoughts and feelings of developers is an illogical stance.Besides all of that, to call for an entity to be completely ostracized over a single piece of software is ridiculous. This is not the way.The SFC should be looking for any instances in which Copilot has violated the law, and seek restitution. When the program becomes more expensive to continue in the face of unending IP violations and fines, it will go away.",Reddit,https://www.reddit.com/r/opensource/comments/vsaa6k/give_up_github_the_time_has_come_says_sfc_what_do/,5
"So anytime anyone does anything wrong, you just bail?Look, I've seen the MS hate for twenty years now. I believe in a healthy distrust of any corporation, but I still think this argument is absurd. I haven't seen EEE behavior in many years now. And while I see a moral argument against copilot, I struggle to see a legal one unless it starts spitting out copyrighted code.Microsoft is a gigantic corporation, and it's entirely possible that the copilot division didn't even talk to the GitHub division during development. Or if they did, the brass said ""we're doing this"" and that was that.What's more, there was nothing really stopping anyone from doing this over the public web. Having access to the servers just makes it slightly faster.",Reddit,https://www.reddit.com/r/opensource/comments/vsaa6k/give_up_github_the_time_has_come_says_sfc_what_do/,1
"Thanks for the detailed explanation and references. Would have been nicer without the undertone of hostility (""asking the question highlights a level of ignorance on the topic""), but I guess that's yours.In my mind, I'm thinking of cases where copilot copies a 50 line long function or class out of a 100 LOC codebase. Is that as clearcut in your mind? How about that same function out of a 70 LOC codebase? Out of a 60 LOC one? What's the magic ratio here?If a client came to you and asked whether they can safely publish the above function without including the license, will your legal advice to them be that they absolutely can and there's no chance they'll be dragged to court and have to pay legal fees?",Reddit,https://www.reddit.com/r/opensource/comments/vsaa6k/give_up_github_the_time_has_come_says_sfc_what_do/,5
"Yes and no. Technically it could write an entire program for you.But most commonly, it is just a tool that can help you get some stuff done quicker, but it’s not called Pilot, it’s Copilot. You need to be in charge of it, know fundamentals of programming, understand proper code security, and more. I’d definitely recommend doing some groundwork to learn the basics of programming before buying Copilot. That way you’ll get the most value from it.",Reddit,https://www.reddit.com/r/github/comments/1cko36o/i_have_never_code_before_but_i_want_to_start_with/,0
"No. That data is not stored. This is also on average what any company as an enterprise user would want. Reason being, that any data stored by Github might end up repurposed by them or could become public in case of leakage. As this risks leaking company secrets depending on the queries / usage, the average company has an interest of this data NOT being stored.There is one exception that I’m aware of as of now: The Copilot CLI (the one usable from terminal). It’s also the reason why this is a separate feature and currently disabled by default.",Reddit,https://www.reddit.com/r/github/comments/1c1rg3a/ive_accidentally_wrote_a_diary_inside_my_editor/,8
"So my code can now be just spitted out like that? Maybe it's time to switch away from GitHub.What if I create a license that disallows using my codebase as part of machine learning / training? Will the copilot be able to pick up on that?Also, what an incredible irony. Microsoft, a company notorious for threatening and killing smaller companies using coding patents, has produced a tool that makes violating code licenses easy.Remember youtubedl? This is a prime example of hypocrisy. When a small organization creates a tool that can be used for violating copyright, it gets deleted / shunned. When a big company does the same thing, it gets praised and supported. But I'd argue that copilot is way worse a perpetrator of this, because it trained their ML on unsuspecting codebases, and now encourages the straightup code stealing, and there's no way this can be considered fair use.",Reddit,https://www.reddit.com/r/programming/comments/oc9qj1/copilot_regurgitating_quake_code_including_sweary/,2
Not sure I would say this is overfitting. The trigger for copilot filling that in was basically the most notorious and known hack implemented in Quake. It surely has been copied into myriads of projects verbatim. I also think I read somewhere that it wasn't even original to Carmack,Reddit,https://www.reddit.com/r/programming/comments/oc9qj1/copilot_regurgitating_quake_code_including_sweary/,0
Is github training copilot also on private repositories? That'd be one big can of worms.GitHub's privacy policy is very clear that they don't process the contents of private repos except as required to host the repository. Even features like Dependabot have always been optin.,Reddit,https://www.reddit.com/r/programming/comments/oc9qj1/copilot_regurgitating_quake_code_including_sweary/,10
"No. When you put your code out, you define the terms of use in your license, and you expect others to follow your license. If your license disallows it to be used in ML algorithm, it shouldn't be. Having your own license doesn't violate TOS.The ethics of copilot is clearly questionable.",Reddit,https://www.reddit.com/r/programming/comments/oc9qj1/copilot_regurgitating_quake_code_including_sweary/,9
But this is not about the viral nature of GPL. Copilot also likely violates MIT and BSD licenses in the same way it violates GPL.,Reddit,https://www.reddit.com/r/programming/comments/oc9qj1/copilot_regurgitating_quake_code_including_sweary/,5
"Do you think if you add a 1% error rate you would have magically bypassed copyright laws?To reiterate, you can't use magical tricks to copy works because the law doesn't care how you copied them, only that you did. It also doesn't care if it isn't an exact copy, otherwise you could change one letter in Harry Potter and republish it yourself.That bit might actually be the biggest problem with CoPilot since it's trivial to detect when it regurgitates an exact copy of some GPL code but it's much harder to detect when it produces a near copy which may still violate copyright.",Reddit,https://www.reddit.com/r/programming/comments/oiql9e/risk_assessment_of_github_copilot/,5
"What if github copilot was exclusively trained on leaked source code(sorry i didnt watch the video, i will watch at home)",Reddit,https://www.reddit.com/r/programming/comments/olfd1h/i_made_a_worse_version_of_github_copilot/,3
"If they're going to lie to you and send your code they'd just do it regardless of whether or not you're using the copilot feature. So if you are already trusting them enough to use Visual Studio, there's probably no consistent reason to not trust them here as well.",Reddit,https://www.reddit.com/r/programming/comments/11zb7mb/github_copilot_chat_for_visual_studio_2022/,6
"On and off entrepreneurs and VCs have discussed whether it was fair/ethical for an investor to invest in 2 or more companies that are competitors.sorta... maybe... kinda... ""off topic"" but reminds me of this slide i saw earlier from way back when google monopolized ads that was found in this article:[How Google's ad business could be saved by a $150 billion spinoff Story by (Lara O'Reilly)]( yknow, looking at that slide, reddit is about the most questionable company that i support but thats kinda counteracted by how they are seemingly shunned in the realm of social media competitors. same reason i like mozilla. same reason i prefer microsoft (greatly) to google. for... similar but more complicated reasons, thats why i prefer firefox over random\_browser\_number\_42069.new, and why i prefer copilot over openai. you can be a huge successful business while still being trustwortyish... google crossed that line. theres a reason i want my windows phone back, and its more to get rid of android than it is to get a windows phone. although im a fan of androids whole making phones/computing accessible to everyone regardless of their income, but i mean, the windows phone was like that too? and despite all the complaints about microsoft, they are far less invasive than google/android.im also some guy who doesnt know what hes talking about but thats how it looks to me...and ive looked at this from a lot of angles for a lot more time than any one person really ever shouldwait this isnt where i parked my car wtf am i talking aboutedit: like if google wants to monopolize the smartphone market and get into computing and be the other apple, then microsoft (well, MSN/bing/copilot(?) mozilla (as in microsoft should drop edge and support mozilla's superior browser) reddit (as the redheaded step child of social media) and yeah openai i guess should make their own secret third thing/OS since everyone wants to play Open Source™️ monopoly gamesdude wheres my car",Reddit,https://www.reddit.com/r/artificial/comments/1fv892g/the_vibes_are_off/,1
"The MS Copilot release has been such ass, maybe Google is just doing what Microsoft should have.First they announced the Copilot release date, and then when that date came is when most people learned it wasn't going to be accessible to them at all, and it would only be accessible to larger companies at $30/month/seat with a large seat minimum.Being a new technology, not letting people actually get a feel for your technology without massive investment seems like a really bad choice, and puts a very large amount of risk onto any of the business that decide to try their product.Then have come these weird integrations of copilot into their ""new"" versions of both their desktop and cloud MS apps (most of which already barely work, both the ""old"" and ""new"" desktop versions, and ""old""/""new"" 365 versions) and the way it's been implemented seems like actual hot garbage.I opened up the ""new"" cloud version of Power Automate the other day and found Copilot was integrated, and tried to get help troubleshooting, asked some basic questions about Power Automate syntax, and asked it to even just add an action to my flow by name, and it wasn't able to do any of them well or correctly.I ended up opening a second browser window with ChatGPT 4 and it was infinitely more able to help with questions and even guiding me through a plan it made itself for how to create the flow I was working on.Whatever is going on with Google though, I definitely do trust them far more to create a useful, simple, well developed, accessible, and aesthetically pleasing system than I would trust Microsoft to do any of the above, even if it meant them taking their time with it.",Reddit,https://www.reddit.com/r/artificial/comments/187qbbh/google_has_been_way_too_quiet/,1
"This is why people endlessly bs about LLMs, how they work is precisely relevant to their limitations. Do you know what an LLM agent is? Because it’s not magic, it’s still a LLM. Do you have examples of LLM agents deployed in complex systems carrying out things outside of interpreting data and presenting it to you in natural language? Because they don’t exist out of marketing snippets, and I’ve built plenty. The best you can do is have an LLM be a copilot to a hacker. You’d have to decide what context it will need about a digital system, it might then able to alert you about vulnerabilities, give you generic suggestions about tasks to be carried out. But there is 0 ability to actually carry out end to end hacking of a system. Downvote me all you like, but technology is objective. If you can’t build it, it simply doesn’t exist.",Reddit,https://www.reddit.com/r/OpenAI/comments/1gcntnx/security_researchers_put_out_honeypots_to/,-1
"I’m very much aware of Google Cloud (~12% of their revenue). And playing catch up to AWS and even MS.Most of the hype I’ve seen from Bard/Gemini has focused on consumer users, so it hasn’t felt like such a strong focus on protecting enterprise or, especially, end user privacy with a very long history of selling user data. I’d be interested to know consumer vs enterprise revenue Google anticipates from their AI offerings.We’ve had MS’s enterprise Bing and now copilot powered by OpenAI and integrated with O365 for some time. So I’m much more familiar with their enterprise offerings and focus on protecting data.",Reddit,https://www.reddit.com/r/OpenAI/comments/1avzshl/gemini_15_will_be_20x_cheaper_than_gpt4_this_is/,1
"Ownership and privacy concerns. I use the gemini advanced mobile app and I could be wrong but it looks like they are allowed to see everything on my screen at any time, I have to agree to that to use the app. Also I would rather see Apple compete with Google and Microsoft on AI then have Apple sign a deal with Google. Edit: It also makes sense for Apple to eventually have their own version of Copilot like Microsoft is integrating into Windows.",Reddit,https://www.reddit.com/r/OpenAI/comments/1bhknr9/apple_is_in_talks_to_let_googles_gemini_power/,1
"It's certainly easier to adopt from a security standpoint, but it just doesn't seem worthwhile. M365 Copilot for Enterprise feels very much like a gimmick at this stage. They'll ideally need to come up with a model more similar to what Google has. Large context and fast inference would help for making their M365 integrations actually useful.",Reddit,https://www.reddit.com/r/OpenAI/comments/1hgna9l/google_has_overshadowed_12_days_of_open_ai_till/,-1
"Important: I am now a lawyer and am not familiar with actual laws regarding this.The programmer is the one responsible (or should be), since they're the one who wrote, checked and shipped the code. It doesn't matter they used copilot, copied code from some other project or even stole it from somewhere.It's not that the AI wrote the code without any supervision, it was a human who used it and (supposedly) checked the code or trusted the machine blindly.I'll give you an analogy to this. You buy an Edison electric car with a cruise control system that maintains the speed on the highway, brakes when there's a car too close and changes lanes when someone is going too slow in yours. The car doesn't detect properly a car in front of you, doesn't brake and you crash, heavily injuring you, your passengers and the other vehicles'.Would the AI or Edison be liable? Or should you, the driver in front of the wheel, in charge of and supposedly supervising the vehicle? I believe the second, since the system is an ""aid"" and not a replacement. The liable ones would be Edison in case it was a fully selfdriving car, where there's no driver.",Reddit,https://www.reddit.com/r/github/comments/128wxy4/the_ethical_dilemma_regarding_the_trajectory_of/,4
"I've said this for a while now when talking to other devs there is a problem here that people who don't know how to code will think they know how to code because they will ask the AI to do something and not have the knowledge to know if it is correct... a little knowledge is a dangerous thing. I've literally cringed looking at CoPilot producing SQL insert statements in VSCode with zero safeguards against injection attacks.You shouldn't be coding (whether freehand or AI) unless you know how to code. If you know how to code, what use is AI? As its capability stands right now, is it much more than advanced intellisense?Example you want a JS function that generates a random number between 2 numbers. Your options: 1. Code it yourself, presuming you are good enough of a coder to be able to produce optimal and bugfree code (granted, the func used as an example is very basic).2. Type ""javascript function generate random number between 2 numbers"", get the first result that comes up (which will be to stackoverflow) and get a function. I just did this it took me about 10 seconds to type in the search string, submit it and [find an answer on SO with 3341 upvotes]( Ask AI to generate the function then: 1. Review it and confirm it is correct, which you can only do if you are good enough to code it to begin with, negating the use of AI. 2. Assume the AI generated solution is bugfree and optimal and you would only assume that if you know so little about coding and AI that you do not realise it may not be optimal and/or bug free.I think scenario 3.2 is the phenomena that has lead to this: [ we get to the stage where we can guarantee AI can produce optimal and bugfree code, I think AI is either: 1. An advanced intellisense only to be used by advanced coders as a way to save time on key strokes2. A liability used by cowboys or the naïve.A selfdriving car that doesn't crash only 99.99% of the time is useless to everyone and will lead to recalls/legal action. I think we are seeing that scenario in the link above.",Reddit,https://www.reddit.com/r/programming/comments/1ctvfh9/netbsd_bans_all_commits_of_aigenerated_code/,0
"Your and prior poster's statements are not mutually exclusive.There are famous examples of people (same or different) creating the same code at different times, hell I've done it Giant project, rewrote the same function because I literally forgot I did it \~8 months ago; nearly identical implementation. Not coding, but my ex was popped for plagiarism... of herself. The issue was she did her masters thesis on an exceptionally narrow subject and had prior written papers on that subject in lower classes (no surprise). But because the problem domain was so specific they were close enough to trigger the tools. It was resolved but it wasn't pretty. There was zero mal intent, but it was still problematic.Now I'm confident we all agree banning the flow `prompt to LLM generated code commit` is the right thing to do, and I'm equally confident we don't mean to ban super fancy autocomplete or really smart linters... Somewhere between these two relatively simple examples is a line. I don't know how sharp or fuzzy it is, but it's there and should be explored and better defined.To the point about CYA that also is absolutely a valid input to the discussion IMO, and again the world is littered with legal landmines and CYAs like this that effectively autoassign blame to the offender and not the consumer (and I think that's fine TBH). If that's part of the project's reasoning then let's put that out there in the discussion. Right now the way both projects come off in the OP and the GPP link is: \[See edit below\]""ZOMG We can't (trust|understand|validate) AI at all so we shall ban it!""Again I am actually in agreement with (my interpretation/assumption of) the core intent of these bans: to maintain project and code integrity. AND I think we do need to start somewhere, and this really is as good a point as any. Now let's start a discussion (RFCs) of what that line looks like. ED:went and actually read the BSD post and not just the link in OP quoting here because it makes u/lelanthran 's statement much more relevant than I initially posited:Code generated by a large language model or similar technology, such as GitHub/Microsoft's Copilot, OpenAI's ChatGPT, or Facebook/Meta's Code Llama, is presumed to be tainted code, and must not be committed without prior written approval by core.Yeah, that totally makes sense... it also doesn't cause an issue with smart autocomplete/linter type tools IMO (though the Gentoo language in GPP is still problematic).",Reddit,https://www.reddit.com/r/programming/comments/1ctvfh9/netbsd_bans_all_commits_of_aigenerated_code/,0
"There's a high change that the function ""ParseImageName(absolutePath string)"" will be implemented similarly regardless of who writes it. It's also probable that many people have already written this function beforeAnd that's where your logic falls apart. A lot of source codes out there are downright terrible. [Knuth noted that the first binary search algorithm was published in 1942 but the first correct one was published in 1962. Bentley also observed that professional programmers at Bell Labs and IBM would be confident at the correctness of their implementations, while whopping 90% of theirs would contain bugs.]( that coding education has become more accessible, the phenomenon has gotten only worse, not better. A lot of CS students, selftaught hobbyists, and even junior developers wouldn't really be write their own implementations of something like `ParseImageName(absolutePath string)`. In most cases, they would simply copypaste a sample implementation from a tutorial. And many of the tutorials out there contain buggy or bad sample codes. They would often swallow errors, implement for only some use cases, deal only with the happy paths, or harbor serious bugs and vulnerabilities.In some cases, it's because the tutorial was made by someone who was trying to learn by teaching. In other cases, the sample code was deliberately ""dumbed down"" for pedagogical purposes. Nevertheless, the effect is the same. If there are multiple source codes implementing the very same `ParseImageName(absolutePath string)`, it is likely because it was copypasted from a tutorial ad nauseum, not because everyone implemented the same perfect and exemplary version again and again.[And it already shows in the AIgenerated code.]( The only thing that has changed was that, instead of copypasting from a tutorial written by human, peoplewould generate code directly from AI, which is generalized from the average Joe code which was, in turn, copypasted from the said tutorial. The current hype in AI started to give people the illusion that AI is infallible and better than human, which makes it actually very dangerous.Garbage in, garbage out.Edit: I've also took a gradschool level systems programming course (taught in C) a couple of years ago. You'd be mindblown to learn how many people with professional dev experience would get wrong with simple path parsing, ranging from incorrect result to infinite loops and even memory leaks. All they had to do was `realpath(3)`.Edit 2:[The NYU researchers produced 89 different scenarios wherein Copilot had to finish incomplete code. Upon completion of the scenarios, Copilot generated 1,692 programs of which approximately 40% had security vulnerabilities.]( “As Copilot is trained over open source code available on GitHub, we theorize that the variable security quality stems from the nature of the communityprovided code,” the researchers said. “Code often contains bugs—and so, given the vast quantity of unvetted code that Copilot has processed, it is certain that the language model will have learned from exploitable, buggy code.” So the chances that certain bugs, which are evident in code from open source repositories will more often be reproduced by Copilot are very high.",Reddit,https://www.reddit.com/r/programming/comments/1c4o0v6/automatic_code_generation_was_anticipated/,0
"I hear what you're saying and I have some sympathy for your point of view, but there's a fundamental difference here. YesterdayYou upgrade your laptop, your software gets faster.You switch from assembly to C your developers get faster and today, so does your software (computers are better at optimizing C than we are at assembly). You upgrade to Python, your developers get faster, accepting the tradeoff that their code gets slower. But if your bottlenecks are disk I/O and network latency, that's often OK.For Generative AI, you upgrade your LLM and your software gets _smarter_. This has never really happened before.I've written code in 4GLs like Easytrieve, but again, all they really do is make developers more productive (but often make it hard to have finegrained control).I've written a fair amount of Prolog, even going so far as to write [a Prolog interpreter in Perl]( (now maintained by someone else). Prolog _seems_ like it's smart, but it's nothing more than symbolic logic and much of its power can be replicated with an RDBMS, especially if you have recursion available.I've also written the frontend for a [backprop neural network]( First release was over 20 years ago and I saw then that this was something new. I could write software that could still give good answers, even with incomplete or incorrect information. We have all sorts of tricks to work around that with traditional software, but they're often fragile.We used to think that symbolic logic was the key to AI, but we quickly realized that its lack of contextual awareness hamstrung it. In the 1960s, the US was going to use this approach to AI to translate masses of Russian documents, giving them a huge intelligence edge in the cold war.This led, in part, to the first AI winter in the 70s. We saw the promise, but our understanding of AI in the 60s failed us. Symbolic logic couldn't really handle: Poor grammar Misspellings Idiomatic expressions Ambiguous expressionsThe AI winter came about because we assumed it was a bust. Generative AI can handle all of the above and more.This is not simply an upgrade of the existing paradigm, it's a new paradigm. TodayThe major players quickly realized that GenAI is something new, saw that they could do things that were literally not possible before, and decided to build out the infrastructure to support it.But they didn't wait for the government to do this, unlike for many previous ""huge projects"" like this (people think that private industry built the internet, but that's missing out on a ton of governmentled initiatives). So the private companies are building this out. Why? Why not wait for the government?It's simple. We're worried about losing our jobs. They're worried about losing their companies. If any company were to reach something conceptually close to safe AGI and the other companies can't close the gap immediately, it's game over for the other companies.Alphabet, Google's parent company, is in the most obvious danger here. Virtually all of their revenue stems directly or indirectly from advertising. Every time they try to innovate out of this space, they largely fail. Tools like Perplexity show that they can kill the traditional search engine by offering answers instead of links, and that largely kills the existing advertising model. If Google doesn't figure out their play, they're in serious trouble.Despite having started this game with their ""Attention is all you need"" paper, and probably having the deepest bench for AI development, they keep fumbling the ball. However, they're not giving up because they can't.Microsoft is probably the safest bet because they have such a diverse suite of offerings that they're everywhere and integrating AI solutions (Copilot) gives them a huge edge. I used to do spreadsheets on paper. I'll never go back to that. With a working Copilot, many people won't use another product unless it offers a similar service. That would be insane!OpenAI and Anthropic both seem to be trying to offer a novelty product as a utility. If they can succeed, they get little profit per customer, but tons of customers in exchange.Meta is ... I don't know what Meta is doing. Many people are scratching their heads trying to figure out Zuck's play. Maybe he's just trying to earn goodwill while he sees Facebook falling, but with powerful AI he controls and data about most humans on the planet, there's a path there. Not sure.In short, this is a game changer like nothing we've ever seen before. I'm not just writing software in languages I've not used before. I'm writing software I _literally_ could not write before. I'm building out AI pipelines for clients and those pipelines do astounding things that weren't possible a few years ago.With potential like this, entrepreneurs are going crazy. You're now limited by your imagination. Think of the internet today: it's nothing like we imagined it back in 1995. The AI boom is going to be like that, but on a scale no one has ever seen before.",Reddit,https://www.reddit.com/r/artificial/comments/1gfcpfm/more_than_a_quarter_of_new_code_at_google_is/,0
"We have been trying to do something similar, prompting and training AI to understand an ""App Definition"", which is an abstracted description of an application it covers everything from the datamodel, security, look and feel, functionality etc. The App Definition can then be turned into the end application by a known process/building blocks (aka nocode engine). So it could display it as a web, native, tv, watch etc The final code whether it be onthefly or ""compile time"" generated is done by the nocode building blocks. The difference here is that the nocode platforms are the intermediaries . So instead of prompt/s code, it's prompts nocode code. In the latter , you don't have to worry about a lot of code as it's created and maintained by the nocode platform. To make changes you can do this without coding via the nocode platform. While this is the general concept, there's reality. We're seeing 3 phases or levels, with folk using this:Level 1 Foundation AI Smart Start. You start with a prompt and use prompting to change your application which is developed on the fly in front of your eyes. At the moment we see folk reaching the capabilities of AI and this approach in less than 60 mins. It will generate your base application, but there definitely somethings it cannot do... but instead of bashing your head arguing with AI, just stop and go to Level 2. Level 2 Elevate use Nocode \ eg Figma & Nocode to make enhancements... but you have started with with Level 1 produced and it's already in the nocode platform. The level/phase allows you to do a lot. But you may still hit barriers. If so, then time for Level 3. Level 3 Extend : This is coding, either front end widgets that you could not do with Level 1 or 2. This also includes API / backend integrations that were not handled out of the box with Level 2. Yes you can use AI Copilot approach to speed things up. It's not ALL the code just nice bite size chunks that the developers user to extend and integrate the solution. So, we're hoping, that some of the benefits over a more traditional code everything or use AI to code everything are: 1) level one can be done in less than an hour and generate the base (boiler plate) of your application this does depend on a number of factors like what you're trying to do, quality of prompting, capabilities of the AI, capabilities of the nocode platform/s 2) Level two means that a broader section of the population can make changes faster and more easily,3) that you are using trusted bits of code (Nocode building blocks) that are created and supported by humans. Yes, there's a dependency, but hey everything's a tradeoff. Hopefully the net results is that you can move faster and more efficiently to build stuff using a hybrid approach of nocode with code... it does not have to be one or the other.",Reddit,https://www.reddit.com/r/OpenAI/comments/1bo1nr3/ai_generating_code_vs_nocode/,0
